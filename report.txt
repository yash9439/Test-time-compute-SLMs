First we Did Prompting Analysis on Reasoning Benchmark Datasets.
Then SFT at differnt epoch at higher epochs to observer gradients.
Then decided on 5 LR for epoch.
Ran Evaluation of those SFT Models.
Need to check like How many did thinking? 
How many of them went info infinite look?
Then BudgetForcing Issues, 6 Days for inference on 500 Samples.
Some conclusion, its not effective to use Deepseek R1 reasoning models in any Application, as with less GPU, they will take a lot of time generating tokens. Sometimes even cross 16384 context window.
Then Running R1 - Baseline i.e. Zero shot and no extra thinking.



Dont use Qwen 2.5 1.5B as LLM as a Judge, it literraly said 204 is same is 240 and gave YES as a Match.

The reason we use SFT + RLHF is that the model learns the output format systenm. In case of GRPO, if we blindly apply GRPO on some dataset, then the model will never learn as the reward in all the cases will be 0. 
Either mention in the prompt or system prompt instructing the model to give output in a certain format, or else first apply SFT and then do GRPO.

In LLM as a judge we tried using Qwen where we faced its wrong judgement, then tried llama3 8b, where due to time constrant, we dropped it. Then we choose GPT 4o mini as the LLM as a Judge.

Show Graphs of model response length, and show that how sft made model generate lots of token before coming to the final answer.

Also the R1 and the SFT tuned models are generating outputs of average 10K tokens, making it not feasible to apply Budget Forcing. Hence it highly depends on the application if you want to fine tune your model on Reasoning trace of a teacher model, as it will lead to high increase in latency.

hence now instead of training the model on that s1

Most of the GRPO Implement i found online just use the final answer of the training set to compute rewards. They didn't consider if the thinking steps of the model were correct or not. The reason is that for each training step, the model response would have to be breken into steps and matched with the solution steps with a llm in between to chcek if its correct or not which will cause tremendious increase in the training time.
Also since in GRPO we are not teaching the model the answer, but letting the model to generate multiple solutions say 100 using num_gen = 100, and then we are checking if any of those solution gave correct answer. 

Hence i observed a pattern that when i fine tuned Qwen 2.5 1.5B instruct on the S1 Dataset, it wasn't able to learn anything thing because out of all the solutions generated by the model, none of them was correct. That means we need to choose a inheriently good llm that have the knowledge but isn't able to use it.

Now since the SFT tuned models on SFT became resoning models but they are doing reasoning for about 7000-10000 tokens, which is clearly taking too much time and applying Budget forcing on it is not feasible. 

Hence what if we fine tune Qwen 2.5 1.5B instruct on a simple dataset using GRPO, its benifits will be:
1. We will learn to give output in Reasoning format
2. Since its a simpler Datset, the reward value should start increasing early in the training.
3. Again, its number of Reasoning Tokens will be much less since the training samples are not that complex.

Hence we used GSM8K which contains 1319 grade school math word problems.


To Do:
See GRPO
Evaluate GRPO
Implement Budget Forcing

GPU Hrs :
Initial SFT Exp to Analyse Train Grad (3 Scripts) : 6 Days = 24*6 = 144 Hrs
SFT-Reasoning Final Run (5 Scripts) : 30 Hrs
Baseline Inference (4 Scripts): 12 Hrs
SFT Inference (5 Scripts) : 58 Hrs * 5 = 290 Hrs
GRPO Finetuning : 58 Hrs
LLM As Judge : 
Budget Forcing :
GRPO : 
GRPO Inference : 
