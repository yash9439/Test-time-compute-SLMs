{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 315,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 0.07576771825551987,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.5031,
      "step": 1
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 0.10457178950309753,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6923,
      "step": 2
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.09130514413118362,
      "learning_rate": 3e-06,
      "loss": 0.6222,
      "step": 3
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 0.07114115357398987,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.4546,
      "step": 4
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 0.08230803906917572,
      "learning_rate": 5e-06,
      "loss": 0.5141,
      "step": 5
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.0826016291975975,
      "learning_rate": 4.983870967741936e-06,
      "loss": 0.5084,
      "step": 6
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.07691366970539093,
      "learning_rate": 4.967741935483871e-06,
      "loss": 0.4688,
      "step": 7
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 0.09171434491872787,
      "learning_rate": 4.951612903225807e-06,
      "loss": 0.6247,
      "step": 8
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.08598581701517105,
      "learning_rate": 4.9354838709677425e-06,
      "loss": 0.6091,
      "step": 9
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 0.07795404642820358,
      "learning_rate": 4.919354838709678e-06,
      "loss": 0.441,
      "step": 10
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 0.07740256935358047,
      "learning_rate": 4.903225806451613e-06,
      "loss": 0.4729,
      "step": 11
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.07575318217277527,
      "learning_rate": 4.8870967741935484e-06,
      "loss": 0.5158,
      "step": 12
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 0.08106215298175812,
      "learning_rate": 4.870967741935485e-06,
      "loss": 0.4954,
      "step": 13
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.08738259226083755,
      "learning_rate": 4.85483870967742e-06,
      "loss": 0.5357,
      "step": 14
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.08743799477815628,
      "learning_rate": 4.838709677419355e-06,
      "loss": 0.5691,
      "step": 15
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 0.08787281066179276,
      "learning_rate": 4.8225806451612905e-06,
      "loss": 0.5485,
      "step": 16
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 0.0852801501750946,
      "learning_rate": 4.806451612903227e-06,
      "loss": 0.5249,
      "step": 17
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.08191629499197006,
      "learning_rate": 4.790322580645161e-06,
      "loss": 0.5489,
      "step": 18
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 0.08401697874069214,
      "learning_rate": 4.774193548387097e-06,
      "loss": 0.5763,
      "step": 19
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 0.09430599212646484,
      "learning_rate": 4.758064516129033e-06,
      "loss": 0.6363,
      "step": 20
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.08529224246740341,
      "learning_rate": 4.741935483870968e-06,
      "loss": 0.609,
      "step": 21
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 0.0766773521900177,
      "learning_rate": 4.725806451612904e-06,
      "loss": 0.4497,
      "step": 22
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 0.08848389238119125,
      "learning_rate": 4.7096774193548385e-06,
      "loss": 0.5859,
      "step": 23
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.09100741147994995,
      "learning_rate": 4.693548387096775e-06,
      "loss": 0.5256,
      "step": 24
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 0.10039762407541275,
      "learning_rate": 4.67741935483871e-06,
      "loss": 0.5849,
      "step": 25
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 0.08869083225727081,
      "learning_rate": 4.661290322580645e-06,
      "loss": 0.6181,
      "step": 26
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.07973756641149521,
      "learning_rate": 4.6451612903225815e-06,
      "loss": 0.5097,
      "step": 27
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.08126208186149597,
      "learning_rate": 4.629032258064517e-06,
      "loss": 0.5405,
      "step": 28
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 0.07802844792604446,
      "learning_rate": 4.612903225806452e-06,
      "loss": 0.4987,
      "step": 29
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.0789375752210617,
      "learning_rate": 4.596774193548387e-06,
      "loss": 0.553,
      "step": 30
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 0.08183944225311279,
      "learning_rate": 4.580645161290323e-06,
      "loss": 0.5168,
      "step": 31
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 0.07935574650764465,
      "learning_rate": 4.564516129032259e-06,
      "loss": 0.4746,
      "step": 32
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 0.08033502101898193,
      "learning_rate": 4.548387096774194e-06,
      "loss": 0.5166,
      "step": 33
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 0.08573776483535767,
      "learning_rate": 4.5322580645161295e-06,
      "loss": 0.5907,
      "step": 34
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.07708115130662918,
      "learning_rate": 4.516129032258065e-06,
      "loss": 0.4693,
      "step": 35
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.08246468752622604,
      "learning_rate": 4.5e-06,
      "loss": 0.5589,
      "step": 36
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 0.07300419360399246,
      "learning_rate": 4.4838709677419354e-06,
      "loss": 0.4461,
      "step": 37
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 0.08120905607938766,
      "learning_rate": 4.467741935483872e-06,
      "loss": 0.5778,
      "step": 38
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 0.0871107280254364,
      "learning_rate": 4.451612903225807e-06,
      "loss": 0.5484,
      "step": 39
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 0.07955947518348694,
      "learning_rate": 4.435483870967742e-06,
      "loss": 0.4689,
      "step": 40
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 0.07128643989562988,
      "learning_rate": 4.419354838709678e-06,
      "loss": 0.5012,
      "step": 41
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.08145174384117126,
      "learning_rate": 4.403225806451613e-06,
      "loss": 0.5317,
      "step": 42
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 0.07119673490524292,
      "learning_rate": 4.387096774193549e-06,
      "loss": 0.4808,
      "step": 43
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 0.08713597804307938,
      "learning_rate": 4.370967741935484e-06,
      "loss": 0.5665,
      "step": 44
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.08058392256498337,
      "learning_rate": 4.35483870967742e-06,
      "loss": 0.5693,
      "step": 45
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 0.07462884485721588,
      "learning_rate": 4.338709677419356e-06,
      "loss": 0.4562,
      "step": 46
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 0.08041282743215561,
      "learning_rate": 4.32258064516129e-06,
      "loss": 0.5345,
      "step": 47
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.08445242047309875,
      "learning_rate": 4.306451612903226e-06,
      "loss": 0.5316,
      "step": 48
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.0818471685051918,
      "learning_rate": 4.290322580645162e-06,
      "loss": 0.4907,
      "step": 49
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 0.06862315535545349,
      "learning_rate": 4.274193548387097e-06,
      "loss": 0.4494,
      "step": 50
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 0.0790727511048317,
      "learning_rate": 4.258064516129032e-06,
      "loss": 0.46,
      "step": 51
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 0.07240922749042511,
      "learning_rate": 4.2419354838709685e-06,
      "loss": 0.4914,
      "step": 52
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 0.07623982429504395,
      "learning_rate": 4.225806451612904e-06,
      "loss": 0.5434,
      "step": 53
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.07528373599052429,
      "learning_rate": 4.209677419354839e-06,
      "loss": 0.4765,
      "step": 54
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 0.08265743404626846,
      "learning_rate": 4.193548387096774e-06,
      "loss": 0.5576,
      "step": 55
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.07219607383012772,
      "learning_rate": 4.17741935483871e-06,
      "loss": 0.4801,
      "step": 56
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 0.08096255362033844,
      "learning_rate": 4.161290322580646e-06,
      "loss": 0.5319,
      "step": 57
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 0.07188337296247482,
      "learning_rate": 4.14516129032258e-06,
      "loss": 0.511,
      "step": 58
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 0.08443102240562439,
      "learning_rate": 4.1290322580645165e-06,
      "loss": 0.5666,
      "step": 59
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.07483159750699997,
      "learning_rate": 4.112903225806452e-06,
      "loss": 0.5382,
      "step": 60
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 0.07298383116722107,
      "learning_rate": 4.096774193548387e-06,
      "loss": 0.5193,
      "step": 61
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 0.07749522477388382,
      "learning_rate": 4.080645161290323e-06,
      "loss": 0.5134,
      "step": 62
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.09637505561113358,
      "learning_rate": 4.064516129032259e-06,
      "loss": 0.6238,
      "step": 63
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 0.0765850767493248,
      "learning_rate": 4.048387096774194e-06,
      "loss": 0.5998,
      "step": 64
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 0.0695696547627449,
      "learning_rate": 4.032258064516129e-06,
      "loss": 0.5196,
      "step": 65
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.07255356013774872,
      "learning_rate": 4.0161290322580645e-06,
      "loss": 0.4805,
      "step": 66
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 0.07009699940681458,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.478,
      "step": 67
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 0.07623548060655594,
      "learning_rate": 3.983870967741936e-06,
      "loss": 0.5503,
      "step": 68
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.06972986459732056,
      "learning_rate": 3.967741935483871e-06,
      "loss": 0.5082,
      "step": 69
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.07547146081924438,
      "learning_rate": 3.951612903225807e-06,
      "loss": 0.5765,
      "step": 70
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 0.06648829579353333,
      "learning_rate": 3.935483870967742e-06,
      "loss": 0.4495,
      "step": 71
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.0663376897573471,
      "learning_rate": 3.919354838709677e-06,
      "loss": 0.4656,
      "step": 72
    },
    {
      "epoch": 1.1587301587301586,
      "grad_norm": 0.06810012459754944,
      "learning_rate": 3.903225806451613e-06,
      "loss": 0.503,
      "step": 73
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 0.06476575136184692,
      "learning_rate": 3.887096774193549e-06,
      "loss": 0.4517,
      "step": 74
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.07164835184812546,
      "learning_rate": 3.870967741935484e-06,
      "loss": 0.575,
      "step": 75
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 0.06988213211297989,
      "learning_rate": 3.85483870967742e-06,
      "loss": 0.5304,
      "step": 76
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.06738739460706711,
      "learning_rate": 3.838709677419355e-06,
      "loss": 0.4963,
      "step": 77
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 0.06612931191921234,
      "learning_rate": 3.822580645161291e-06,
      "loss": 0.4517,
      "step": 78
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 0.06558936089277267,
      "learning_rate": 3.8064516129032257e-06,
      "loss": 0.4881,
      "step": 79
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 0.0752505213022232,
      "learning_rate": 3.7903225806451614e-06,
      "loss": 0.5843,
      "step": 80
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.06368919461965561,
      "learning_rate": 3.774193548387097e-06,
      "loss": 0.4634,
      "step": 81
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 0.06528422981500626,
      "learning_rate": 3.7580645161290324e-06,
      "loss": 0.498,
      "step": 82
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 0.0754394456744194,
      "learning_rate": 3.741935483870968e-06,
      "loss": 0.6654,
      "step": 83
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.07449641078710556,
      "learning_rate": 3.7258064516129035e-06,
      "loss": 0.5313,
      "step": 84
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 0.06355324387550354,
      "learning_rate": 3.7096774193548392e-06,
      "loss": 0.5053,
      "step": 85
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 0.06088121607899666,
      "learning_rate": 3.693548387096775e-06,
      "loss": 0.4559,
      "step": 86
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.06402455270290375,
      "learning_rate": 3.67741935483871e-06,
      "loss": 0.4697,
      "step": 87
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 0.057689279317855835,
      "learning_rate": 3.6612903225806456e-06,
      "loss": 0.4079,
      "step": 88
    },
    {
      "epoch": 1.4126984126984126,
      "grad_norm": 0.06410281360149384,
      "learning_rate": 3.645161290322581e-06,
      "loss": 0.5437,
      "step": 89
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.06264496594667435,
      "learning_rate": 3.6290322580645166e-06,
      "loss": 0.4741,
      "step": 90
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.06490541994571686,
      "learning_rate": 3.6129032258064515e-06,
      "loss": 0.4831,
      "step": 91
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 0.07207579910755157,
      "learning_rate": 3.5967741935483872e-06,
      "loss": 0.646,
      "step": 92
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 0.06217965856194496,
      "learning_rate": 3.580645161290323e-06,
      "loss": 0.4477,
      "step": 93
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 0.07282615453004837,
      "learning_rate": 3.5645161290322583e-06,
      "loss": 0.6691,
      "step": 94
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 0.06609819829463959,
      "learning_rate": 3.548387096774194e-06,
      "loss": 0.5158,
      "step": 95
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.06629928946495056,
      "learning_rate": 3.5322580645161293e-06,
      "loss": 0.5661,
      "step": 96
    },
    {
      "epoch": 1.5396825396825395,
      "grad_norm": 0.064742311835289,
      "learning_rate": 3.516129032258065e-06,
      "loss": 0.4998,
      "step": 97
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.06477538496255875,
      "learning_rate": 3.5e-06,
      "loss": 0.4948,
      "step": 98
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.06296133249998093,
      "learning_rate": 3.4838709677419357e-06,
      "loss": 0.5142,
      "step": 99
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 0.06210988014936447,
      "learning_rate": 3.4677419354838714e-06,
      "loss": 0.4731,
      "step": 100
    },
    {
      "epoch": 1.6031746031746033,
      "grad_norm": 0.05627292022109032,
      "learning_rate": 3.4516129032258067e-06,
      "loss": 0.4847,
      "step": 101
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 0.0644472986459732,
      "learning_rate": 3.4354838709677425e-06,
      "loss": 0.5155,
      "step": 102
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 0.06500030308961868,
      "learning_rate": 3.4193548387096773e-06,
      "loss": 0.5121,
      "step": 103
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 0.06088189035654068,
      "learning_rate": 3.403225806451613e-06,
      "loss": 0.4633,
      "step": 104
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.06265779584646225,
      "learning_rate": 3.3870967741935484e-06,
      "loss": 0.542,
      "step": 105
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 0.06259393692016602,
      "learning_rate": 3.370967741935484e-06,
      "loss": 0.5733,
      "step": 106
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 0.05968916416168213,
      "learning_rate": 3.35483870967742e-06,
      "loss": 0.4884,
      "step": 107
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.059708356857299805,
      "learning_rate": 3.338709677419355e-06,
      "loss": 0.4834,
      "step": 108
    },
    {
      "epoch": 1.7301587301587302,
      "grad_norm": 0.06483674794435501,
      "learning_rate": 3.322580645161291e-06,
      "loss": 0.5107,
      "step": 109
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 0.06297985464334488,
      "learning_rate": 3.306451612903226e-06,
      "loss": 0.5583,
      "step": 110
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 0.05504227802157402,
      "learning_rate": 3.2903225806451615e-06,
      "loss": 0.4801,
      "step": 111
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.058233123272657394,
      "learning_rate": 3.274193548387097e-06,
      "loss": 0.5213,
      "step": 112
    },
    {
      "epoch": 1.7936507936507935,
      "grad_norm": 0.0580352246761322,
      "learning_rate": 3.2580645161290326e-06,
      "loss": 0.5332,
      "step": 113
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 0.0625985786318779,
      "learning_rate": 3.2419354838709683e-06,
      "loss": 0.5304,
      "step": 114
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 0.056557100266218185,
      "learning_rate": 3.225806451612903e-06,
      "loss": 0.5056,
      "step": 115
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 0.05721088871359825,
      "learning_rate": 3.209677419354839e-06,
      "loss": 0.4796,
      "step": 116
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.06003372371196747,
      "learning_rate": 3.1935483870967742e-06,
      "loss": 0.4853,
      "step": 117
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 0.06131996214389801,
      "learning_rate": 3.17741935483871e-06,
      "loss": 0.5049,
      "step": 118
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.05667506903409958,
      "learning_rate": 3.1612903225806453e-06,
      "loss": 0.513,
      "step": 119
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 0.05973419174551964,
      "learning_rate": 3.145161290322581e-06,
      "loss": 0.5494,
      "step": 120
    },
    {
      "epoch": 1.9206349206349205,
      "grad_norm": 0.058280862867832184,
      "learning_rate": 3.1290322580645167e-06,
      "loss": 0.5169,
      "step": 121
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 0.05454613268375397,
      "learning_rate": 3.1129032258064516e-06,
      "loss": 0.4395,
      "step": 122
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 0.05824033170938492,
      "learning_rate": 3.0967741935483874e-06,
      "loss": 0.4879,
      "step": 123
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 0.0612824447453022,
      "learning_rate": 3.0806451612903227e-06,
      "loss": 0.6059,
      "step": 124
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 0.054217129945755005,
      "learning_rate": 3.0645161290322584e-06,
      "loss": 0.4397,
      "step": 125
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.06527340412139893,
      "learning_rate": 3.0483870967741937e-06,
      "loss": 0.4579,
      "step": 126
    },
    {
      "epoch": 2.015873015873016,
      "grad_norm": 0.05570518970489502,
      "learning_rate": 3.0322580645161295e-06,
      "loss": 0.4586,
      "step": 127
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 0.05855906009674072,
      "learning_rate": 3.0161290322580648e-06,
      "loss": 0.5529,
      "step": 128
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 0.06243079528212547,
      "learning_rate": 3e-06,
      "loss": 0.5999,
      "step": 129
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 0.059722501784563065,
      "learning_rate": 2.983870967741936e-06,
      "loss": 0.4963,
      "step": 130
    },
    {
      "epoch": 2.0793650793650795,
      "grad_norm": 0.06204649806022644,
      "learning_rate": 2.967741935483871e-06,
      "loss": 0.5608,
      "step": 131
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.05820710211992264,
      "learning_rate": 2.951612903225807e-06,
      "loss": 0.562,
      "step": 132
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.0537724569439888,
      "learning_rate": 2.9354838709677417e-06,
      "loss": 0.4881,
      "step": 133
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 0.05457578971982002,
      "learning_rate": 2.9193548387096775e-06,
      "loss": 0.4919,
      "step": 134
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.05561745539307594,
      "learning_rate": 2.903225806451613e-06,
      "loss": 0.4353,
      "step": 135
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 0.058567579835653305,
      "learning_rate": 2.8870967741935485e-06,
      "loss": 0.5569,
      "step": 136
    },
    {
      "epoch": 2.1746031746031744,
      "grad_norm": 0.058323491364717484,
      "learning_rate": 2.8709677419354843e-06,
      "loss": 0.436,
      "step": 137
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.055497076362371445,
      "learning_rate": 2.8548387096774196e-06,
      "loss": 0.4349,
      "step": 138
    },
    {
      "epoch": 2.2063492063492065,
      "grad_norm": 0.05745953693985939,
      "learning_rate": 2.8387096774193553e-06,
      "loss": 0.5096,
      "step": 139
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.06287651509046555,
      "learning_rate": 2.822580645161291e-06,
      "loss": 0.5664,
      "step": 140
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 0.06043208763003349,
      "learning_rate": 2.806451612903226e-06,
      "loss": 0.5267,
      "step": 141
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 0.05494304373860359,
      "learning_rate": 2.7903225806451617e-06,
      "loss": 0.5179,
      "step": 142
    },
    {
      "epoch": 2.2698412698412698,
      "grad_norm": 0.05780433863401413,
      "learning_rate": 2.774193548387097e-06,
      "loss": 0.4965,
      "step": 143
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.05441446974873543,
      "learning_rate": 2.7580645161290327e-06,
      "loss": 0.5091,
      "step": 144
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 0.0569382905960083,
      "learning_rate": 2.7419354838709676e-06,
      "loss": 0.4687,
      "step": 145
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 0.0640302300453186,
      "learning_rate": 2.7258064516129033e-06,
      "loss": 0.6045,
      "step": 146
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.0545048862695694,
      "learning_rate": 2.709677419354839e-06,
      "loss": 0.4875,
      "step": 147
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 0.05235051363706589,
      "learning_rate": 2.6935483870967744e-06,
      "loss": 0.4456,
      "step": 148
    },
    {
      "epoch": 2.365079365079365,
      "grad_norm": 0.051182787865400314,
      "learning_rate": 2.67741935483871e-06,
      "loss": 0.4615,
      "step": 149
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.05717286095023155,
      "learning_rate": 2.6612903225806454e-06,
      "loss": 0.5726,
      "step": 150
    },
    {
      "epoch": 2.3968253968253967,
      "grad_norm": 0.05687982961535454,
      "learning_rate": 2.645161290322581e-06,
      "loss": 0.5214,
      "step": 151
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 0.056606270372867584,
      "learning_rate": 2.629032258064516e-06,
      "loss": 0.4579,
      "step": 152
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.05289517715573311,
      "learning_rate": 2.6129032258064518e-06,
      "loss": 0.4775,
      "step": 153
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.05773982033133507,
      "learning_rate": 2.5967741935483875e-06,
      "loss": 0.5113,
      "step": 154
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 0.04997682571411133,
      "learning_rate": 2.580645161290323e-06,
      "loss": 0.3926,
      "step": 155
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.051723770797252655,
      "learning_rate": 2.5645161290322585e-06,
      "loss": 0.49,
      "step": 156
    },
    {
      "epoch": 2.492063492063492,
      "grad_norm": 0.05287877842783928,
      "learning_rate": 2.5483870967741934e-06,
      "loss": 0.4852,
      "step": 157
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 0.05513234809041023,
      "learning_rate": 2.532258064516129e-06,
      "loss": 0.5007,
      "step": 158
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 0.054082825779914856,
      "learning_rate": 2.5161290322580645e-06,
      "loss": 0.463,
      "step": 159
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 0.06020207330584526,
      "learning_rate": 2.5e-06,
      "loss": 0.549,
      "step": 160
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.056372057646512985,
      "learning_rate": 2.4838709677419355e-06,
      "loss": 0.4939,
      "step": 161
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.04847389832139015,
      "learning_rate": 2.4677419354838712e-06,
      "loss": 0.4161,
      "step": 162
    },
    {
      "epoch": 2.5873015873015874,
      "grad_norm": 0.06519752740859985,
      "learning_rate": 2.4516129032258066e-06,
      "loss": 0.5988,
      "step": 163
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 0.0560627281665802,
      "learning_rate": 2.4354838709677423e-06,
      "loss": 0.521,
      "step": 164
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 0.05543885380029678,
      "learning_rate": 2.4193548387096776e-06,
      "loss": 0.4919,
      "step": 165
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 0.060657866299152374,
      "learning_rate": 2.4032258064516133e-06,
      "loss": 0.5864,
      "step": 166
    },
    {
      "epoch": 2.6507936507936507,
      "grad_norm": 0.0559258833527565,
      "learning_rate": 2.3870967741935486e-06,
      "loss": 0.469,
      "step": 167
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.053152959793806076,
      "learning_rate": 2.370967741935484e-06,
      "loss": 0.4339,
      "step": 168
    },
    {
      "epoch": 2.682539682539683,
      "grad_norm": 0.057022083550691605,
      "learning_rate": 2.3548387096774193e-06,
      "loss": 0.5179,
      "step": 169
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 0.056515198200941086,
      "learning_rate": 2.338709677419355e-06,
      "loss": 0.5114,
      "step": 170
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 0.055253807455301285,
      "learning_rate": 2.3225806451612907e-06,
      "loss": 0.4975,
      "step": 171
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 0.050770945847034454,
      "learning_rate": 2.306451612903226e-06,
      "loss": 0.3959,
      "step": 172
    },
    {
      "epoch": 2.746031746031746,
      "grad_norm": 0.05476607754826546,
      "learning_rate": 2.2903225806451614e-06,
      "loss": 0.5248,
      "step": 173
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.04863478243350983,
      "learning_rate": 2.274193548387097e-06,
      "loss": 0.4376,
      "step": 174
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.05497009679675102,
      "learning_rate": 2.2580645161290324e-06,
      "loss": 0.415,
      "step": 175
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 0.05411161482334137,
      "learning_rate": 2.2419354838709677e-06,
      "loss": 0.4645,
      "step": 176
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 0.05143052339553833,
      "learning_rate": 2.2258064516129034e-06,
      "loss": 0.4491,
      "step": 177
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 0.06440345197916031,
      "learning_rate": 2.209677419354839e-06,
      "loss": 0.6103,
      "step": 178
    },
    {
      "epoch": 2.8412698412698414,
      "grad_norm": 0.05380659177899361,
      "learning_rate": 2.1935483870967745e-06,
      "loss": 0.5002,
      "step": 179
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.05610280483961105,
      "learning_rate": 2.17741935483871e-06,
      "loss": 0.5243,
      "step": 180
    },
    {
      "epoch": 2.873015873015873,
      "grad_norm": 0.055768366903066635,
      "learning_rate": 2.161290322580645e-06,
      "loss": 0.5362,
      "step": 181
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.06459609419107437,
      "learning_rate": 2.145161290322581e-06,
      "loss": 0.6537,
      "step": 182
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 0.04949212819337845,
      "learning_rate": 2.129032258064516e-06,
      "loss": 0.4179,
      "step": 183
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 0.058381371200084686,
      "learning_rate": 2.112903225806452e-06,
      "loss": 0.5967,
      "step": 184
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 0.05682291463017464,
      "learning_rate": 2.096774193548387e-06,
      "loss": 0.5074,
      "step": 185
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.04747257009148598,
      "learning_rate": 2.080645161290323e-06,
      "loss": 0.3904,
      "step": 186
    },
    {
      "epoch": 2.9682539682539684,
      "grad_norm": 0.04951846972107887,
      "learning_rate": 2.0645161290322582e-06,
      "loss": 0.4414,
      "step": 187
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 0.04915264621376991,
      "learning_rate": 2.0483870967741936e-06,
      "loss": 0.4434,
      "step": 188
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.06567876040935516,
      "learning_rate": 2.0322580645161293e-06,
      "loss": 0.5684,
      "step": 189
    },
    {
      "epoch": 3.015873015873016,
      "grad_norm": 0.04975946247577667,
      "learning_rate": 2.0161290322580646e-06,
      "loss": 0.4603,
      "step": 190
    },
    {
      "epoch": 3.0317460317460316,
      "grad_norm": 0.05322004482150078,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.4534,
      "step": 191
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 0.04833943396806717,
      "learning_rate": 1.9838709677419356e-06,
      "loss": 0.4259,
      "step": 192
    },
    {
      "epoch": 3.0634920634920633,
      "grad_norm": 0.05271314084529877,
      "learning_rate": 1.967741935483871e-06,
      "loss": 0.5025,
      "step": 193
    },
    {
      "epoch": 3.0793650793650795,
      "grad_norm": 0.047544874250888824,
      "learning_rate": 1.9516129032258067e-06,
      "loss": 0.4319,
      "step": 194
    },
    {
      "epoch": 3.0952380952380953,
      "grad_norm": 0.0526626892387867,
      "learning_rate": 1.935483870967742e-06,
      "loss": 0.5151,
      "step": 195
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.04794132336974144,
      "learning_rate": 1.9193548387096773e-06,
      "loss": 0.4332,
      "step": 196
    },
    {
      "epoch": 3.126984126984127,
      "grad_norm": 0.05113818123936653,
      "learning_rate": 1.9032258064516128e-06,
      "loss": 0.4786,
      "step": 197
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.053213972598314285,
      "learning_rate": 1.8870967741935486e-06,
      "loss": 0.5019,
      "step": 198
    },
    {
      "epoch": 3.1587301587301586,
      "grad_norm": 0.05428680032491684,
      "learning_rate": 1.870967741935484e-06,
      "loss": 0.5074,
      "step": 199
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 0.05093353986740112,
      "learning_rate": 1.8548387096774196e-06,
      "loss": 0.4827,
      "step": 200
    },
    {
      "epoch": 3.1904761904761907,
      "grad_norm": 0.06232593581080437,
      "learning_rate": 1.838709677419355e-06,
      "loss": 0.6305,
      "step": 201
    },
    {
      "epoch": 3.2063492063492065,
      "grad_norm": 0.048471227288246155,
      "learning_rate": 1.8225806451612904e-06,
      "loss": 0.4385,
      "step": 202
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.053310543298721313,
      "learning_rate": 1.8064516129032258e-06,
      "loss": 0.488,
      "step": 203
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 0.04999511316418648,
      "learning_rate": 1.7903225806451615e-06,
      "loss": 0.4838,
      "step": 204
    },
    {
      "epoch": 3.253968253968254,
      "grad_norm": 0.054068271070718765,
      "learning_rate": 1.774193548387097e-06,
      "loss": 0.5033,
      "step": 205
    },
    {
      "epoch": 3.2698412698412698,
      "grad_norm": 0.059573572129011154,
      "learning_rate": 1.7580645161290325e-06,
      "loss": 0.5706,
      "step": 206
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.05521145090460777,
      "learning_rate": 1.7419354838709678e-06,
      "loss": 0.5431,
      "step": 207
    },
    {
      "epoch": 3.3015873015873014,
      "grad_norm": 0.04890759661793709,
      "learning_rate": 1.7258064516129034e-06,
      "loss": 0.4292,
      "step": 208
    },
    {
      "epoch": 3.317460317460317,
      "grad_norm": 0.05245886743068695,
      "learning_rate": 1.7096774193548387e-06,
      "loss": 0.4828,
      "step": 209
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.05214422196149826,
      "learning_rate": 1.6935483870967742e-06,
      "loss": 0.4757,
      "step": 210
    },
    {
      "epoch": 3.3492063492063493,
      "grad_norm": 0.04877236485481262,
      "learning_rate": 1.67741935483871e-06,
      "loss": 0.4349,
      "step": 211
    },
    {
      "epoch": 3.365079365079365,
      "grad_norm": 0.04751499742269516,
      "learning_rate": 1.6612903225806455e-06,
      "loss": 0.455,
      "step": 212
    },
    {
      "epoch": 3.380952380952381,
      "grad_norm": 0.05026089772582054,
      "learning_rate": 1.6451612903225808e-06,
      "loss": 0.5092,
      "step": 213
    },
    {
      "epoch": 3.3968253968253967,
      "grad_norm": 0.04834747314453125,
      "learning_rate": 1.6290322580645163e-06,
      "loss": 0.4291,
      "step": 214
    },
    {
      "epoch": 3.4126984126984126,
      "grad_norm": 0.05782558023929596,
      "learning_rate": 1.6129032258064516e-06,
      "loss": 0.634,
      "step": 215
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 0.05530749633908272,
      "learning_rate": 1.5967741935483871e-06,
      "loss": 0.5029,
      "step": 216
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.053955405950546265,
      "learning_rate": 1.5806451612903226e-06,
      "loss": 0.5552,
      "step": 217
    },
    {
      "epoch": 3.4603174603174605,
      "grad_norm": 0.0522751584649086,
      "learning_rate": 1.5645161290322584e-06,
      "loss": 0.4823,
      "step": 218
    },
    {
      "epoch": 3.4761904761904763,
      "grad_norm": 0.05237238481640816,
      "learning_rate": 1.5483870967741937e-06,
      "loss": 0.4731,
      "step": 219
    },
    {
      "epoch": 3.492063492063492,
      "grad_norm": 0.05141486972570419,
      "learning_rate": 1.5322580645161292e-06,
      "loss": 0.4826,
      "step": 220
    },
    {
      "epoch": 3.507936507936508,
      "grad_norm": 0.05691741406917572,
      "learning_rate": 1.5161290322580647e-06,
      "loss": 0.5459,
      "step": 221
    },
    {
      "epoch": 3.5238095238095237,
      "grad_norm": 0.0530221089720726,
      "learning_rate": 1.5e-06,
      "loss": 0.5486,
      "step": 222
    },
    {
      "epoch": 3.5396825396825395,
      "grad_norm": 0.0512666217982769,
      "learning_rate": 1.4838709677419356e-06,
      "loss": 0.4792,
      "step": 223
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.050637125968933105,
      "learning_rate": 1.4677419354838709e-06,
      "loss": 0.4496,
      "step": 224
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 0.0595364086329937,
      "learning_rate": 1.4516129032258066e-06,
      "loss": 0.5485,
      "step": 225
    },
    {
      "epoch": 3.5873015873015874,
      "grad_norm": 0.050466109067201614,
      "learning_rate": 1.4354838709677421e-06,
      "loss": 0.4627,
      "step": 226
    },
    {
      "epoch": 3.6031746031746033,
      "grad_norm": 0.05411699786782265,
      "learning_rate": 1.4193548387096776e-06,
      "loss": 0.4858,
      "step": 227
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 0.051099393516778946,
      "learning_rate": 1.403225806451613e-06,
      "loss": 0.4681,
      "step": 228
    },
    {
      "epoch": 3.634920634920635,
      "grad_norm": 0.05318473279476166,
      "learning_rate": 1.3870967741935485e-06,
      "loss": 0.5126,
      "step": 229
    },
    {
      "epoch": 3.6507936507936507,
      "grad_norm": 0.05115949362516403,
      "learning_rate": 1.3709677419354838e-06,
      "loss": 0.4748,
      "step": 230
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.05943838879466057,
      "learning_rate": 1.3548387096774195e-06,
      "loss": 0.6026,
      "step": 231
    },
    {
      "epoch": 3.682539682539683,
      "grad_norm": 0.04796231538057327,
      "learning_rate": 1.338709677419355e-06,
      "loss": 0.4396,
      "step": 232
    },
    {
      "epoch": 3.6984126984126986,
      "grad_norm": 0.049703702330589294,
      "learning_rate": 1.3225806451612906e-06,
      "loss": 0.5043,
      "step": 233
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 0.05068688094615936,
      "learning_rate": 1.3064516129032259e-06,
      "loss": 0.5037,
      "step": 234
    },
    {
      "epoch": 3.7301587301587302,
      "grad_norm": 0.05189601331949234,
      "learning_rate": 1.2903225806451614e-06,
      "loss": 0.4637,
      "step": 235
    },
    {
      "epoch": 3.746031746031746,
      "grad_norm": 0.05885480344295502,
      "learning_rate": 1.2741935483870967e-06,
      "loss": 0.52,
      "step": 236
    },
    {
      "epoch": 3.761904761904762,
      "grad_norm": 0.05226357281208038,
      "learning_rate": 1.2580645161290322e-06,
      "loss": 0.5037,
      "step": 237
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.05125487595796585,
      "learning_rate": 1.2419354838709678e-06,
      "loss": 0.4784,
      "step": 238
    },
    {
      "epoch": 3.7936507936507935,
      "grad_norm": 0.04911249876022339,
      "learning_rate": 1.2258064516129033e-06,
      "loss": 0.4356,
      "step": 239
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 0.050959184765815735,
      "learning_rate": 1.2096774193548388e-06,
      "loss": 0.4785,
      "step": 240
    },
    {
      "epoch": 3.825396825396825,
      "grad_norm": 0.057275645434856415,
      "learning_rate": 1.1935483870967743e-06,
      "loss": 0.6184,
      "step": 241
    },
    {
      "epoch": 3.8412698412698414,
      "grad_norm": 0.04898501932621002,
      "learning_rate": 1.1774193548387096e-06,
      "loss": 0.4532,
      "step": 242
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 0.051056694239377975,
      "learning_rate": 1.1612903225806454e-06,
      "loss": 0.4677,
      "step": 243
    },
    {
      "epoch": 3.873015873015873,
      "grad_norm": 0.05131945386528969,
      "learning_rate": 1.1451612903225807e-06,
      "loss": 0.4281,
      "step": 244
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.051774412393569946,
      "learning_rate": 1.1290322580645162e-06,
      "loss": 0.4852,
      "step": 245
    },
    {
      "epoch": 3.9047619047619047,
      "grad_norm": 0.05203520134091377,
      "learning_rate": 1.1129032258064517e-06,
      "loss": 0.4981,
      "step": 246
    },
    {
      "epoch": 3.9206349206349205,
      "grad_norm": 0.04700109362602234,
      "learning_rate": 1.0967741935483872e-06,
      "loss": 0.4228,
      "step": 247
    },
    {
      "epoch": 3.9365079365079367,
      "grad_norm": 0.05294264107942581,
      "learning_rate": 1.0806451612903226e-06,
      "loss": 0.5006,
      "step": 248
    },
    {
      "epoch": 3.9523809523809526,
      "grad_norm": 0.04782140627503395,
      "learning_rate": 1.064516129032258e-06,
      "loss": 0.4016,
      "step": 249
    },
    {
      "epoch": 3.9682539682539684,
      "grad_norm": 0.052122555673122406,
      "learning_rate": 1.0483870967741936e-06,
      "loss": 0.54,
      "step": 250
    },
    {
      "epoch": 3.984126984126984,
      "grad_norm": 605.0298461914062,
      "learning_rate": 1.0322580645161291e-06,
      "loss": 0.5143,
      "step": 251
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.07630778849124908,
      "learning_rate": 1.0161290322580646e-06,
      "loss": 0.6442,
      "step": 252
    },
    {
      "epoch": 4.015873015873016,
      "grad_norm": 0.054313793778419495,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.5627,
      "step": 253
    },
    {
      "epoch": 4.031746031746032,
      "grad_norm": 0.05513836070895195,
      "learning_rate": 9.838709677419355e-07,
      "loss": 0.5368,
      "step": 254
    },
    {
      "epoch": 4.0476190476190474,
      "grad_norm": 0.049131035804748535,
      "learning_rate": 9.67741935483871e-07,
      "loss": 0.431,
      "step": 255
    },
    {
      "epoch": 4.063492063492063,
      "grad_norm": 0.05309339612722397,
      "learning_rate": 9.516129032258064e-07,
      "loss": 0.5237,
      "step": 256
    },
    {
      "epoch": 4.079365079365079,
      "grad_norm": 0.05207057297229767,
      "learning_rate": 9.35483870967742e-07,
      "loss": 0.4781,
      "step": 257
    },
    {
      "epoch": 4.095238095238095,
      "grad_norm": 0.05145217850804329,
      "learning_rate": 9.193548387096775e-07,
      "loss": 0.4833,
      "step": 258
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.04812674969434738,
      "learning_rate": 9.032258064516129e-07,
      "loss": 0.4631,
      "step": 259
    },
    {
      "epoch": 4.1269841269841265,
      "grad_norm": 0.05162526294589043,
      "learning_rate": 8.870967741935485e-07,
      "loss": 0.5028,
      "step": 260
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 0.05022968724370003,
      "learning_rate": 8.709677419354839e-07,
      "loss": 0.4796,
      "step": 261
    },
    {
      "epoch": 4.158730158730159,
      "grad_norm": 0.05394340679049492,
      "learning_rate": 8.548387096774193e-07,
      "loss": 0.5674,
      "step": 262
    },
    {
      "epoch": 4.174603174603175,
      "grad_norm": 0.05109831690788269,
      "learning_rate": 8.38709677419355e-07,
      "loss": 0.4978,
      "step": 263
    },
    {
      "epoch": 4.190476190476191,
      "grad_norm": 0.053520601242780685,
      "learning_rate": 8.225806451612904e-07,
      "loss": 0.5221,
      "step": 264
    },
    {
      "epoch": 4.2063492063492065,
      "grad_norm": 0.04775198549032211,
      "learning_rate": 8.064516129032258e-07,
      "loss": 0.4435,
      "step": 265
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.05214504152536392,
      "learning_rate": 7.903225806451613e-07,
      "loss": 0.5128,
      "step": 266
    },
    {
      "epoch": 4.238095238095238,
      "grad_norm": 0.049395427107810974,
      "learning_rate": 7.741935483870968e-07,
      "loss": 0.4507,
      "step": 267
    },
    {
      "epoch": 4.253968253968254,
      "grad_norm": 0.058967266231775284,
      "learning_rate": 7.580645161290324e-07,
      "loss": 0.6559,
      "step": 268
    },
    {
      "epoch": 4.26984126984127,
      "grad_norm": 0.052716489881277084,
      "learning_rate": 7.419354838709678e-07,
      "loss": 0.5424,
      "step": 269
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.04787110537290573,
      "learning_rate": 7.258064516129033e-07,
      "loss": 0.4565,
      "step": 270
    },
    {
      "epoch": 4.301587301587301,
      "grad_norm": 0.054178379476070404,
      "learning_rate": 7.096774193548388e-07,
      "loss": 0.51,
      "step": 271
    },
    {
      "epoch": 4.317460317460317,
      "grad_norm": 0.05364539846777916,
      "learning_rate": 6.935483870967742e-07,
      "loss": 0.4822,
      "step": 272
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.0465899296104908,
      "learning_rate": 6.774193548387098e-07,
      "loss": 0.4228,
      "step": 273
    },
    {
      "epoch": 4.349206349206349,
      "grad_norm": 0.055440161377191544,
      "learning_rate": 6.612903225806453e-07,
      "loss": 0.5542,
      "step": 274
    },
    {
      "epoch": 4.365079365079365,
      "grad_norm": 0.05130774527788162,
      "learning_rate": 6.451612903225807e-07,
      "loss": 0.5065,
      "step": 275
    },
    {
      "epoch": 4.380952380952381,
      "grad_norm": 0.05356642231345177,
      "learning_rate": 6.290322580645161e-07,
      "loss": 0.5325,
      "step": 276
    },
    {
      "epoch": 4.396825396825397,
      "grad_norm": 0.051938336342573166,
      "learning_rate": 6.129032258064516e-07,
      "loss": 0.482,
      "step": 277
    },
    {
      "epoch": 4.412698412698413,
      "grad_norm": 0.05112293362617493,
      "learning_rate": 5.967741935483872e-07,
      "loss": 0.5114,
      "step": 278
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 0.04885285347700119,
      "learning_rate": 5.806451612903227e-07,
      "loss": 0.4445,
      "step": 279
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.046317752450704575,
      "learning_rate": 5.645161290322581e-07,
      "loss": 0.4825,
      "step": 280
    },
    {
      "epoch": 4.4603174603174605,
      "grad_norm": 0.052289512008428574,
      "learning_rate": 5.483870967741936e-07,
      "loss": 0.4593,
      "step": 281
    },
    {
      "epoch": 4.476190476190476,
      "grad_norm": 0.05370168015360832,
      "learning_rate": 5.32258064516129e-07,
      "loss": 0.496,
      "step": 282
    },
    {
      "epoch": 4.492063492063492,
      "grad_norm": 0.04989177733659744,
      "learning_rate": 5.161290322580646e-07,
      "loss": 0.4612,
      "step": 283
    },
    {
      "epoch": 4.507936507936508,
      "grad_norm": 0.047107845544815063,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.4123,
      "step": 284
    },
    {
      "epoch": 4.523809523809524,
      "grad_norm": 0.051561418920755386,
      "learning_rate": 4.838709677419355e-07,
      "loss": 0.4458,
      "step": 285
    },
    {
      "epoch": 4.5396825396825395,
      "grad_norm": 0.05569900944828987,
      "learning_rate": 4.67741935483871e-07,
      "loss": 0.5291,
      "step": 286
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.04794807359576225,
      "learning_rate": 4.5161290322580644e-07,
      "loss": 0.4459,
      "step": 287
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.05013132095336914,
      "learning_rate": 4.3548387096774196e-07,
      "loss": 0.4725,
      "step": 288
    },
    {
      "epoch": 4.587301587301587,
      "grad_norm": 0.05080787464976311,
      "learning_rate": 4.193548387096775e-07,
      "loss": 0.4829,
      "step": 289
    },
    {
      "epoch": 4.603174603174603,
      "grad_norm": 0.048681654036045074,
      "learning_rate": 4.032258064516129e-07,
      "loss": 0.4418,
      "step": 290
    },
    {
      "epoch": 4.619047619047619,
      "grad_norm": 0.045249093323946,
      "learning_rate": 3.870967741935484e-07,
      "loss": 0.3889,
      "step": 291
    },
    {
      "epoch": 4.634920634920634,
      "grad_norm": 0.050585292279720306,
      "learning_rate": 3.709677419354839e-07,
      "loss": 0.487,
      "step": 292
    },
    {
      "epoch": 4.650793650793651,
      "grad_norm": 0.04671139270067215,
      "learning_rate": 3.548387096774194e-07,
      "loss": 0.4589,
      "step": 293
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.053844258189201355,
      "learning_rate": 3.387096774193549e-07,
      "loss": 0.5102,
      "step": 294
    },
    {
      "epoch": 4.682539682539683,
      "grad_norm": 0.05043618753552437,
      "learning_rate": 3.2258064516129035e-07,
      "loss": 0.4473,
      "step": 295
    },
    {
      "epoch": 4.698412698412699,
      "grad_norm": 0.047002654522657394,
      "learning_rate": 3.064516129032258e-07,
      "loss": 0.4074,
      "step": 296
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 0.04783378541469574,
      "learning_rate": 2.9032258064516134e-07,
      "loss": 0.4637,
      "step": 297
    },
    {
      "epoch": 4.73015873015873,
      "grad_norm": 0.0465601347386837,
      "learning_rate": 2.741935483870968e-07,
      "loss": 0.3952,
      "step": 298
    },
    {
      "epoch": 4.746031746031746,
      "grad_norm": 0.05354093760251999,
      "learning_rate": 2.580645161290323e-07,
      "loss": 0.5275,
      "step": 299
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 0.05296767130494118,
      "learning_rate": 2.4193548387096775e-07,
      "loss": 0.5103,
      "step": 300
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 0.05087999999523163,
      "learning_rate": 2.2580645161290322e-07,
      "loss": 0.4988,
      "step": 301
    },
    {
      "epoch": 4.7936507936507935,
      "grad_norm": 0.046965114772319794,
      "learning_rate": 2.0967741935483874e-07,
      "loss": 0.44,
      "step": 302
    },
    {
      "epoch": 4.809523809523809,
      "grad_norm": 0.05283587798476219,
      "learning_rate": 1.935483870967742e-07,
      "loss": 0.5618,
      "step": 303
    },
    {
      "epoch": 4.825396825396825,
      "grad_norm": 0.0497298501431942,
      "learning_rate": 1.774193548387097e-07,
      "loss": 0.5081,
      "step": 304
    },
    {
      "epoch": 4.841269841269841,
      "grad_norm": 0.047422777861356735,
      "learning_rate": 1.6129032258064518e-07,
      "loss": 0.4963,
      "step": 305
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.05550041422247887,
      "learning_rate": 1.4516129032258067e-07,
      "loss": 0.5851,
      "step": 306
    },
    {
      "epoch": 4.8730158730158735,
      "grad_norm": 0.04824543744325638,
      "learning_rate": 1.2903225806451614e-07,
      "loss": 0.4381,
      "step": 307
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.0502413734793663,
      "learning_rate": 1.1290322580645161e-07,
      "loss": 0.4936,
      "step": 308
    },
    {
      "epoch": 4.904761904761905,
      "grad_norm": 0.054817602038383484,
      "learning_rate": 9.67741935483871e-08,
      "loss": 0.5737,
      "step": 309
    },
    {
      "epoch": 4.920634920634921,
      "grad_norm": 0.0463951900601387,
      "learning_rate": 8.064516129032259e-08,
      "loss": 0.4391,
      "step": 310
    },
    {
      "epoch": 4.936507936507937,
      "grad_norm": 0.05024459585547447,
      "learning_rate": 6.451612903225807e-08,
      "loss": 0.5263,
      "step": 311
    },
    {
      "epoch": 4.9523809523809526,
      "grad_norm": 0.048081230372190475,
      "learning_rate": 4.838709677419355e-08,
      "loss": 0.4771,
      "step": 312
    },
    {
      "epoch": 4.968253968253968,
      "grad_norm": 0.050400298088788986,
      "learning_rate": 3.2258064516129035e-08,
      "loss": 0.4699,
      "step": 313
    },
    {
      "epoch": 4.984126984126984,
      "grad_norm": 0.056380968540906906,
      "learning_rate": 1.6129032258064518e-08,
      "loss": 0.4896,
      "step": 314
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.055724967271089554,
      "learning_rate": 0.0,
      "loss": 0.418,
      "step": 315
    }
  ],
  "logging_steps": 1,
  "max_steps": 315,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.105903549739745e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
