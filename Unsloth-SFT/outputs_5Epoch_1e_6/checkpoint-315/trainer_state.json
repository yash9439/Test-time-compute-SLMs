{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 315,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 0.0757729709148407,
      "learning_rate": 2e-07,
      "loss": 0.5031,
      "step": 1
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 0.10456769913434982,
      "learning_rate": 4e-07,
      "loss": 0.6923,
      "step": 2
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.09110363572835922,
      "learning_rate": 6e-07,
      "loss": 0.6222,
      "step": 3
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 0.07148061692714691,
      "learning_rate": 8e-07,
      "loss": 0.4547,
      "step": 4
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 0.08196887373924255,
      "learning_rate": 1e-06,
      "loss": 0.5138,
      "step": 5
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.08226881921291351,
      "learning_rate": 9.967741935483871e-07,
      "loss": 0.5084,
      "step": 6
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.07659178227186203,
      "learning_rate": 9.935483870967741e-07,
      "loss": 0.4691,
      "step": 7
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 0.09141986817121506,
      "learning_rate": 9.903225806451613e-07,
      "loss": 0.6249,
      "step": 8
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.0856509581208229,
      "learning_rate": 9.870967741935483e-07,
      "loss": 0.6093,
      "step": 9
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 0.07768163084983826,
      "learning_rate": 9.838709677419355e-07,
      "loss": 0.4413,
      "step": 10
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 0.07685583829879761,
      "learning_rate": 9.806451612903225e-07,
      "loss": 0.4731,
      "step": 11
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.07484421133995056,
      "learning_rate": 9.774193548387096e-07,
      "loss": 0.5159,
      "step": 12
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 0.08033383637666702,
      "learning_rate": 9.741935483870968e-07,
      "loss": 0.4958,
      "step": 13
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.08659695088863373,
      "learning_rate": 9.709677419354838e-07,
      "loss": 0.5362,
      "step": 14
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.08658531308174133,
      "learning_rate": 9.67741935483871e-07,
      "loss": 0.5698,
      "step": 15
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 0.08689288794994354,
      "learning_rate": 9.64516129032258e-07,
      "loss": 0.5493,
      "step": 16
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 0.08400114625692368,
      "learning_rate": 9.612903225806452e-07,
      "loss": 0.5257,
      "step": 17
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.08114093542098999,
      "learning_rate": 9.580645161290321e-07,
      "loss": 0.5498,
      "step": 18
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 0.08313405513763428,
      "learning_rate": 9.548387096774193e-07,
      "loss": 0.5774,
      "step": 19
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 0.09317318350076675,
      "learning_rate": 9.516129032258064e-07,
      "loss": 0.6374,
      "step": 20
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.08417226374149323,
      "learning_rate": 9.483870967741935e-07,
      "loss": 0.6104,
      "step": 21
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 0.07600488513708115,
      "learning_rate": 9.451612903225806e-07,
      "loss": 0.4512,
      "step": 22
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 0.08776123821735382,
      "learning_rate": 9.419354838709677e-07,
      "loss": 0.588,
      "step": 23
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.0898309051990509,
      "learning_rate": 9.387096774193549e-07,
      "loss": 0.5277,
      "step": 24
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 0.09983713924884796,
      "learning_rate": 9.354838709677418e-07,
      "loss": 0.5874,
      "step": 25
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 0.08795753121376038,
      "learning_rate": 9.32258064516129e-07,
      "loss": 0.6204,
      "step": 26
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.07946249842643738,
      "learning_rate": 9.290322580645161e-07,
      "loss": 0.512,
      "step": 27
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.08052518218755722,
      "learning_rate": 9.258064516129032e-07,
      "loss": 0.543,
      "step": 28
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 0.07760779559612274,
      "learning_rate": 9.225806451612903e-07,
      "loss": 0.5014,
      "step": 29
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.07868558913469315,
      "learning_rate": 9.193548387096774e-07,
      "loss": 0.5557,
      "step": 30
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 0.08203662931919098,
      "learning_rate": 9.161290322580644e-07,
      "loss": 0.5201,
      "step": 31
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 0.07922776788473129,
      "learning_rate": 9.129032258064516e-07,
      "loss": 0.478,
      "step": 32
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 0.08039364963769913,
      "learning_rate": 9.096774193548387e-07,
      "loss": 0.5204,
      "step": 33
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 0.08696340024471283,
      "learning_rate": 9.064516129032258e-07,
      "loss": 0.5949,
      "step": 34
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.07793542742729187,
      "learning_rate": 9.032258064516129e-07,
      "loss": 0.4732,
      "step": 35
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.08341091126203537,
      "learning_rate": 9e-07,
      "loss": 0.5636,
      "step": 36
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 0.07397718727588654,
      "learning_rate": 8.96774193548387e-07,
      "loss": 0.4504,
      "step": 37
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 0.08283890038728714,
      "learning_rate": 8.935483870967741e-07,
      "loss": 0.5828,
      "step": 38
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 0.08952104300260544,
      "learning_rate": 8.903225806451613e-07,
      "loss": 0.5544,
      "step": 39
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 0.08119018375873566,
      "learning_rate": 8.870967741935483e-07,
      "loss": 0.4745,
      "step": 40
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 0.07265102863311768,
      "learning_rate": 8.838709677419355e-07,
      "loss": 0.5063,
      "step": 41
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.08411093801259995,
      "learning_rate": 8.806451612903226e-07,
      "loss": 0.5379,
      "step": 42
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 0.07413557916879654,
      "learning_rate": 8.774193548387096e-07,
      "loss": 0.4865,
      "step": 43
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 0.09179985523223877,
      "learning_rate": 8.741935483870967e-07,
      "loss": 0.574,
      "step": 44
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.08468130230903625,
      "learning_rate": 8.709677419354838e-07,
      "loss": 0.5764,
      "step": 45
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 0.0786527767777443,
      "learning_rate": 8.67741935483871e-07,
      "loss": 0.4632,
      "step": 46
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 0.08528164029121399,
      "learning_rate": 8.64516129032258e-07,
      "loss": 0.5422,
      "step": 47
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.089035764336586,
      "learning_rate": 8.612903225806452e-07,
      "loss": 0.5403,
      "step": 48
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.0888221338391304,
      "learning_rate": 8.580645161290321e-07,
      "loss": 0.4997,
      "step": 49
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 0.07343018054962158,
      "learning_rate": 8.548387096774193e-07,
      "loss": 0.4568,
      "step": 50
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 0.08530699461698532,
      "learning_rate": 8.516129032258064e-07,
      "loss": 0.4692,
      "step": 51
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 0.07825624942779541,
      "learning_rate": 8.483870967741935e-07,
      "loss": 0.4998,
      "step": 52
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 0.08196321129798889,
      "learning_rate": 8.451612903225807e-07,
      "loss": 0.5526,
      "step": 53
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.08185028284788132,
      "learning_rate": 8.419354838709677e-07,
      "loss": 0.4862,
      "step": 54
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 0.09109409153461456,
      "learning_rate": 8.387096774193549e-07,
      "loss": 0.5687,
      "step": 55
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.08042331039905548,
      "learning_rate": 8.354838709677418e-07,
      "loss": 0.4901,
      "step": 56
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 0.09211284667253494,
      "learning_rate": 8.32258064516129e-07,
      "loss": 0.5436,
      "step": 57
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 0.08030740171670914,
      "learning_rate": 8.29032258064516e-07,
      "loss": 0.5218,
      "step": 58
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 0.09449155628681183,
      "learning_rate": 8.258064516129032e-07,
      "loss": 0.5792,
      "step": 59
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.08227214217185974,
      "learning_rate": 8.225806451612904e-07,
      "loss": 0.5495,
      "step": 60
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 0.08387558907270432,
      "learning_rate": 8.193548387096774e-07,
      "loss": 0.5315,
      "step": 61
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 0.08879406750202179,
      "learning_rate": 8.161290322580645e-07,
      "loss": 0.5266,
      "step": 62
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.11244386434555054,
      "learning_rate": 8.129032258064515e-07,
      "loss": 0.6396,
      "step": 63
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 0.08738645911216736,
      "learning_rate": 8.096774193548387e-07,
      "loss": 0.6127,
      "step": 64
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 0.07960973680019379,
      "learning_rate": 8.064516129032257e-07,
      "loss": 0.5323,
      "step": 65
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.08439847826957703,
      "learning_rate": 8.032258064516129e-07,
      "loss": 0.4942,
      "step": 66
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 0.08320368081331253,
      "learning_rate": 8e-07,
      "loss": 0.4919,
      "step": 67
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 0.09400102496147156,
      "learning_rate": 7.96774193548387e-07,
      "loss": 0.5662,
      "step": 68
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.08304250985383987,
      "learning_rate": 7.935483870967742e-07,
      "loss": 0.5225,
      "step": 69
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.09340748935937881,
      "learning_rate": 7.903225806451612e-07,
      "loss": 0.5933,
      "step": 70
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 0.08105280250310898,
      "learning_rate": 7.870967741935484e-07,
      "loss": 0.4643,
      "step": 71
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.07999370247125626,
      "learning_rate": 7.838709677419354e-07,
      "loss": 0.4806,
      "step": 72
    },
    {
      "epoch": 1.1587301587301586,
      "grad_norm": 0.07949867099523544,
      "learning_rate": 7.806451612903226e-07,
      "loss": 0.5172,
      "step": 73
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 0.07760538160800934,
      "learning_rate": 7.774193548387097e-07,
      "loss": 0.4665,
      "step": 74
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.09118989109992981,
      "learning_rate": 7.741935483870967e-07,
      "loss": 0.5927,
      "step": 75
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 0.08787091076374054,
      "learning_rate": 7.709677419354838e-07,
      "loss": 0.5475,
      "step": 76
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.08178594708442688,
      "learning_rate": 7.677419354838709e-07,
      "loss": 0.5128,
      "step": 77
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 0.08394785225391388,
      "learning_rate": 7.645161290322581e-07,
      "loss": 0.4689,
      "step": 78
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 0.08208559453487396,
      "learning_rate": 7.612903225806451e-07,
      "loss": 0.5051,
      "step": 79
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 0.1013328805565834,
      "learning_rate": 7.580645161290323e-07,
      "loss": 0.6047,
      "step": 80
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.08154819160699844,
      "learning_rate": 7.548387096774193e-07,
      "loss": 0.4808,
      "step": 81
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 0.08476021885871887,
      "learning_rate": 7.516129032258064e-07,
      "loss": 0.5164,
      "step": 82
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 0.0921955406665802,
      "learning_rate": 7.483870967741935e-07,
      "loss": 0.6852,
      "step": 83
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.09494943916797638,
      "learning_rate": 7.451612903225806e-07,
      "loss": 0.5532,
      "step": 84
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 0.07863921672105789,
      "learning_rate": 7.419354838709677e-07,
      "loss": 0.5228,
      "step": 85
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 0.07962135970592499,
      "learning_rate": 7.387096774193549e-07,
      "loss": 0.4741,
      "step": 86
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.08555280417203903,
      "learning_rate": 7.354838709677418e-07,
      "loss": 0.4898,
      "step": 87
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 0.07457301765680313,
      "learning_rate": 7.32258064516129e-07,
      "loss": 0.4251,
      "step": 88
    },
    {
      "epoch": 1.4126984126984126,
      "grad_norm": 0.08074457198381424,
      "learning_rate": 7.290322580645161e-07,
      "loss": 0.5623,
      "step": 89
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.08413195610046387,
      "learning_rate": 7.258064516129032e-07,
      "loss": 0.4939,
      "step": 90
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.0857624039053917,
      "learning_rate": 7.225806451612903e-07,
      "loss": 0.5043,
      "step": 91
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 0.09280700981616974,
      "learning_rate": 7.193548387096774e-07,
      "loss": 0.6689,
      "step": 92
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 0.08328945189714432,
      "learning_rate": 7.161290322580646e-07,
      "loss": 0.4689,
      "step": 93
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 0.0981181189417839,
      "learning_rate": 7.129032258064515e-07,
      "loss": 0.6926,
      "step": 94
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 0.08606013655662537,
      "learning_rate": 7.096774193548387e-07,
      "loss": 0.537,
      "step": 95
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.0915234237909317,
      "learning_rate": 7.064516129032257e-07,
      "loss": 0.5898,
      "step": 96
    },
    {
      "epoch": 1.5396825396825395,
      "grad_norm": 0.09117354452610016,
      "learning_rate": 7.032258064516129e-07,
      "loss": 0.5233,
      "step": 97
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.08228162676095963,
      "learning_rate": 7e-07,
      "loss": 0.5157,
      "step": 98
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.08239168673753738,
      "learning_rate": 6.96774193548387e-07,
      "loss": 0.5363,
      "step": 99
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 0.08169560134410858,
      "learning_rate": 6.935483870967742e-07,
      "loss": 0.4953,
      "step": 100
    },
    {
      "epoch": 1.6031746031746033,
      "grad_norm": 0.07995764911174774,
      "learning_rate": 6.903225806451612e-07,
      "loss": 0.5055,
      "step": 101
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 0.09108133614063263,
      "learning_rate": 6.870967741935484e-07,
      "loss": 0.5402,
      "step": 102
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 0.08442852646112442,
      "learning_rate": 6.838709677419354e-07,
      "loss": 0.5359,
      "step": 103
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 0.07831661403179169,
      "learning_rate": 6.806451612903226e-07,
      "loss": 0.4851,
      "step": 104
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.09150158613920212,
      "learning_rate": 6.774193548387096e-07,
      "loss": 0.5672,
      "step": 105
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 0.08789295703172684,
      "learning_rate": 6.741935483870967e-07,
      "loss": 0.5978,
      "step": 106
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 0.08693278580904007,
      "learning_rate": 6.709677419354839e-07,
      "loss": 0.5129,
      "step": 107
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.0815759152173996,
      "learning_rate": 6.677419354838709e-07,
      "loss": 0.5072,
      "step": 108
    },
    {
      "epoch": 1.7301587301587302,
      "grad_norm": 0.08969885110855103,
      "learning_rate": 6.645161290322581e-07,
      "loss": 0.5361,
      "step": 109
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 0.08935802429914474,
      "learning_rate": 6.612903225806451e-07,
      "loss": 0.5845,
      "step": 110
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 0.0792463943362236,
      "learning_rate": 6.580645161290323e-07,
      "loss": 0.5027,
      "step": 111
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.0818902924656868,
      "learning_rate": 6.548387096774192e-07,
      "loss": 0.5449,
      "step": 112
    },
    {
      "epoch": 1.7936507936507935,
      "grad_norm": 0.08415591716766357,
      "learning_rate": 6.516129032258064e-07,
      "loss": 0.5573,
      "step": 113
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 0.08636344969272614,
      "learning_rate": 6.483870967741935e-07,
      "loss": 0.5573,
      "step": 114
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 0.08106192946434021,
      "learning_rate": 6.451612903225806e-07,
      "loss": 0.5299,
      "step": 115
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 0.0868695005774498,
      "learning_rate": 6.419354838709678e-07,
      "loss": 0.5061,
      "step": 116
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.08719395101070404,
      "learning_rate": 6.387096774193548e-07,
      "loss": 0.5128,
      "step": 117
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 0.08886736631393433,
      "learning_rate": 6.35483870967742e-07,
      "loss": 0.5319,
      "step": 118
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.08199955523014069,
      "learning_rate": 6.322580645161289e-07,
      "loss": 0.538,
      "step": 119
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 0.08452656865119934,
      "learning_rate": 6.290322580645161e-07,
      "loss": 0.5754,
      "step": 120
    },
    {
      "epoch": 1.9206349206349205,
      "grad_norm": 0.08465896546840668,
      "learning_rate": 6.258064516129032e-07,
      "loss": 0.5442,
      "step": 121
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 0.07655840367078781,
      "learning_rate": 6.225806451612903e-07,
      "loss": 0.4643,
      "step": 122
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 0.0806647390127182,
      "learning_rate": 6.193548387096774e-07,
      "loss": 0.5146,
      "step": 123
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 0.09273950755596161,
      "learning_rate": 6.161290322580645e-07,
      "loss": 0.6346,
      "step": 124
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 0.07753252983093262,
      "learning_rate": 6.129032258064516e-07,
      "loss": 0.4649,
      "step": 125
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.08590662479400635,
      "learning_rate": 6.096774193548386e-07,
      "loss": 0.4853,
      "step": 126
    },
    {
      "epoch": 2.015873015873016,
      "grad_norm": 0.07864192128181458,
      "learning_rate": 6.064516129032258e-07,
      "loss": 0.4847,
      "step": 127
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 0.08579979091882706,
      "learning_rate": 6.032258064516129e-07,
      "loss": 0.5814,
      "step": 128
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 0.09304977208375931,
      "learning_rate": 6e-07,
      "loss": 0.6298,
      "step": 129
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 0.0832812562584877,
      "learning_rate": 5.967741935483871e-07,
      "loss": 0.5247,
      "step": 130
    },
    {
      "epoch": 2.0793650793650795,
      "grad_norm": 0.09287507086992264,
      "learning_rate": 5.935483870967741e-07,
      "loss": 0.592,
      "step": 131
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.08418600261211395,
      "learning_rate": 5.903225806451612e-07,
      "loss": 0.5906,
      "step": 132
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.08339300006628036,
      "learning_rate": 5.870967741935483e-07,
      "loss": 0.5163,
      "step": 133
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 0.07640338689088821,
      "learning_rate": 5.838709677419355e-07,
      "loss": 0.518,
      "step": 134
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.0829896405339241,
      "learning_rate": 5.806451612903226e-07,
      "loss": 0.4647,
      "step": 135
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 0.08541285991668701,
      "learning_rate": 5.774193548387097e-07,
      "loss": 0.5867,
      "step": 136
    },
    {
      "epoch": 2.1746031746031744,
      "grad_norm": 0.0885622650384903,
      "learning_rate": 5.741935483870967e-07,
      "loss": 0.4669,
      "step": 137
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.08107717335224152,
      "learning_rate": 5.709677419354838e-07,
      "loss": 0.4644,
      "step": 138
    },
    {
      "epoch": 2.2063492063492065,
      "grad_norm": 0.08812133967876434,
      "learning_rate": 5.677419354838709e-07,
      "loss": 0.5406,
      "step": 139
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.09594618529081345,
      "learning_rate": 5.645161290322581e-07,
      "loss": 0.598,
      "step": 140
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 0.09359584748744965,
      "learning_rate": 5.612903225806451e-07,
      "loss": 0.56,
      "step": 141
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 0.08304886519908905,
      "learning_rate": 5.580645161290323e-07,
      "loss": 0.5474,
      "step": 142
    },
    {
      "epoch": 2.2698412698412698,
      "grad_norm": 0.08994074165821075,
      "learning_rate": 5.548387096774194e-07,
      "loss": 0.5285,
      "step": 143
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.08382880687713623,
      "learning_rate": 5.516129032258064e-07,
      "loss": 0.5398,
      "step": 144
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 0.0813802182674408,
      "learning_rate": 5.483870967741935e-07,
      "loss": 0.4983,
      "step": 145
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 0.09716761112213135,
      "learning_rate": 5.451612903225806e-07,
      "loss": 0.6396,
      "step": 146
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.08052005618810654,
      "learning_rate": 5.419354838709678e-07,
      "loss": 0.5171,
      "step": 147
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 0.07759924978017807,
      "learning_rate": 5.387096774193548e-07,
      "loss": 0.475,
      "step": 148
    },
    {
      "epoch": 2.365079365079365,
      "grad_norm": 0.07657031714916229,
      "learning_rate": 5.35483870967742e-07,
      "loss": 0.4891,
      "step": 149
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.08939190208911896,
      "learning_rate": 5.322580645161289e-07,
      "loss": 0.6041,
      "step": 150
    },
    {
      "epoch": 2.3968253968253967,
      "grad_norm": 0.08703561872243881,
      "learning_rate": 5.290322580645161e-07,
      "loss": 0.5537,
      "step": 151
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 0.08516939729452133,
      "learning_rate": 5.258064516129032e-07,
      "loss": 0.4907,
      "step": 152
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.08598913252353668,
      "learning_rate": 5.225806451612903e-07,
      "loss": 0.5101,
      "step": 153
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.08451271802186966,
      "learning_rate": 5.193548387096775e-07,
      "loss": 0.5439,
      "step": 154
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 0.07589144259691238,
      "learning_rate": 5.161290322580645e-07,
      "loss": 0.4212,
      "step": 155
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.08388065546751022,
      "learning_rate": 5.129032258064516e-07,
      "loss": 0.5216,
      "step": 156
    },
    {
      "epoch": 2.492063492063492,
      "grad_norm": 0.08285939693450928,
      "learning_rate": 5.096774193548386e-07,
      "loss": 0.5167,
      "step": 157
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 0.08835066854953766,
      "learning_rate": 5.064516129032258e-07,
      "loss": 0.5346,
      "step": 158
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 0.08010625839233398,
      "learning_rate": 5.032258064516128e-07,
      "loss": 0.4938,
      "step": 159
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 0.09586449712514877,
      "learning_rate": 5e-07,
      "loss": 0.5849,
      "step": 160
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.08600971847772598,
      "learning_rate": 4.967741935483871e-07,
      "loss": 0.5264,
      "step": 161
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.07991641759872437,
      "learning_rate": 4.935483870967741e-07,
      "loss": 0.447,
      "step": 162
    },
    {
      "epoch": 2.5873015873015874,
      "grad_norm": 0.10119625180959702,
      "learning_rate": 4.903225806451612e-07,
      "loss": 0.6385,
      "step": 163
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 0.08190969377756119,
      "learning_rate": 4.870967741935484e-07,
      "loss": 0.5537,
      "step": 164
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 0.09024769067764282,
      "learning_rate": 4.838709677419355e-07,
      "loss": 0.5267,
      "step": 165
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 0.09173857420682907,
      "learning_rate": 4.806451612903226e-07,
      "loss": 0.6221,
      "step": 166
    },
    {
      "epoch": 2.6507936507936507,
      "grad_norm": 0.08770377933979034,
      "learning_rate": 4.774193548387097e-07,
      "loss": 0.5045,
      "step": 167
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.08012720197439194,
      "learning_rate": 4.7419354838709675e-07,
      "loss": 0.4667,
      "step": 168
    },
    {
      "epoch": 2.682539682539683,
      "grad_norm": 0.08056364208459854,
      "learning_rate": 4.7096774193548383e-07,
      "loss": 0.5508,
      "step": 169
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 0.08343643695116043,
      "learning_rate": 4.677419354838709e-07,
      "loss": 0.5453,
      "step": 170
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 0.08560929447412491,
      "learning_rate": 4.6451612903225805e-07,
      "loss": 0.5325,
      "step": 171
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 0.07640734314918518,
      "learning_rate": 4.6129032258064514e-07,
      "loss": 0.4275,
      "step": 172
    },
    {
      "epoch": 2.746031746031746,
      "grad_norm": 0.08584076911211014,
      "learning_rate": 4.580645161290322e-07,
      "loss": 0.5599,
      "step": 173
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.07741207629442215,
      "learning_rate": 4.5483870967741935e-07,
      "loss": 0.4692,
      "step": 174
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.07530573010444641,
      "learning_rate": 4.5161290322580644e-07,
      "loss": 0.448,
      "step": 175
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 0.08618810772895813,
      "learning_rate": 4.483870967741935e-07,
      "loss": 0.5008,
      "step": 176
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 0.08163503557443619,
      "learning_rate": 4.4516129032258066e-07,
      "loss": 0.483,
      "step": 177
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 0.09971731901168823,
      "learning_rate": 4.4193548387096774e-07,
      "loss": 0.6507,
      "step": 178
    },
    {
      "epoch": 2.8412698412698414,
      "grad_norm": 0.08772981911897659,
      "learning_rate": 4.387096774193548e-07,
      "loss": 0.5364,
      "step": 179
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.0871661901473999,
      "learning_rate": 4.354838709677419e-07,
      "loss": 0.5617,
      "step": 180
    },
    {
      "epoch": 2.873015873015873,
      "grad_norm": 0.09552024304866791,
      "learning_rate": 4.32258064516129e-07,
      "loss": 0.5746,
      "step": 181
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.09882204234600067,
      "learning_rate": 4.290322580645161e-07,
      "loss": 0.6932,
      "step": 182
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 0.08041910082101822,
      "learning_rate": 4.258064516129032e-07,
      "loss": 0.4505,
      "step": 183
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 0.09409993141889572,
      "learning_rate": 4.2258064516129035e-07,
      "loss": 0.6339,
      "step": 184
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 0.08418773859739304,
      "learning_rate": 4.1935483870967743e-07,
      "loss": 0.5427,
      "step": 185
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.07438014447689056,
      "learning_rate": 4.161290322580645e-07,
      "loss": 0.4225,
      "step": 186
    },
    {
      "epoch": 2.9682539682539684,
      "grad_norm": 0.08232180029153824,
      "learning_rate": 4.129032258064516e-07,
      "loss": 0.4766,
      "step": 187
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 0.08362004905939102,
      "learning_rate": 4.096774193548387e-07,
      "loss": 0.4789,
      "step": 188
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.09635183215141296,
      "learning_rate": 4.0645161290322576e-07,
      "loss": 0.6035,
      "step": 189
    },
    {
      "epoch": 3.015873015873016,
      "grad_norm": 0.07732747495174408,
      "learning_rate": 4.0322580645161285e-07,
      "loss": 0.4945,
      "step": 190
    },
    {
      "epoch": 3.0317460317460316,
      "grad_norm": 0.07986941933631897,
      "learning_rate": 4e-07,
      "loss": 0.4878,
      "step": 191
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 0.07760170102119446,
      "learning_rate": 3.967741935483871e-07,
      "loss": 0.4594,
      "step": 192
    },
    {
      "epoch": 3.0634920634920633,
      "grad_norm": 0.08767224848270416,
      "learning_rate": 3.935483870967742e-07,
      "loss": 0.5395,
      "step": 193
    },
    {
      "epoch": 3.0793650793650795,
      "grad_norm": 0.08248447626829147,
      "learning_rate": 3.903225806451613e-07,
      "loss": 0.4668,
      "step": 194
    },
    {
      "epoch": 3.0952380952380953,
      "grad_norm": 0.08735333383083344,
      "learning_rate": 3.8709677419354837e-07,
      "loss": 0.5531,
      "step": 195
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.07877091318368912,
      "learning_rate": 3.8387096774193545e-07,
      "loss": 0.4679,
      "step": 196
    },
    {
      "epoch": 3.126984126984127,
      "grad_norm": 0.08084172755479813,
      "learning_rate": 3.8064516129032253e-07,
      "loss": 0.5139,
      "step": 197
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.08847403526306152,
      "learning_rate": 3.7741935483870967e-07,
      "loss": 0.539,
      "step": 198
    },
    {
      "epoch": 3.1587301587301586,
      "grad_norm": 0.08280888199806213,
      "learning_rate": 3.7419354838709675e-07,
      "loss": 0.5443,
      "step": 199
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 0.08607553690671921,
      "learning_rate": 3.7096774193548384e-07,
      "loss": 0.5203,
      "step": 200
    },
    {
      "epoch": 3.1904761904761907,
      "grad_norm": 0.09689563512802124,
      "learning_rate": 3.677419354838709e-07,
      "loss": 0.6706,
      "step": 201
    },
    {
      "epoch": 3.2063492063492065,
      "grad_norm": 0.08297910541296005,
      "learning_rate": 3.6451612903225806e-07,
      "loss": 0.4742,
      "step": 202
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.08823957294225693,
      "learning_rate": 3.6129032258064514e-07,
      "loss": 0.5279,
      "step": 203
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 0.0789642482995987,
      "learning_rate": 3.580645161290323e-07,
      "loss": 0.5186,
      "step": 204
    },
    {
      "epoch": 3.253968253968254,
      "grad_norm": 0.08158791065216064,
      "learning_rate": 3.5483870967741936e-07,
      "loss": 0.5399,
      "step": 205
    },
    {
      "epoch": 3.2698412698412698,
      "grad_norm": 0.0959441065788269,
      "learning_rate": 3.5161290322580644e-07,
      "loss": 0.6135,
      "step": 206
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.09042898565530777,
      "learning_rate": 3.483870967741935e-07,
      "loss": 0.581,
      "step": 207
    },
    {
      "epoch": 3.3015873015873014,
      "grad_norm": 0.08219560980796814,
      "learning_rate": 3.451612903225806e-07,
      "loss": 0.4656,
      "step": 208
    },
    {
      "epoch": 3.317460317460317,
      "grad_norm": 0.08185253292322159,
      "learning_rate": 3.419354838709677e-07,
      "loss": 0.5198,
      "step": 209
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.09117335081100464,
      "learning_rate": 3.387096774193548e-07,
      "loss": 0.5157,
      "step": 210
    },
    {
      "epoch": 3.3492063492063493,
      "grad_norm": 0.07688974589109421,
      "learning_rate": 3.3548387096774196e-07,
      "loss": 0.4701,
      "step": 211
    },
    {
      "epoch": 3.365079365079365,
      "grad_norm": 0.08200626075267792,
      "learning_rate": 3.3225806451612905e-07,
      "loss": 0.4919,
      "step": 212
    },
    {
      "epoch": 3.380952380952381,
      "grad_norm": 0.07922375202178955,
      "learning_rate": 3.2903225806451613e-07,
      "loss": 0.5448,
      "step": 213
    },
    {
      "epoch": 3.3968253968253967,
      "grad_norm": 0.07354777306318283,
      "learning_rate": 3.258064516129032e-07,
      "loss": 0.4631,
      "step": 214
    },
    {
      "epoch": 3.4126984126984126,
      "grad_norm": 0.08582358807325363,
      "learning_rate": 3.225806451612903e-07,
      "loss": 0.6717,
      "step": 215
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 0.09305796027183533,
      "learning_rate": 3.193548387096774e-07,
      "loss": 0.5446,
      "step": 216
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.08914609998464584,
      "learning_rate": 3.1612903225806446e-07,
      "loss": 0.5952,
      "step": 217
    },
    {
      "epoch": 3.4603174603174605,
      "grad_norm": 0.08664758503437042,
      "learning_rate": 3.129032258064516e-07,
      "loss": 0.5206,
      "step": 218
    },
    {
      "epoch": 3.4761904761904763,
      "grad_norm": 0.08052846789360046,
      "learning_rate": 3.096774193548387e-07,
      "loss": 0.5104,
      "step": 219
    },
    {
      "epoch": 3.492063492063492,
      "grad_norm": 0.09087233990430832,
      "learning_rate": 3.064516129032258e-07,
      "loss": 0.523,
      "step": 220
    },
    {
      "epoch": 3.507936507936508,
      "grad_norm": 0.10343293845653534,
      "learning_rate": 3.032258064516129e-07,
      "loss": 0.5894,
      "step": 221
    },
    {
      "epoch": 3.5238095238095237,
      "grad_norm": 0.08057500422000885,
      "learning_rate": 3e-07,
      "loss": 0.5839,
      "step": 222
    },
    {
      "epoch": 3.5396825396825395,
      "grad_norm": 0.08007200807332993,
      "learning_rate": 2.9677419354838707e-07,
      "loss": 0.5169,
      "step": 223
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.08124635368585587,
      "learning_rate": 2.9354838709677415e-07,
      "loss": 0.4878,
      "step": 224
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 0.09853457659482956,
      "learning_rate": 2.903225806451613e-07,
      "loss": 0.5929,
      "step": 225
    },
    {
      "epoch": 3.5873015873015874,
      "grad_norm": 0.08125443011522293,
      "learning_rate": 2.8709677419354837e-07,
      "loss": 0.5007,
      "step": 226
    },
    {
      "epoch": 3.6031746031746033,
      "grad_norm": 0.08414146304130554,
      "learning_rate": 2.8387096774193546e-07,
      "loss": 0.5247,
      "step": 227
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 0.08299839496612549,
      "learning_rate": 2.8064516129032254e-07,
      "loss": 0.5069,
      "step": 228
    },
    {
      "epoch": 3.634920634920635,
      "grad_norm": 0.08960200101137161,
      "learning_rate": 2.774193548387097e-07,
      "loss": 0.5533,
      "step": 229
    },
    {
      "epoch": 3.6507936507936507,
      "grad_norm": 0.08258480578660965,
      "learning_rate": 2.7419354838709676e-07,
      "loss": 0.513,
      "step": 230
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.09712333232164383,
      "learning_rate": 2.709677419354839e-07,
      "loss": 0.6458,
      "step": 231
    },
    {
      "epoch": 3.682539682539683,
      "grad_norm": 0.07433126866817474,
      "learning_rate": 2.67741935483871e-07,
      "loss": 0.4738,
      "step": 232
    },
    {
      "epoch": 3.6984126984126986,
      "grad_norm": 0.07761737704277039,
      "learning_rate": 2.6451612903225806e-07,
      "loss": 0.54,
      "step": 233
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 0.08521576970815659,
      "learning_rate": 2.6129032258064514e-07,
      "loss": 0.5432,
      "step": 234
    },
    {
      "epoch": 3.7301587301587302,
      "grad_norm": 0.08348704874515533,
      "learning_rate": 2.5806451612903223e-07,
      "loss": 0.5024,
      "step": 235
    },
    {
      "epoch": 3.746031746031746,
      "grad_norm": 0.08777260035276413,
      "learning_rate": 2.548387096774193e-07,
      "loss": 0.5599,
      "step": 236
    },
    {
      "epoch": 3.761904761904762,
      "grad_norm": 0.08164726942777634,
      "learning_rate": 2.516129032258064e-07,
      "loss": 0.5429,
      "step": 237
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.08122071623802185,
      "learning_rate": 2.4838709677419353e-07,
      "loss": 0.5171,
      "step": 238
    },
    {
      "epoch": 3.7936507936507935,
      "grad_norm": 0.08103279024362564,
      "learning_rate": 2.451612903225806e-07,
      "loss": 0.4725,
      "step": 239
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 0.08125236630439758,
      "learning_rate": 2.4193548387096775e-07,
      "loss": 0.5171,
      "step": 240
    },
    {
      "epoch": 3.825396825396825,
      "grad_norm": 0.0900782123208046,
      "learning_rate": 2.3870967741935483e-07,
      "loss": 0.6598,
      "step": 241
    },
    {
      "epoch": 3.8412698412698414,
      "grad_norm": 0.08031485974788666,
      "learning_rate": 2.3548387096774192e-07,
      "loss": 0.4903,
      "step": 242
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 0.08824196457862854,
      "learning_rate": 2.3225806451612903e-07,
      "loss": 0.5095,
      "step": 243
    },
    {
      "epoch": 3.873015873015873,
      "grad_norm": 0.08657051622867584,
      "learning_rate": 2.290322580645161e-07,
      "loss": 0.4661,
      "step": 244
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.08848729729652405,
      "learning_rate": 2.2580645161290322e-07,
      "loss": 0.5268,
      "step": 245
    },
    {
      "epoch": 3.9047619047619047,
      "grad_norm": 0.08728305250406265,
      "learning_rate": 2.2258064516129033e-07,
      "loss": 0.538,
      "step": 246
    },
    {
      "epoch": 3.9206349206349205,
      "grad_norm": 0.07426198571920395,
      "learning_rate": 2.193548387096774e-07,
      "loss": 0.4589,
      "step": 247
    },
    {
      "epoch": 3.9365079365079367,
      "grad_norm": 0.07997578382492065,
      "learning_rate": 2.161290322580645e-07,
      "loss": 0.5395,
      "step": 248
    },
    {
      "epoch": 3.9523809523809526,
      "grad_norm": 0.07566525042057037,
      "learning_rate": 2.129032258064516e-07,
      "loss": 0.4385,
      "step": 249
    },
    {
      "epoch": 3.9682539682539684,
      "grad_norm": 0.08315140008926392,
      "learning_rate": 2.0967741935483871e-07,
      "loss": 0.5792,
      "step": 250
    },
    {
      "epoch": 3.984126984126984,
      "grad_norm": 0.09719687700271606,
      "learning_rate": 2.064516129032258e-07,
      "loss": 0.5615,
      "step": 251
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.11043065041303635,
      "learning_rate": 2.0322580645161288e-07,
      "loss": 0.6918,
      "step": 252
    },
    {
      "epoch": 4.015873015873016,
      "grad_norm": 0.09006559103727341,
      "learning_rate": 2e-07,
      "loss": 0.6058,
      "step": 253
    },
    {
      "epoch": 4.031746031746032,
      "grad_norm": 0.09003575146198273,
      "learning_rate": 1.967741935483871e-07,
      "loss": 0.5784,
      "step": 254
    },
    {
      "epoch": 4.0476190476190474,
      "grad_norm": 0.08160466700792313,
      "learning_rate": 1.9354838709677418e-07,
      "loss": 0.4708,
      "step": 255
    },
    {
      "epoch": 4.063492063492063,
      "grad_norm": 0.09012667089700699,
      "learning_rate": 1.9032258064516127e-07,
      "loss": 0.5663,
      "step": 256
    },
    {
      "epoch": 4.079365079365079,
      "grad_norm": 0.08205915987491608,
      "learning_rate": 1.8709677419354838e-07,
      "loss": 0.5174,
      "step": 257
    },
    {
      "epoch": 4.095238095238095,
      "grad_norm": 0.07983853667974472,
      "learning_rate": 1.8387096774193546e-07,
      "loss": 0.5231,
      "step": 258
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.07866284996271133,
      "learning_rate": 1.8064516129032257e-07,
      "loss": 0.5004,
      "step": 259
    },
    {
      "epoch": 4.1269841269841265,
      "grad_norm": 0.0874229297041893,
      "learning_rate": 1.7741935483870968e-07,
      "loss": 0.5436,
      "step": 260
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 0.08393213152885437,
      "learning_rate": 1.7419354838709676e-07,
      "loss": 0.5203,
      "step": 261
    },
    {
      "epoch": 4.158730158730159,
      "grad_norm": 0.0870572179555893,
      "learning_rate": 1.7096774193548385e-07,
      "loss": 0.6089,
      "step": 262
    },
    {
      "epoch": 4.174603174603175,
      "grad_norm": 0.08422065526247025,
      "learning_rate": 1.6774193548387098e-07,
      "loss": 0.5379,
      "step": 263
    },
    {
      "epoch": 4.190476190476191,
      "grad_norm": 0.09765549004077911,
      "learning_rate": 1.6451612903225807e-07,
      "loss": 0.568,
      "step": 264
    },
    {
      "epoch": 4.2063492063492065,
      "grad_norm": 0.08022619038820267,
      "learning_rate": 1.6129032258064515e-07,
      "loss": 0.4828,
      "step": 265
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.09055925905704498,
      "learning_rate": 1.5806451612903223e-07,
      "loss": 0.5551,
      "step": 266
    },
    {
      "epoch": 4.238095238095238,
      "grad_norm": 0.07803313434123993,
      "learning_rate": 1.5483870967741934e-07,
      "loss": 0.4894,
      "step": 267
    },
    {
      "epoch": 4.253968253968254,
      "grad_norm": 0.09245108813047409,
      "learning_rate": 1.5161290322580645e-07,
      "loss": 0.697,
      "step": 268
    },
    {
      "epoch": 4.26984126984127,
      "grad_norm": 0.09163392335176468,
      "learning_rate": 1.4838709677419353e-07,
      "loss": 0.5852,
      "step": 269
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.08013242483139038,
      "learning_rate": 1.4516129032258064e-07,
      "loss": 0.495,
      "step": 270
    },
    {
      "epoch": 4.301587301587301,
      "grad_norm": 0.08475815504789352,
      "learning_rate": 1.4193548387096773e-07,
      "loss": 0.549,
      "step": 271
    },
    {
      "epoch": 4.317460317460317,
      "grad_norm": 0.08085551112890244,
      "learning_rate": 1.3870967741935484e-07,
      "loss": 0.5217,
      "step": 272
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.07837216556072235,
      "learning_rate": 1.3548387096774195e-07,
      "loss": 0.4605,
      "step": 273
    },
    {
      "epoch": 4.349206349206349,
      "grad_norm": 0.0871390774846077,
      "learning_rate": 1.3225806451612903e-07,
      "loss": 0.5951,
      "step": 274
    },
    {
      "epoch": 4.365079365079365,
      "grad_norm": 0.08364333212375641,
      "learning_rate": 1.2903225806451611e-07,
      "loss": 0.5473,
      "step": 275
    },
    {
      "epoch": 4.380952380952381,
      "grad_norm": 0.0856563150882721,
      "learning_rate": 1.258064516129032e-07,
      "loss": 0.5722,
      "step": 276
    },
    {
      "epoch": 4.396825396825397,
      "grad_norm": 0.08824635297060013,
      "learning_rate": 1.225806451612903e-07,
      "loss": 0.5236,
      "step": 277
    },
    {
      "epoch": 4.412698412698413,
      "grad_norm": 0.0900762602686882,
      "learning_rate": 1.1935483870967742e-07,
      "loss": 0.5553,
      "step": 278
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 0.0769597738981247,
      "learning_rate": 1.1612903225806451e-07,
      "loss": 0.4816,
      "step": 279
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.07610645890235901,
      "learning_rate": 1.1290322580645161e-07,
      "loss": 0.5188,
      "step": 280
    },
    {
      "epoch": 4.4603174603174605,
      "grad_norm": 0.08192720264196396,
      "learning_rate": 1.096774193548387e-07,
      "loss": 0.4994,
      "step": 281
    },
    {
      "epoch": 4.476190476190476,
      "grad_norm": 0.09101567417383194,
      "learning_rate": 1.064516129032258e-07,
      "loss": 0.5393,
      "step": 282
    },
    {
      "epoch": 4.492063492063492,
      "grad_norm": 0.07845370471477509,
      "learning_rate": 1.032258064516129e-07,
      "loss": 0.4993,
      "step": 283
    },
    {
      "epoch": 4.507936507936508,
      "grad_norm": 0.08410011231899261,
      "learning_rate": 1e-07,
      "loss": 0.452,
      "step": 284
    },
    {
      "epoch": 4.523809523809524,
      "grad_norm": 0.08315154910087585,
      "learning_rate": 9.677419354838709e-08,
      "loss": 0.4881,
      "step": 285
    },
    {
      "epoch": 4.5396825396825395,
      "grad_norm": 0.09647907316684723,
      "learning_rate": 9.354838709677419e-08,
      "loss": 0.5767,
      "step": 286
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.0803913101553917,
      "learning_rate": 9.032258064516128e-08,
      "loss": 0.4834,
      "step": 287
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.08565288037061691,
      "learning_rate": 8.709677419354838e-08,
      "loss": 0.5143,
      "step": 288
    },
    {
      "epoch": 4.587301587301587,
      "grad_norm": 0.08166325092315674,
      "learning_rate": 8.387096774193549e-08,
      "loss": 0.5228,
      "step": 289
    },
    {
      "epoch": 4.603174603174603,
      "grad_norm": 0.08347207307815552,
      "learning_rate": 8.064516129032257e-08,
      "loss": 0.4829,
      "step": 290
    },
    {
      "epoch": 4.619047619047619,
      "grad_norm": 0.07474778592586517,
      "learning_rate": 7.741935483870967e-08,
      "loss": 0.4261,
      "step": 291
    },
    {
      "epoch": 4.634920634920634,
      "grad_norm": 0.08829993009567261,
      "learning_rate": 7.419354838709677e-08,
      "loss": 0.5289,
      "step": 292
    },
    {
      "epoch": 4.650793650793651,
      "grad_norm": 0.07518059015274048,
      "learning_rate": 7.096774193548386e-08,
      "loss": 0.4956,
      "step": 293
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.08653676509857178,
      "learning_rate": 6.774193548387097e-08,
      "loss": 0.5511,
      "step": 294
    },
    {
      "epoch": 4.682539682539683,
      "grad_norm": 0.08319926261901855,
      "learning_rate": 6.451612903225806e-08,
      "loss": 0.4885,
      "step": 295
    },
    {
      "epoch": 4.698412698412699,
      "grad_norm": 0.07483350485563278,
      "learning_rate": 6.129032258064515e-08,
      "loss": 0.445,
      "step": 296
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 0.08378885686397552,
      "learning_rate": 5.8064516129032257e-08,
      "loss": 0.5052,
      "step": 297
    },
    {
      "epoch": 4.73015873015873,
      "grad_norm": 0.07454745471477509,
      "learning_rate": 5.483870967741935e-08,
      "loss": 0.4317,
      "step": 298
    },
    {
      "epoch": 4.746031746031746,
      "grad_norm": 0.09048523753881454,
      "learning_rate": 5.161290322580645e-08,
      "loss": 0.5719,
      "step": 299
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 0.08999642729759216,
      "learning_rate": 4.8387096774193546e-08,
      "loss": 0.553,
      "step": 300
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 0.08892671763896942,
      "learning_rate": 4.516129032258064e-08,
      "loss": 0.5394,
      "step": 301
    },
    {
      "epoch": 4.7936507936507935,
      "grad_norm": 0.08040272444486618,
      "learning_rate": 4.1935483870967746e-08,
      "loss": 0.4784,
      "step": 302
    },
    {
      "epoch": 4.809523809523809,
      "grad_norm": 0.08485914021730423,
      "learning_rate": 3.8709677419354835e-08,
      "loss": 0.6019,
      "step": 303
    },
    {
      "epoch": 4.825396825396825,
      "grad_norm": 0.08255019783973694,
      "learning_rate": 3.548387096774193e-08,
      "loss": 0.5489,
      "step": 304
    },
    {
      "epoch": 4.841269841269841,
      "grad_norm": 0.07957793027162552,
      "learning_rate": 3.225806451612903e-08,
      "loss": 0.5346,
      "step": 305
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.0874810442328453,
      "learning_rate": 2.9032258064516128e-08,
      "loss": 0.6267,
      "step": 306
    },
    {
      "epoch": 4.8730158730158735,
      "grad_norm": 0.08384446799755096,
      "learning_rate": 2.5806451612903225e-08,
      "loss": 0.4797,
      "step": 307
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.07985666394233704,
      "learning_rate": 2.258064516129032e-08,
      "loss": 0.5317,
      "step": 308
    },
    {
      "epoch": 4.904761904761905,
      "grad_norm": 0.09928786754608154,
      "learning_rate": 1.9354838709677418e-08,
      "loss": 0.6199,
      "step": 309
    },
    {
      "epoch": 4.920634920634921,
      "grad_norm": 0.07293644547462463,
      "learning_rate": 1.6129032258064514e-08,
      "loss": 0.4745,
      "step": 310
    },
    {
      "epoch": 4.936507936507937,
      "grad_norm": 0.08174165338277817,
      "learning_rate": 1.2903225806451612e-08,
      "loss": 0.5641,
      "step": 311
    },
    {
      "epoch": 4.9523809523809526,
      "grad_norm": 0.08171974867582321,
      "learning_rate": 9.677419354838709e-09,
      "loss": 0.5164,
      "step": 312
    },
    {
      "epoch": 4.968253968253968,
      "grad_norm": 0.07989053428173065,
      "learning_rate": 6.451612903225806e-09,
      "loss": 0.5104,
      "step": 313
    },
    {
      "epoch": 4.984126984126984,
      "grad_norm": 0.08882681280374527,
      "learning_rate": 3.225806451612903e-09,
      "loss": 0.5349,
      "step": 314
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.07429240643978119,
      "learning_rate": 0.0,
      "loss": 0.453,
      "step": 315
    }
  ],
  "logging_steps": 1,
  "max_steps": 315,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.105903549739745e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
