{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 315,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 0.07576216757297516,
      "learning_rate": 2e-05,
      "loss": 0.5031,
      "step": 1
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 0.10458523780107498,
      "learning_rate": 4e-05,
      "loss": 0.6923,
      "step": 2
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.09113238006830215,
      "learning_rate": 6e-05,
      "loss": 0.6222,
      "step": 3
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 0.07142698019742966,
      "learning_rate": 8e-05,
      "loss": 0.4539,
      "step": 4
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 0.0811820924282074,
      "learning_rate": 0.0001,
      "loss": 0.511,
      "step": 5
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.07810796052217484,
      "learning_rate": 9.967741935483872e-05,
      "loss": 0.5013,
      "step": 6
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.061400171369314194,
      "learning_rate": 9.935483870967742e-05,
      "loss": 0.456,
      "step": 7
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 0.06369384378194809,
      "learning_rate": 9.903225806451614e-05,
      "loss": 0.6032,
      "step": 8
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.06107444316148758,
      "learning_rate": 9.870967741935484e-05,
      "loss": 0.5846,
      "step": 9
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 0.056360356509685516,
      "learning_rate": 9.838709677419355e-05,
      "loss": 0.4131,
      "step": 10
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 0.06684060394763947,
      "learning_rate": 9.806451612903226e-05,
      "loss": 0.4444,
      "step": 11
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.06409868597984314,
      "learning_rate": 9.774193548387098e-05,
      "loss": 0.485,
      "step": 12
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 0.06606119126081467,
      "learning_rate": 9.741935483870968e-05,
      "loss": 0.4589,
      "step": 13
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.06222626939415932,
      "learning_rate": 9.709677419354839e-05,
      "loss": 0.4928,
      "step": 14
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.05633857101202011,
      "learning_rate": 9.677419354838711e-05,
      "loss": 0.5221,
      "step": 15
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 0.05493403226137161,
      "learning_rate": 9.645161290322581e-05,
      "loss": 0.4987,
      "step": 16
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 0.05108742415904999,
      "learning_rate": 9.612903225806452e-05,
      "loss": 0.4739,
      "step": 17
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.055356718599796295,
      "learning_rate": 9.580645161290323e-05,
      "loss": 0.4991,
      "step": 18
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 0.05218052491545677,
      "learning_rate": 9.548387096774195e-05,
      "loss": 0.5223,
      "step": 19
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 0.06082151085138321,
      "learning_rate": 9.516129032258065e-05,
      "loss": 0.5771,
      "step": 20
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.05159289762377739,
      "learning_rate": 9.483870967741936e-05,
      "loss": 0.5529,
      "step": 21
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 0.0480327345430851,
      "learning_rate": 9.451612903225808e-05,
      "loss": 0.3952,
      "step": 22
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 0.05249056965112686,
      "learning_rate": 9.419354838709677e-05,
      "loss": 0.5236,
      "step": 23
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.05197424441576004,
      "learning_rate": 9.387096774193549e-05,
      "loss": 0.4597,
      "step": 24
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 0.05334659293293953,
      "learning_rate": 9.35483870967742e-05,
      "loss": 0.5151,
      "step": 25
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 0.04890645667910576,
      "learning_rate": 9.32258064516129e-05,
      "loss": 0.551,
      "step": 26
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.04354934021830559,
      "learning_rate": 9.290322580645162e-05,
      "loss": 0.4445,
      "step": 27
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.05122864246368408,
      "learning_rate": 9.258064516129033e-05,
      "loss": 0.4768,
      "step": 28
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 0.04557710513472557,
      "learning_rate": 9.225806451612904e-05,
      "loss": 0.4365,
      "step": 29
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.04660245031118393,
      "learning_rate": 9.193548387096774e-05,
      "loss": 0.4907,
      "step": 30
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 0.04610448703169823,
      "learning_rate": 9.161290322580646e-05,
      "loss": 0.4463,
      "step": 31
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 0.043416693806648254,
      "learning_rate": 9.129032258064517e-05,
      "loss": 0.4033,
      "step": 32
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 0.04254554957151413,
      "learning_rate": 9.096774193548387e-05,
      "loss": 0.4487,
      "step": 33
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 0.0465942844748497,
      "learning_rate": 9.06451612903226e-05,
      "loss": 0.5197,
      "step": 34
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.04257497936487198,
      "learning_rate": 9.032258064516129e-05,
      "loss": 0.4002,
      "step": 35
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.046029288321733475,
      "learning_rate": 9e-05,
      "loss": 0.4853,
      "step": 36
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 0.0388987697660923,
      "learning_rate": 8.967741935483871e-05,
      "loss": 0.3807,
      "step": 37
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 0.0457199364900589,
      "learning_rate": 8.935483870967742e-05,
      "loss": 0.5058,
      "step": 38
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 0.04430949687957764,
      "learning_rate": 8.903225806451614e-05,
      "loss": 0.4692,
      "step": 39
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 0.042712897062301636,
      "learning_rate": 8.870967741935484e-05,
      "loss": 0.3954,
      "step": 40
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 0.04097622260451317,
      "learning_rate": 8.838709677419355e-05,
      "loss": 0.4367,
      "step": 41
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.04274159297347069,
      "learning_rate": 8.806451612903226e-05,
      "loss": 0.4563,
      "step": 42
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 0.04115583375096321,
      "learning_rate": 8.774193548387098e-05,
      "loss": 0.41,
      "step": 43
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 0.046439409255981445,
      "learning_rate": 8.741935483870968e-05,
      "loss": 0.4843,
      "step": 44
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.045561470091342926,
      "learning_rate": 8.709677419354839e-05,
      "loss": 0.4901,
      "step": 45
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 0.040198419243097305,
      "learning_rate": 8.677419354838711e-05,
      "loss": 0.3816,
      "step": 46
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 0.04266628623008728,
      "learning_rate": 8.645161290322581e-05,
      "loss": 0.4601,
      "step": 47
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.04766292870044708,
      "learning_rate": 8.612903225806452e-05,
      "loss": 0.4543,
      "step": 48
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.04184215888381004,
      "learning_rate": 8.580645161290323e-05,
      "loss": 0.4094,
      "step": 49
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 0.04404502362012863,
      "learning_rate": 8.548387096774195e-05,
      "loss": 0.3808,
      "step": 50
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 0.04088279977440834,
      "learning_rate": 8.516129032258064e-05,
      "loss": 0.3808,
      "step": 51
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 0.04412391409277916,
      "learning_rate": 8.483870967741936e-05,
      "loss": 0.4179,
      "step": 52
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 0.044802237302064896,
      "learning_rate": 8.451612903225808e-05,
      "loss": 0.4634,
      "step": 53
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.042377300560474396,
      "learning_rate": 8.419354838709677e-05,
      "loss": 0.3992,
      "step": 54
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 0.04717809706926346,
      "learning_rate": 8.387096774193549e-05,
      "loss": 0.4727,
      "step": 55
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.0425918884575367,
      "learning_rate": 8.35483870967742e-05,
      "loss": 0.4062,
      "step": 56
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 0.0462975911796093,
      "learning_rate": 8.32258064516129e-05,
      "loss": 0.4483,
      "step": 57
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 0.04213953763246536,
      "learning_rate": 8.290322580645161e-05,
      "loss": 0.4297,
      "step": 58
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 0.046410974115133286,
      "learning_rate": 8.258064516129033e-05,
      "loss": 0.481,
      "step": 59
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.049969639629125595,
      "learning_rate": 8.225806451612904e-05,
      "loss": 0.4618,
      "step": 60
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 0.048774633556604385,
      "learning_rate": 8.193548387096774e-05,
      "loss": 0.4408,
      "step": 61
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 0.04844663664698601,
      "learning_rate": 8.161290322580646e-05,
      "loss": 0.4318,
      "step": 62
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.06629633158445358,
      "learning_rate": 8.129032258064517e-05,
      "loss": 0.5291,
      "step": 63
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 0.04943076893687248,
      "learning_rate": 8.096774193548387e-05,
      "loss": 0.5089,
      "step": 64
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 0.04648229479789734,
      "learning_rate": 8.064516129032258e-05,
      "loss": 0.4418,
      "step": 65
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.043945714831352234,
      "learning_rate": 8.03225806451613e-05,
      "loss": 0.3923,
      "step": 66
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 0.04776495695114136,
      "learning_rate": 8e-05,
      "loss": 0.3937,
      "step": 67
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 0.046913884580135345,
      "learning_rate": 7.967741935483871e-05,
      "loss": 0.4588,
      "step": 68
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.04586895927786827,
      "learning_rate": 7.935483870967743e-05,
      "loss": 0.4258,
      "step": 69
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.047559257596731186,
      "learning_rate": 7.903225806451613e-05,
      "loss": 0.4886,
      "step": 70
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 0.04979739710688591,
      "learning_rate": 7.870967741935484e-05,
      "loss": 0.3707,
      "step": 71
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.04651982709765434,
      "learning_rate": 7.838709677419355e-05,
      "loss": 0.3815,
      "step": 72
    },
    {
      "epoch": 1.1587301587301586,
      "grad_norm": 0.05145541578531265,
      "learning_rate": 7.806451612903226e-05,
      "loss": 0.4248,
      "step": 73
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 0.049106888473033905,
      "learning_rate": 7.774193548387098e-05,
      "loss": 0.3736,
      "step": 74
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.053515125066041946,
      "learning_rate": 7.741935483870968e-05,
      "loss": 0.4882,
      "step": 75
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 0.051709529012441635,
      "learning_rate": 7.709677419354839e-05,
      "loss": 0.4453,
      "step": 76
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.052621886134147644,
      "learning_rate": 7.67741935483871e-05,
      "loss": 0.4112,
      "step": 77
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 0.04654877632856369,
      "learning_rate": 7.645161290322582e-05,
      "loss": 0.3666,
      "step": 78
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 0.05389733612537384,
      "learning_rate": 7.612903225806451e-05,
      "loss": 0.4117,
      "step": 79
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 0.05136292800307274,
      "learning_rate": 7.580645161290323e-05,
      "loss": 0.4933,
      "step": 80
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.04780597239732742,
      "learning_rate": 7.548387096774195e-05,
      "loss": 0.3831,
      "step": 81
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 0.04936926066875458,
      "learning_rate": 7.516129032258064e-05,
      "loss": 0.413,
      "step": 82
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 0.06200019270181656,
      "learning_rate": 7.483870967741936e-05,
      "loss": 0.5706,
      "step": 83
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.058107148855924606,
      "learning_rate": 7.451612903225807e-05,
      "loss": 0.4343,
      "step": 84
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 0.05787625163793564,
      "learning_rate": 7.419354838709677e-05,
      "loss": 0.4242,
      "step": 85
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 0.04716969653964043,
      "learning_rate": 7.387096774193549e-05,
      "loss": 0.3801,
      "step": 86
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.05823333561420441,
      "learning_rate": 7.35483870967742e-05,
      "loss": 0.3885,
      "step": 87
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 0.0496029406785965,
      "learning_rate": 7.32258064516129e-05,
      "loss": 0.3336,
      "step": 88
    },
    {
      "epoch": 1.4126984126984126,
      "grad_norm": 0.057580333203077316,
      "learning_rate": 7.290322580645161e-05,
      "loss": 0.4613,
      "step": 89
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.055059198290109634,
      "learning_rate": 7.258064516129033e-05,
      "loss": 0.3971,
      "step": 90
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.05283082276582718,
      "learning_rate": 7.225806451612904e-05,
      "loss": 0.3995,
      "step": 91
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 0.057663533836603165,
      "learning_rate": 7.193548387096774e-05,
      "loss": 0.5528,
      "step": 92
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 0.053925588726997375,
      "learning_rate": 7.161290322580646e-05,
      "loss": 0.3647,
      "step": 93
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 0.06723572313785553,
      "learning_rate": 7.129032258064517e-05,
      "loss": 0.5732,
      "step": 94
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 0.054497212171554565,
      "learning_rate": 7.096774193548388e-05,
      "loss": 0.4309,
      "step": 95
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.053887128829956055,
      "learning_rate": 7.064516129032258e-05,
      "loss": 0.4772,
      "step": 96
    },
    {
      "epoch": 1.5396825396825395,
      "grad_norm": 0.05308767408132553,
      "learning_rate": 7.03225806451613e-05,
      "loss": 0.4154,
      "step": 97
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.05395730212330818,
      "learning_rate": 7e-05,
      "loss": 0.4099,
      "step": 98
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.05380746349692345,
      "learning_rate": 6.967741935483871e-05,
      "loss": 0.4275,
      "step": 99
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 0.05304594710469246,
      "learning_rate": 6.935483870967743e-05,
      "loss": 0.3874,
      "step": 100
    },
    {
      "epoch": 1.6031746031746033,
      "grad_norm": 0.05319123715162277,
      "learning_rate": 6.903225806451613e-05,
      "loss": 0.4078,
      "step": 101
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 0.056957922875881195,
      "learning_rate": 6.870967741935485e-05,
      "loss": 0.4291,
      "step": 102
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 0.05576823651790619,
      "learning_rate": 6.838709677419355e-05,
      "loss": 0.4286,
      "step": 103
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 0.05916175618767738,
      "learning_rate": 6.806451612903226e-05,
      "loss": 0.3842,
      "step": 104
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.06396201997995377,
      "learning_rate": 6.774193548387096e-05,
      "loss": 0.4585,
      "step": 105
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 0.05915219336748123,
      "learning_rate": 6.741935483870968e-05,
      "loss": 0.4838,
      "step": 106
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 0.05897779390215874,
      "learning_rate": 6.709677419354839e-05,
      "loss": 0.4096,
      "step": 107
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.05575665086507797,
      "learning_rate": 6.67741935483871e-05,
      "loss": 0.4028,
      "step": 108
    },
    {
      "epoch": 1.7301587301587302,
      "grad_norm": 0.06774401664733887,
      "learning_rate": 6.645161290322582e-05,
      "loss": 0.425,
      "step": 109
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 0.06407034397125244,
      "learning_rate": 6.612903225806452e-05,
      "loss": 0.4722,
      "step": 110
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 0.05422832444310188,
      "learning_rate": 6.580645161290323e-05,
      "loss": 0.4057,
      "step": 111
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.05986645072698593,
      "learning_rate": 6.548387096774193e-05,
      "loss": 0.4406,
      "step": 112
    },
    {
      "epoch": 1.7936507936507935,
      "grad_norm": 0.05810622498393059,
      "learning_rate": 6.516129032258065e-05,
      "loss": 0.4571,
      "step": 113
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 0.05836774408817291,
      "learning_rate": 6.483870967741936e-05,
      "loss": 0.4433,
      "step": 114
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 0.056447360664606094,
      "learning_rate": 6.451612903225807e-05,
      "loss": 0.4271,
      "step": 115
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 0.05414909869432449,
      "learning_rate": 6.419354838709679e-05,
      "loss": 0.3978,
      "step": 116
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.05888356640934944,
      "learning_rate": 6.387096774193548e-05,
      "loss": 0.406,
      "step": 117
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 0.061470042914152145,
      "learning_rate": 6.35483870967742e-05,
      "loss": 0.422,
      "step": 118
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.05695800110697746,
      "learning_rate": 6.32258064516129e-05,
      "loss": 0.4351,
      "step": 119
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 0.058453235775232315,
      "learning_rate": 6.290322580645161e-05,
      "loss": 0.4692,
      "step": 120
    },
    {
      "epoch": 1.9206349206349205,
      "grad_norm": 0.05541378632187843,
      "learning_rate": 6.258064516129033e-05,
      "loss": 0.4366,
      "step": 121
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 0.055218104273080826,
      "learning_rate": 6.225806451612904e-05,
      "loss": 0.3641,
      "step": 122
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 0.061556823551654816,
      "learning_rate": 6.193548387096774e-05,
      "loss": 0.4094,
      "step": 123
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 0.06279738992452621,
      "learning_rate": 6.161290322580645e-05,
      "loss": 0.5178,
      "step": 124
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 0.05167330801486969,
      "learning_rate": 6.129032258064517e-05,
      "loss": 0.3636,
      "step": 125
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.07673394680023193,
      "learning_rate": 6.096774193548387e-05,
      "loss": 0.3802,
      "step": 126
    },
    {
      "epoch": 2.015873015873016,
      "grad_norm": 0.05450648069381714,
      "learning_rate": 6.064516129032258e-05,
      "loss": 0.3732,
      "step": 127
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 0.061343465000391006,
      "learning_rate": 6.0322580645161295e-05,
      "loss": 0.4656,
      "step": 128
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 0.06421998143196106,
      "learning_rate": 6e-05,
      "loss": 0.5084,
      "step": 129
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 0.0596974641084671,
      "learning_rate": 5.9677419354838715e-05,
      "loss": 0.4069,
      "step": 130
    },
    {
      "epoch": 2.0793650793650795,
      "grad_norm": 0.06161348894238472,
      "learning_rate": 5.935483870967742e-05,
      "loss": 0.4672,
      "step": 131
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.06536874920129776,
      "learning_rate": 5.9032258064516134e-05,
      "loss": 0.4702,
      "step": 132
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.06574459373950958,
      "learning_rate": 5.870967741935483e-05,
      "loss": 0.4089,
      "step": 133
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 0.0573127306997776,
      "learning_rate": 5.838709677419355e-05,
      "loss": 0.4092,
      "step": 134
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.06085209548473358,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 0.3476,
      "step": 135
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 0.06264346092939377,
      "learning_rate": 5.7741935483870965e-05,
      "loss": 0.4677,
      "step": 136
    },
    {
      "epoch": 2.1746031746031744,
      "grad_norm": 0.06014183163642883,
      "learning_rate": 5.7419354838709685e-05,
      "loss": 0.3514,
      "step": 137
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.061129529029130936,
      "learning_rate": 5.7096774193548384e-05,
      "loss": 0.3479,
      "step": 138
    },
    {
      "epoch": 2.2063492063492065,
      "grad_norm": 0.0637439414858818,
      "learning_rate": 5.67741935483871e-05,
      "loss": 0.4198,
      "step": 139
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.06803480535745621,
      "learning_rate": 5.645161290322582e-05,
      "loss": 0.4763,
      "step": 140
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 0.073043093085289,
      "learning_rate": 5.612903225806452e-05,
      "loss": 0.4349,
      "step": 141
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 0.06504002958536148,
      "learning_rate": 5.580645161290323e-05,
      "loss": 0.4348,
      "step": 142
    },
    {
      "epoch": 2.2698412698412698,
      "grad_norm": 0.05686529353260994,
      "learning_rate": 5.5483870967741936e-05,
      "loss": 0.4111,
      "step": 143
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.06259525567293167,
      "learning_rate": 5.516129032258065e-05,
      "loss": 0.4254,
      "step": 144
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 0.06444687396287918,
      "learning_rate": 5.4838709677419355e-05,
      "loss": 0.3807,
      "step": 145
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 0.07152538001537323,
      "learning_rate": 5.451612903225807e-05,
      "loss": 0.5091,
      "step": 146
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.06212814152240753,
      "learning_rate": 5.419354838709678e-05,
      "loss": 0.4042,
      "step": 147
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 0.06188872456550598,
      "learning_rate": 5.387096774193549e-05,
      "loss": 0.3637,
      "step": 148
    },
    {
      "epoch": 2.365079365079365,
      "grad_norm": 0.06022093445062637,
      "learning_rate": 5.35483870967742e-05,
      "loss": 0.3822,
      "step": 149
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.07097094506025314,
      "learning_rate": 5.32258064516129e-05,
      "loss": 0.483,
      "step": 150
    },
    {
      "epoch": 2.3968253968253967,
      "grad_norm": 0.06512099504470825,
      "learning_rate": 5.290322580645162e-05,
      "loss": 0.4328,
      "step": 151
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 0.06229754537343979,
      "learning_rate": 5.258064516129032e-05,
      "loss": 0.372,
      "step": 152
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.062207065522670746,
      "learning_rate": 5.225806451612903e-05,
      "loss": 0.3922,
      "step": 153
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.06698767840862274,
      "learning_rate": 5.193548387096775e-05,
      "loss": 0.4193,
      "step": 154
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 0.062149178236722946,
      "learning_rate": 5.161290322580645e-05,
      "loss": 0.3132,
      "step": 155
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.06410950422286987,
      "learning_rate": 5.1290322580645164e-05,
      "loss": 0.4088,
      "step": 156
    },
    {
      "epoch": 2.492063492063492,
      "grad_norm": 0.06118849292397499,
      "learning_rate": 5.096774193548387e-05,
      "loss": 0.4041,
      "step": 157
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 0.06360729783773422,
      "learning_rate": 5.064516129032258e-05,
      "loss": 0.4143,
      "step": 158
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 0.07116158306598663,
      "learning_rate": 5.032258064516129e-05,
      "loss": 0.3827,
      "step": 159
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 0.06972448527812958,
      "learning_rate": 5e-05,
      "loss": 0.4609,
      "step": 160
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.06363383680582047,
      "learning_rate": 4.967741935483871e-05,
      "loss": 0.4095,
      "step": 161
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.06302540749311447,
      "learning_rate": 4.935483870967742e-05,
      "loss": 0.3382,
      "step": 162
    },
    {
      "epoch": 2.5873015873015874,
      "grad_norm": 0.07798280566930771,
      "learning_rate": 4.903225806451613e-05,
      "loss": 0.4963,
      "step": 163
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 0.0696374922990799,
      "learning_rate": 4.870967741935484e-05,
      "loss": 0.4367,
      "step": 164
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 0.06648410856723785,
      "learning_rate": 4.8387096774193554e-05,
      "loss": 0.4048,
      "step": 165
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 0.07369227707386017,
      "learning_rate": 4.806451612903226e-05,
      "loss": 0.496,
      "step": 166
    },
    {
      "epoch": 2.6507936507936507,
      "grad_norm": 0.06781552731990814,
      "learning_rate": 4.774193548387097e-05,
      "loss": 0.3823,
      "step": 167
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.06232914701104164,
      "learning_rate": 4.741935483870968e-05,
      "loss": 0.3533,
      "step": 168
    },
    {
      "epoch": 2.682539682539683,
      "grad_norm": 0.06734704226255417,
      "learning_rate": 4.7096774193548385e-05,
      "loss": 0.4344,
      "step": 169
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 0.06748051196336746,
      "learning_rate": 4.67741935483871e-05,
      "loss": 0.4206,
      "step": 170
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 0.07176529616117477,
      "learning_rate": 4.645161290322581e-05,
      "loss": 0.4103,
      "step": 171
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 0.0608476959168911,
      "learning_rate": 4.612903225806452e-05,
      "loss": 0.3171,
      "step": 172
    },
    {
      "epoch": 2.746031746031746,
      "grad_norm": 0.06838694214820862,
      "learning_rate": 4.580645161290323e-05,
      "loss": 0.4421,
      "step": 173
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.06609457731246948,
      "learning_rate": 4.548387096774194e-05,
      "loss": 0.3632,
      "step": 174
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.06288681924343109,
      "learning_rate": 4.516129032258064e-05,
      "loss": 0.3329,
      "step": 175
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 0.0660625547170639,
      "learning_rate": 4.4838709677419356e-05,
      "loss": 0.3753,
      "step": 176
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 0.0672590583562851,
      "learning_rate": 4.451612903225807e-05,
      "loss": 0.3672,
      "step": 177
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 0.08032297343015671,
      "learning_rate": 4.4193548387096775e-05,
      "loss": 0.5152,
      "step": 178
    },
    {
      "epoch": 2.8412698412698414,
      "grad_norm": 0.07281044870615005,
      "learning_rate": 4.387096774193549e-05,
      "loss": 0.4182,
      "step": 179
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.07073714584112167,
      "learning_rate": 4.3548387096774194e-05,
      "loss": 0.4331,
      "step": 180
    },
    {
      "epoch": 2.873015873015873,
      "grad_norm": 0.07470233738422394,
      "learning_rate": 4.322580645161291e-05,
      "loss": 0.4477,
      "step": 181
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.08410092443227768,
      "learning_rate": 4.2903225806451614e-05,
      "loss": 0.5548,
      "step": 182
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 0.06423322856426239,
      "learning_rate": 4.258064516129032e-05,
      "loss": 0.3435,
      "step": 183
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 0.08123717457056046,
      "learning_rate": 4.225806451612904e-05,
      "loss": 0.5053,
      "step": 184
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 0.06999558210372925,
      "learning_rate": 4.1935483870967746e-05,
      "loss": 0.4233,
      "step": 185
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.062326740473508835,
      "learning_rate": 4.161290322580645e-05,
      "loss": 0.3164,
      "step": 186
    },
    {
      "epoch": 2.9682539682539684,
      "grad_norm": 0.0683780312538147,
      "learning_rate": 4.1290322580645165e-05,
      "loss": 0.3634,
      "step": 187
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 0.0645333081483841,
      "learning_rate": 4.096774193548387e-05,
      "loss": 0.3637,
      "step": 188
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.09523583948612213,
      "learning_rate": 4.0645161290322584e-05,
      "loss": 0.4896,
      "step": 189
    },
    {
      "epoch": 3.015873015873016,
      "grad_norm": 0.06403031945228577,
      "learning_rate": 4.032258064516129e-05,
      "loss": 0.3777,
      "step": 190
    },
    {
      "epoch": 3.0317460317460316,
      "grad_norm": 0.06517292559146881,
      "learning_rate": 4e-05,
      "loss": 0.3661,
      "step": 191
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 0.06774237006902695,
      "learning_rate": 3.9677419354838716e-05,
      "loss": 0.3463,
      "step": 192
    },
    {
      "epoch": 3.0634920634920633,
      "grad_norm": 0.07015440613031387,
      "learning_rate": 3.935483870967742e-05,
      "loss": 0.4098,
      "step": 193
    },
    {
      "epoch": 3.0793650793650795,
      "grad_norm": 0.06616640836000443,
      "learning_rate": 3.903225806451613e-05,
      "loss": 0.3471,
      "step": 194
    },
    {
      "epoch": 3.0952380952380953,
      "grad_norm": 0.07075586169958115,
      "learning_rate": 3.870967741935484e-05,
      "loss": 0.4213,
      "step": 195
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.06593161821365356,
      "learning_rate": 3.838709677419355e-05,
      "loss": 0.3486,
      "step": 196
    },
    {
      "epoch": 3.126984126984127,
      "grad_norm": 0.06902993470430374,
      "learning_rate": 3.8064516129032254e-05,
      "loss": 0.3901,
      "step": 197
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.07414506375789642,
      "learning_rate": 3.7741935483870974e-05,
      "loss": 0.4143,
      "step": 198
    },
    {
      "epoch": 3.1587301587301586,
      "grad_norm": 0.07434172928333282,
      "learning_rate": 3.741935483870968e-05,
      "loss": 0.4172,
      "step": 199
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 0.0724806934595108,
      "learning_rate": 3.7096774193548386e-05,
      "loss": 0.3935,
      "step": 200
    },
    {
      "epoch": 3.1904761904761907,
      "grad_norm": 0.08157327771186829,
      "learning_rate": 3.67741935483871e-05,
      "loss": 0.5262,
      "step": 201
    },
    {
      "epoch": 3.2063492063492065,
      "grad_norm": 0.06515661627054214,
      "learning_rate": 3.6451612903225805e-05,
      "loss": 0.3564,
      "step": 202
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.07240559160709381,
      "learning_rate": 3.612903225806452e-05,
      "loss": 0.3942,
      "step": 203
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 0.06990107148885727,
      "learning_rate": 3.580645161290323e-05,
      "loss": 0.3981,
      "step": 204
    },
    {
      "epoch": 3.253968253968254,
      "grad_norm": 0.07707206159830093,
      "learning_rate": 3.548387096774194e-05,
      "loss": 0.4157,
      "step": 205
    },
    {
      "epoch": 3.2698412698412698,
      "grad_norm": 0.07639270275831223,
      "learning_rate": 3.516129032258065e-05,
      "loss": 0.4689,
      "step": 206
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.08987829089164734,
      "learning_rate": 3.483870967741936e-05,
      "loss": 0.4545,
      "step": 207
    },
    {
      "epoch": 3.3015873015873014,
      "grad_norm": 0.07155808806419373,
      "learning_rate": 3.451612903225806e-05,
      "loss": 0.3455,
      "step": 208
    },
    {
      "epoch": 3.317460317460317,
      "grad_norm": 0.07093101739883423,
      "learning_rate": 3.4193548387096776e-05,
      "loss": 0.3944,
      "step": 209
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.07250124961137772,
      "learning_rate": 3.387096774193548e-05,
      "loss": 0.3871,
      "step": 210
    },
    {
      "epoch": 3.3492063492063493,
      "grad_norm": 0.06927445530891418,
      "learning_rate": 3.3548387096774195e-05,
      "loss": 0.3515,
      "step": 211
    },
    {
      "epoch": 3.365079365079365,
      "grad_norm": 0.07062845677137375,
      "learning_rate": 3.322580645161291e-05,
      "loss": 0.3711,
      "step": 212
    },
    {
      "epoch": 3.380952380952381,
      "grad_norm": 0.07143618911504745,
      "learning_rate": 3.2903225806451614e-05,
      "loss": 0.4213,
      "step": 213
    },
    {
      "epoch": 3.3968253968253967,
      "grad_norm": 0.06499861180782318,
      "learning_rate": 3.258064516129033e-05,
      "loss": 0.3529,
      "step": 214
    },
    {
      "epoch": 3.4126984126984126,
      "grad_norm": 0.08063112944364548,
      "learning_rate": 3.2258064516129034e-05,
      "loss": 0.5413,
      "step": 215
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 0.07448151707649231,
      "learning_rate": 3.193548387096774e-05,
      "loss": 0.409,
      "step": 216
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.07299645245075226,
      "learning_rate": 3.161290322580645e-05,
      "loss": 0.4608,
      "step": 217
    },
    {
      "epoch": 3.4603174603174605,
      "grad_norm": 0.07053133845329285,
      "learning_rate": 3.1290322580645166e-05,
      "loss": 0.3934,
      "step": 218
    },
    {
      "epoch": 3.4761904761904763,
      "grad_norm": 0.07108025997877121,
      "learning_rate": 3.096774193548387e-05,
      "loss": 0.3846,
      "step": 219
    },
    {
      "epoch": 3.492063492063492,
      "grad_norm": 0.07647310942411423,
      "learning_rate": 3.0645161290322585e-05,
      "loss": 0.3885,
      "step": 220
    },
    {
      "epoch": 3.507936507936508,
      "grad_norm": 0.07823354005813599,
      "learning_rate": 3.032258064516129e-05,
      "loss": 0.4469,
      "step": 221
    },
    {
      "epoch": 3.5238095238095237,
      "grad_norm": 0.07514266669750214,
      "learning_rate": 3e-05,
      "loss": 0.4667,
      "step": 222
    },
    {
      "epoch": 3.5396825396825395,
      "grad_norm": 0.06813749670982361,
      "learning_rate": 2.967741935483871e-05,
      "loss": 0.3948,
      "step": 223
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.07061238586902618,
      "learning_rate": 2.9354838709677417e-05,
      "loss": 0.3616,
      "step": 224
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 0.07940101623535156,
      "learning_rate": 2.9032258064516133e-05,
      "loss": 0.4523,
      "step": 225
    },
    {
      "epoch": 3.5873015873015874,
      "grad_norm": 0.06982545554637909,
      "learning_rate": 2.8709677419354843e-05,
      "loss": 0.3751,
      "step": 226
    },
    {
      "epoch": 3.6031746031746033,
      "grad_norm": 0.08228139579296112,
      "learning_rate": 2.838709677419355e-05,
      "loss": 0.3928,
      "step": 227
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 0.07438891381025314,
      "learning_rate": 2.806451612903226e-05,
      "loss": 0.3821,
      "step": 228
    },
    {
      "epoch": 3.634920634920635,
      "grad_norm": 0.07666332274675369,
      "learning_rate": 2.7741935483870968e-05,
      "loss": 0.4215,
      "step": 229
    },
    {
      "epoch": 3.6507936507936507,
      "grad_norm": 0.07656123489141464,
      "learning_rate": 2.7419354838709678e-05,
      "loss": 0.3841,
      "step": 230
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.08514106273651123,
      "learning_rate": 2.709677419354839e-05,
      "loss": 0.5035,
      "step": 231
    },
    {
      "epoch": 3.682539682539683,
      "grad_norm": 0.06852288544178009,
      "learning_rate": 2.67741935483871e-05,
      "loss": 0.3616,
      "step": 232
    },
    {
      "epoch": 3.6984126984126986,
      "grad_norm": 0.07123994827270508,
      "learning_rate": 2.645161290322581e-05,
      "loss": 0.419,
      "step": 233
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 0.07360682636499405,
      "learning_rate": 2.6129032258064516e-05,
      "loss": 0.4152,
      "step": 234
    },
    {
      "epoch": 3.7301587301587302,
      "grad_norm": 0.07840315997600555,
      "learning_rate": 2.5806451612903226e-05,
      "loss": 0.3784,
      "step": 235
    },
    {
      "epoch": 3.746031746031746,
      "grad_norm": 0.08004681020975113,
      "learning_rate": 2.5483870967741935e-05,
      "loss": 0.4297,
      "step": 236
    },
    {
      "epoch": 3.761904761904762,
      "grad_norm": 0.07760364562273026,
      "learning_rate": 2.5161290322580645e-05,
      "loss": 0.4155,
      "step": 237
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.07543783634901047,
      "learning_rate": 2.4838709677419354e-05,
      "loss": 0.3943,
      "step": 238
    },
    {
      "epoch": 3.7936507936507935,
      "grad_norm": 0.06805877387523651,
      "learning_rate": 2.4516129032258064e-05,
      "loss": 0.3541,
      "step": 239
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 0.07670442759990692,
      "learning_rate": 2.4193548387096777e-05,
      "loss": 0.3902,
      "step": 240
    },
    {
      "epoch": 3.825396825396825,
      "grad_norm": 0.0871642604470253,
      "learning_rate": 2.3870967741935486e-05,
      "loss": 0.5199,
      "step": 241
    },
    {
      "epoch": 3.8412698412698414,
      "grad_norm": 0.07493454217910767,
      "learning_rate": 2.3548387096774193e-05,
      "loss": 0.3704,
      "step": 242
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 0.07781630754470825,
      "learning_rate": 2.3225806451612906e-05,
      "loss": 0.3798,
      "step": 243
    },
    {
      "epoch": 3.873015873015873,
      "grad_norm": 0.082707978785038,
      "learning_rate": 2.2903225806451615e-05,
      "loss": 0.3468,
      "step": 244
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.0839216485619545,
      "learning_rate": 2.258064516129032e-05,
      "loss": 0.3978,
      "step": 245
    },
    {
      "epoch": 3.9047619047619047,
      "grad_norm": 0.07507694512605667,
      "learning_rate": 2.2258064516129034e-05,
      "loss": 0.4079,
      "step": 246
    },
    {
      "epoch": 3.9206349206349205,
      "grad_norm": 0.07181837409734726,
      "learning_rate": 2.1935483870967744e-05,
      "loss": 0.3433,
      "step": 247
    },
    {
      "epoch": 3.9365079365079367,
      "grad_norm": 0.0788164809346199,
      "learning_rate": 2.1612903225806454e-05,
      "loss": 0.4153,
      "step": 248
    },
    {
      "epoch": 3.9523809523809526,
      "grad_norm": 0.07437039911746979,
      "learning_rate": 2.129032258064516e-05,
      "loss": 0.3232,
      "step": 249
    },
    {
      "epoch": 3.9682539682539684,
      "grad_norm": 0.07817775756120682,
      "learning_rate": 2.0967741935483873e-05,
      "loss": 0.4489,
      "step": 250
    },
    {
      "epoch": 3.984126984126984,
      "grad_norm": 0.0809004083275795,
      "learning_rate": 2.0645161290322582e-05,
      "loss": 0.4189,
      "step": 251
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.12757429480552673,
      "learning_rate": 2.0322580645161292e-05,
      "loss": 0.5347,
      "step": 252
    },
    {
      "epoch": 4.015873015873016,
      "grad_norm": 0.07564050704240799,
      "learning_rate": 2e-05,
      "loss": 0.4649,
      "step": 253
    },
    {
      "epoch": 4.031746031746032,
      "grad_norm": 0.08298367261886597,
      "learning_rate": 1.967741935483871e-05,
      "loss": 0.4408,
      "step": 254
    },
    {
      "epoch": 4.0476190476190474,
      "grad_norm": 0.07231239974498749,
      "learning_rate": 1.935483870967742e-05,
      "loss": 0.3452,
      "step": 255
    },
    {
      "epoch": 4.063492063492063,
      "grad_norm": 0.07782192528247833,
      "learning_rate": 1.9032258064516127e-05,
      "loss": 0.4297,
      "step": 256
    },
    {
      "epoch": 4.079365079365079,
      "grad_norm": 0.07820593565702438,
      "learning_rate": 1.870967741935484e-05,
      "loss": 0.3876,
      "step": 257
    },
    {
      "epoch": 4.095238095238095,
      "grad_norm": 0.07468805462121964,
      "learning_rate": 1.838709677419355e-05,
      "loss": 0.3913,
      "step": 258
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.07107950001955032,
      "learning_rate": 1.806451612903226e-05,
      "loss": 0.3748,
      "step": 259
    },
    {
      "epoch": 4.1269841269841265,
      "grad_norm": 0.07708931714296341,
      "learning_rate": 1.774193548387097e-05,
      "loss": 0.4098,
      "step": 260
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 0.07346481084823608,
      "learning_rate": 1.741935483870968e-05,
      "loss": 0.3883,
      "step": 261
    },
    {
      "epoch": 4.158730158730159,
      "grad_norm": 0.07988614588975906,
      "learning_rate": 1.7096774193548388e-05,
      "loss": 0.4633,
      "step": 262
    },
    {
      "epoch": 4.174603174603175,
      "grad_norm": 0.07899882644414902,
      "learning_rate": 1.6774193548387098e-05,
      "loss": 0.405,
      "step": 263
    },
    {
      "epoch": 4.190476190476191,
      "grad_norm": 0.08658407628536224,
      "learning_rate": 1.6451612903225807e-05,
      "loss": 0.4201,
      "step": 264
    },
    {
      "epoch": 4.2063492063492065,
      "grad_norm": 0.0731176808476448,
      "learning_rate": 1.6129032258064517e-05,
      "loss": 0.3597,
      "step": 265
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.077212855219841,
      "learning_rate": 1.5806451612903226e-05,
      "loss": 0.4194,
      "step": 266
    },
    {
      "epoch": 4.238095238095238,
      "grad_norm": 0.07444500178098679,
      "learning_rate": 1.5483870967741936e-05,
      "loss": 0.3621,
      "step": 267
    },
    {
      "epoch": 4.253968253968254,
      "grad_norm": 0.08547846227884293,
      "learning_rate": 1.5161290322580646e-05,
      "loss": 0.5551,
      "step": 268
    },
    {
      "epoch": 4.26984126984127,
      "grad_norm": 0.07839558273553848,
      "learning_rate": 1.4838709677419355e-05,
      "loss": 0.4425,
      "step": 269
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.07010795176029205,
      "learning_rate": 1.4516129032258066e-05,
      "loss": 0.3674,
      "step": 270
    },
    {
      "epoch": 4.301587301587301,
      "grad_norm": 0.07809635251760483,
      "learning_rate": 1.4193548387096774e-05,
      "loss": 0.4212,
      "step": 271
    },
    {
      "epoch": 4.317460317460317,
      "grad_norm": 0.07806512713432312,
      "learning_rate": 1.3870967741935484e-05,
      "loss": 0.3932,
      "step": 272
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.07270673662424088,
      "learning_rate": 1.3548387096774195e-05,
      "loss": 0.3381,
      "step": 273
    },
    {
      "epoch": 4.349206349206349,
      "grad_norm": 0.0778871700167656,
      "learning_rate": 1.3225806451612905e-05,
      "loss": 0.4598,
      "step": 274
    },
    {
      "epoch": 4.365079365079365,
      "grad_norm": 0.07654031366109848,
      "learning_rate": 1.2903225806451613e-05,
      "loss": 0.4127,
      "step": 275
    },
    {
      "epoch": 4.380952380952381,
      "grad_norm": 0.08155998587608337,
      "learning_rate": 1.2580645161290322e-05,
      "loss": 0.4342,
      "step": 276
    },
    {
      "epoch": 4.396825396825397,
      "grad_norm": 0.08101197332143784,
      "learning_rate": 1.2258064516129032e-05,
      "loss": 0.391,
      "step": 277
    },
    {
      "epoch": 4.412698412698413,
      "grad_norm": 0.07359611988067627,
      "learning_rate": 1.1935483870967743e-05,
      "loss": 0.4176,
      "step": 278
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 0.07512898743152618,
      "learning_rate": 1.1612903225806453e-05,
      "loss": 0.3575,
      "step": 279
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.07011528313159943,
      "learning_rate": 1.129032258064516e-05,
      "loss": 0.3969,
      "step": 280
    },
    {
      "epoch": 4.4603174603174605,
      "grad_norm": 0.07716195285320282,
      "learning_rate": 1.0967741935483872e-05,
      "loss": 0.3701,
      "step": 281
    },
    {
      "epoch": 4.476190476190476,
      "grad_norm": 0.07590426504611969,
      "learning_rate": 1.064516129032258e-05,
      "loss": 0.4013,
      "step": 282
    },
    {
      "epoch": 4.492063492063492,
      "grad_norm": 0.07609281688928604,
      "learning_rate": 1.0322580645161291e-05,
      "loss": 0.3713,
      "step": 283
    },
    {
      "epoch": 4.507936507936508,
      "grad_norm": 0.07271591573953629,
      "learning_rate": 1e-05,
      "loss": 0.3266,
      "step": 284
    },
    {
      "epoch": 4.523809523809524,
      "grad_norm": 0.07474313676357269,
      "learning_rate": 9.67741935483871e-06,
      "loss": 0.3534,
      "step": 285
    },
    {
      "epoch": 4.5396825396825395,
      "grad_norm": 0.08654330670833588,
      "learning_rate": 9.35483870967742e-06,
      "loss": 0.4251,
      "step": 286
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.0719628632068634,
      "learning_rate": 9.03225806451613e-06,
      "loss": 0.3618,
      "step": 287
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.07511646300554276,
      "learning_rate": 8.70967741935484e-06,
      "loss": 0.3789,
      "step": 288
    },
    {
      "epoch": 4.587301587301587,
      "grad_norm": 0.07402034103870392,
      "learning_rate": 8.387096774193549e-06,
      "loss": 0.3939,
      "step": 289
    },
    {
      "epoch": 4.603174603174603,
      "grad_norm": 0.07389985769987106,
      "learning_rate": 8.064516129032258e-06,
      "loss": 0.3538,
      "step": 290
    },
    {
      "epoch": 4.619047619047619,
      "grad_norm": 0.07071951776742935,
      "learning_rate": 7.741935483870968e-06,
      "loss": 0.308,
      "step": 291
    },
    {
      "epoch": 4.634920634920634,
      "grad_norm": 0.0766642689704895,
      "learning_rate": 7.419354838709678e-06,
      "loss": 0.3935,
      "step": 292
    },
    {
      "epoch": 4.650793650793651,
      "grad_norm": 0.07373537868261337,
      "learning_rate": 7.096774193548387e-06,
      "loss": 0.3748,
      "step": 293
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.08196863532066345,
      "learning_rate": 6.774193548387098e-06,
      "loss": 0.4161,
      "step": 294
    },
    {
      "epoch": 4.682539682539683,
      "grad_norm": 0.07441911846399307,
      "learning_rate": 6.451612903225806e-06,
      "loss": 0.3576,
      "step": 295
    },
    {
      "epoch": 4.698412698412699,
      "grad_norm": 0.07001787424087524,
      "learning_rate": 6.129032258064516e-06,
      "loss": 0.3264,
      "step": 296
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 0.07378097623586655,
      "learning_rate": 5.806451612903226e-06,
      "loss": 0.3724,
      "step": 297
    },
    {
      "epoch": 4.73015873015873,
      "grad_norm": 0.0721716433763504,
      "learning_rate": 5.483870967741936e-06,
      "loss": 0.3177,
      "step": 298
    },
    {
      "epoch": 4.746031746031746,
      "grad_norm": 0.08187282830476761,
      "learning_rate": 5.161290322580646e-06,
      "loss": 0.4249,
      "step": 299
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 0.07863786816596985,
      "learning_rate": 4.838709677419355e-06,
      "loss": 0.418,
      "step": 300
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 0.08527539670467377,
      "learning_rate": 4.516129032258065e-06,
      "loss": 0.4091,
      "step": 301
    },
    {
      "epoch": 4.7936507936507935,
      "grad_norm": 0.08165731281042099,
      "learning_rate": 4.193548387096774e-06,
      "loss": 0.3572,
      "step": 302
    },
    {
      "epoch": 4.809523809523809,
      "grad_norm": 0.07901745289564133,
      "learning_rate": 3.870967741935484e-06,
      "loss": 0.4677,
      "step": 303
    },
    {
      "epoch": 4.825396825396825,
      "grad_norm": 0.07349260151386261,
      "learning_rate": 3.5483870967741936e-06,
      "loss": 0.4159,
      "step": 304
    },
    {
      "epoch": 4.841269841269841,
      "grad_norm": 0.07321806997060776,
      "learning_rate": 3.225806451612903e-06,
      "loss": 0.409,
      "step": 305
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.08197671920061111,
      "learning_rate": 2.903225806451613e-06,
      "loss": 0.4852,
      "step": 306
    },
    {
      "epoch": 4.8730158730158735,
      "grad_norm": 0.07480282336473465,
      "learning_rate": 2.580645161290323e-06,
      "loss": 0.3508,
      "step": 307
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.07375568896532059,
      "learning_rate": 2.2580645161290324e-06,
      "loss": 0.4037,
      "step": 308
    },
    {
      "epoch": 4.904761904761905,
      "grad_norm": 0.08064710348844528,
      "learning_rate": 1.935483870967742e-06,
      "loss": 0.4718,
      "step": 309
    },
    {
      "epoch": 4.920634920634921,
      "grad_norm": 0.07272341847419739,
      "learning_rate": 1.6129032258064516e-06,
      "loss": 0.3581,
      "step": 310
    },
    {
      "epoch": 4.936507936507937,
      "grad_norm": 0.07656922936439514,
      "learning_rate": 1.2903225806451614e-06,
      "loss": 0.4403,
      "step": 311
    },
    {
      "epoch": 4.9523809523809526,
      "grad_norm": 0.0756097286939621,
      "learning_rate": 9.67741935483871e-07,
      "loss": 0.386,
      "step": 312
    },
    {
      "epoch": 4.968253968253968,
      "grad_norm": 0.07521409541368484,
      "learning_rate": 6.451612903225807e-07,
      "loss": 0.3793,
      "step": 313
    },
    {
      "epoch": 4.984126984126984,
      "grad_norm": 0.07984190434217453,
      "learning_rate": 3.2258064516129035e-07,
      "loss": 0.3914,
      "step": 314
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.09992519021034241,
      "learning_rate": 0.0,
      "loss": 0.3424,
      "step": 315
    }
  ],
  "logging_steps": 1,
  "max_steps": 315,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.105903549739745e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
