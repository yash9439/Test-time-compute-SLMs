{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 315,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 0.07576775550842285,
      "learning_rate": 4e-05,
      "loss": 0.5031,
      "step": 1
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 0.10457547008991241,
      "learning_rate": 8e-05,
      "loss": 0.6923,
      "step": 2
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.09080036729574203,
      "learning_rate": 0.00012,
      "loss": 0.6221,
      "step": 3
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 0.06999203562736511,
      "learning_rate": 0.00016,
      "loss": 0.4525,
      "step": 4
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 0.07325846701860428,
      "learning_rate": 0.0002,
      "loss": 0.5044,
      "step": 5
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.06235227361321449,
      "learning_rate": 0.00019935483870967745,
      "loss": 0.4888,
      "step": 6
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.05390797555446625,
      "learning_rate": 0.00019870967741935483,
      "loss": 0.443,
      "step": 7
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 0.08297771960496902,
      "learning_rate": 0.00019806451612903227,
      "loss": 0.591,
      "step": 8
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.08436036109924316,
      "learning_rate": 0.00019741935483870969,
      "loss": 0.5745,
      "step": 9
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 0.06006801873445511,
      "learning_rate": 0.0001967741935483871,
      "loss": 0.3994,
      "step": 10
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 0.05712435394525528,
      "learning_rate": 0.0001961290322580645,
      "loss": 0.4296,
      "step": 11
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.054299771785736084,
      "learning_rate": 0.00019548387096774195,
      "loss": 0.4698,
      "step": 12
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 0.051878612488508224,
      "learning_rate": 0.00019483870967741936,
      "loss": 0.4425,
      "step": 13
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.05426296591758728,
      "learning_rate": 0.00019419354838709678,
      "loss": 0.4768,
      "step": 14
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.051958173513412476,
      "learning_rate": 0.00019354838709677422,
      "loss": 0.5062,
      "step": 15
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 0.04893578588962555,
      "learning_rate": 0.00019290322580645163,
      "loss": 0.4821,
      "step": 16
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 0.04860398918390274,
      "learning_rate": 0.00019225806451612904,
      "loss": 0.4585,
      "step": 17
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.04742973670363426,
      "learning_rate": 0.00019161290322580645,
      "loss": 0.4823,
      "step": 18
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 0.04713957756757736,
      "learning_rate": 0.0001909677419354839,
      "loss": 0.5065,
      "step": 19
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 0.04878705367445946,
      "learning_rate": 0.0001903225806451613,
      "loss": 0.5585,
      "step": 20
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.04678639769554138,
      "learning_rate": 0.00018967741935483872,
      "loss": 0.5368,
      "step": 21
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 0.03758104518055916,
      "learning_rate": 0.00018903225806451616,
      "loss": 0.3792,
      "step": 22
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 0.04319046810269356,
      "learning_rate": 0.00018838709677419354,
      "loss": 0.5065,
      "step": 23
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.04611843079328537,
      "learning_rate": 0.00018774193548387098,
      "loss": 0.4438,
      "step": 24
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 0.04641388729214668,
      "learning_rate": 0.0001870967741935484,
      "loss": 0.4979,
      "step": 25
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 0.04503212124109268,
      "learning_rate": 0.0001864516129032258,
      "loss": 0.5361,
      "step": 26
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.03974624350667,
      "learning_rate": 0.00018580645161290325,
      "loss": 0.4289,
      "step": 27
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.03886877000331879,
      "learning_rate": 0.00018516129032258066,
      "loss": 0.4618,
      "step": 28
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 0.04117419198155403,
      "learning_rate": 0.00018451612903225807,
      "loss": 0.4229,
      "step": 29
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.04196338728070259,
      "learning_rate": 0.00018387096774193548,
      "loss": 0.478,
      "step": 30
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 0.04160473868250847,
      "learning_rate": 0.00018322580645161292,
      "loss": 0.4327,
      "step": 31
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 0.039454080164432526,
      "learning_rate": 0.00018258064516129033,
      "loss": 0.3897,
      "step": 32
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 0.041046854108572006,
      "learning_rate": 0.00018193548387096775,
      "loss": 0.4366,
      "step": 33
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 0.04689367488026619,
      "learning_rate": 0.0001812903225806452,
      "loss": 0.5076,
      "step": 34
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.03789837285876274,
      "learning_rate": 0.00018064516129032257,
      "loss": 0.387,
      "step": 35
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.04293869435787201,
      "learning_rate": 0.00018,
      "loss": 0.4725,
      "step": 36
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 0.03817489370703697,
      "learning_rate": 0.00017935483870967742,
      "loss": 0.3703,
      "step": 37
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 0.044243838638067245,
      "learning_rate": 0.00017870967741935484,
      "loss": 0.4941,
      "step": 38
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 0.04416990652680397,
      "learning_rate": 0.00017806451612903228,
      "loss": 0.4566,
      "step": 39
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 0.04245772585272789,
      "learning_rate": 0.0001774193548387097,
      "loss": 0.3845,
      "step": 40
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 0.04044696316123009,
      "learning_rate": 0.0001767741935483871,
      "loss": 0.4287,
      "step": 41
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.04278910905122757,
      "learning_rate": 0.0001761290322580645,
      "loss": 0.4449,
      "step": 42
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 0.04377312213182449,
      "learning_rate": 0.00017548387096774195,
      "loss": 0.3989,
      "step": 43
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 0.04593938961625099,
      "learning_rate": 0.00017483870967741936,
      "loss": 0.4723,
      "step": 44
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.04607459530234337,
      "learning_rate": 0.00017419354838709678,
      "loss": 0.4786,
      "step": 45
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 0.04615197703242302,
      "learning_rate": 0.00017354838709677422,
      "loss": 0.3704,
      "step": 46
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 0.04889662563800812,
      "learning_rate": 0.00017290322580645163,
      "loss": 0.4503,
      "step": 47
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.049701493233442307,
      "learning_rate": 0.00017225806451612904,
      "loss": 0.4426,
      "step": 48
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.044844310730695724,
      "learning_rate": 0.00017161290322580645,
      "loss": 0.3974,
      "step": 49
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 0.044780004769563675,
      "learning_rate": 0.0001709677419354839,
      "loss": 0.3706,
      "step": 50
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 0.04220212250947952,
      "learning_rate": 0.00017032258064516128,
      "loss": 0.3701,
      "step": 51
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 0.04850425198674202,
      "learning_rate": 0.00016967741935483872,
      "loss": 0.4081,
      "step": 52
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 0.05057944357395172,
      "learning_rate": 0.00016903225806451616,
      "loss": 0.4529,
      "step": 53
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.043491996824741364,
      "learning_rate": 0.00016838709677419354,
      "loss": 0.3878,
      "step": 54
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 0.050338953733444214,
      "learning_rate": 0.00016774193548387098,
      "loss": 0.4613,
      "step": 55
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.04359426349401474,
      "learning_rate": 0.0001670967741935484,
      "loss": 0.3963,
      "step": 56
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 0.04887944832444191,
      "learning_rate": 0.0001664516129032258,
      "loss": 0.4372,
      "step": 57
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 0.04245791584253311,
      "learning_rate": 0.00016580645161290322,
      "loss": 0.4189,
      "step": 58
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 0.04671984910964966,
      "learning_rate": 0.00016516129032258066,
      "loss": 0.4687,
      "step": 59
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.0516972579061985,
      "learning_rate": 0.00016451612903225807,
      "loss": 0.4509,
      "step": 60
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 0.05010441690683365,
      "learning_rate": 0.00016387096774193548,
      "loss": 0.4297,
      "step": 61
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 0.05060860142111778,
      "learning_rate": 0.00016322580645161292,
      "loss": 0.4223,
      "step": 62
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.07058700174093246,
      "learning_rate": 0.00016258064516129034,
      "loss": 0.5173,
      "step": 63
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 0.05283621326088905,
      "learning_rate": 0.00016193548387096775,
      "loss": 0.4912,
      "step": 64
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 0.050167351961135864,
      "learning_rate": 0.00016129032258064516,
      "loss": 0.4274,
      "step": 65
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.04647490382194519,
      "learning_rate": 0.0001606451612903226,
      "loss": 0.3763,
      "step": 66
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 0.04694013670086861,
      "learning_rate": 0.00016,
      "loss": 0.3777,
      "step": 67
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 0.055086154490709305,
      "learning_rate": 0.00015935483870967743,
      "loss": 0.4416,
      "step": 68
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.0495741069316864,
      "learning_rate": 0.00015870967741935487,
      "loss": 0.4123,
      "step": 69
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.04969959706068039,
      "learning_rate": 0.00015806451612903225,
      "loss": 0.4718,
      "step": 70
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 0.050781261175870895,
      "learning_rate": 0.0001574193548387097,
      "loss": 0.357,
      "step": 71
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.05015818029642105,
      "learning_rate": 0.0001567741935483871,
      "loss": 0.368,
      "step": 72
    },
    {
      "epoch": 1.1587301587301586,
      "grad_norm": 0.05445878207683563,
      "learning_rate": 0.00015612903225806451,
      "loss": 0.4091,
      "step": 73
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 0.05132124945521355,
      "learning_rate": 0.00015548387096774195,
      "loss": 0.3593,
      "step": 74
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.05560041218996048,
      "learning_rate": 0.00015483870967741937,
      "loss": 0.4707,
      "step": 75
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 0.05316292494535446,
      "learning_rate": 0.00015419354838709678,
      "loss": 0.4299,
      "step": 76
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.05460898578166962,
      "learning_rate": 0.0001535483870967742,
      "loss": 0.3963,
      "step": 77
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 0.04898044839501381,
      "learning_rate": 0.00015290322580645163,
      "loss": 0.3508,
      "step": 78
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 0.056660592555999756,
      "learning_rate": 0.00015225806451612902,
      "loss": 0.3981,
      "step": 79
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 0.06051187962293625,
      "learning_rate": 0.00015161290322580646,
      "loss": 0.4771,
      "step": 80
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.05227714404463768,
      "learning_rate": 0.0001509677419354839,
      "loss": 0.3689,
      "step": 81
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 0.05158430337905884,
      "learning_rate": 0.00015032258064516128,
      "loss": 0.3987,
      "step": 82
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 0.06313829123973846,
      "learning_rate": 0.00014967741935483872,
      "loss": 0.5519,
      "step": 83
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.057954173535108566,
      "learning_rate": 0.00014903225806451613,
      "loss": 0.418,
      "step": 84
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 0.05872631445527077,
      "learning_rate": 0.00014838709677419355,
      "loss": 0.4099,
      "step": 85
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 0.05348013713955879,
      "learning_rate": 0.00014774193548387098,
      "loss": 0.366,
      "step": 86
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.07431694120168686,
      "learning_rate": 0.0001470967741935484,
      "loss": 0.3743,
      "step": 87
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 0.05191585794091225,
      "learning_rate": 0.0001464516129032258,
      "loss": 0.3221,
      "step": 88
    },
    {
      "epoch": 1.4126984126984126,
      "grad_norm": 0.06120302155613899,
      "learning_rate": 0.00014580645161290322,
      "loss": 0.4472,
      "step": 89
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.0612100325524807,
      "learning_rate": 0.00014516129032258066,
      "loss": 0.3836,
      "step": 90
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.05922394245862961,
      "learning_rate": 0.00014451612903225807,
      "loss": 0.3846,
      "step": 91
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 0.07110624015331268,
      "learning_rate": 0.00014387096774193549,
      "loss": 0.5364,
      "step": 92
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 0.05982879549264908,
      "learning_rate": 0.00014322580645161293,
      "loss": 0.3508,
      "step": 93
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 0.07010211795568466,
      "learning_rate": 0.00014258064516129034,
      "loss": 0.5532,
      "step": 94
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 0.056435029953718185,
      "learning_rate": 0.00014193548387096775,
      "loss": 0.4164,
      "step": 95
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.06308866292238235,
      "learning_rate": 0.00014129032258064516,
      "loss": 0.4622,
      "step": 96
    },
    {
      "epoch": 1.5396825396825395,
      "grad_norm": 0.0610683374106884,
      "learning_rate": 0.0001406451612903226,
      "loss": 0.4008,
      "step": 97
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.057326678186655045,
      "learning_rate": 0.00014,
      "loss": 0.3952,
      "step": 98
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.056850869208574295,
      "learning_rate": 0.00013935483870967743,
      "loss": 0.4134,
      "step": 99
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 0.0580170601606369,
      "learning_rate": 0.00013870967741935487,
      "loss": 0.3733,
      "step": 100
    },
    {
      "epoch": 1.6031746031746033,
      "grad_norm": 0.06000349298119545,
      "learning_rate": 0.00013806451612903225,
      "loss": 0.3956,
      "step": 101
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 0.06317882239818573,
      "learning_rate": 0.0001374193548387097,
      "loss": 0.4139,
      "step": 102
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 0.05963805317878723,
      "learning_rate": 0.0001367741935483871,
      "loss": 0.4149,
      "step": 103
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 0.06706991791725159,
      "learning_rate": 0.00013612903225806452,
      "loss": 0.3728,
      "step": 104
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.07336215674877167,
      "learning_rate": 0.00013548387096774193,
      "loss": 0.4441,
      "step": 105
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 0.06353894621133804,
      "learning_rate": 0.00013483870967741937,
      "loss": 0.4679,
      "step": 106
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 0.056121524423360825,
      "learning_rate": 0.00013419354838709678,
      "loss": 0.3952,
      "step": 107
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.06048262119293213,
      "learning_rate": 0.0001335483870967742,
      "loss": 0.3902,
      "step": 108
    },
    {
      "epoch": 1.7301587301587302,
      "grad_norm": 0.0762965977191925,
      "learning_rate": 0.00013290322580645163,
      "loss": 0.4108,
      "step": 109
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 0.06465302407741547,
      "learning_rate": 0.00013225806451612905,
      "loss": 0.4562,
      "step": 110
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 0.058679744601249695,
      "learning_rate": 0.00013161290322580646,
      "loss": 0.3935,
      "step": 111
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.06245913729071617,
      "learning_rate": 0.00013096774193548387,
      "loss": 0.4262,
      "step": 112
    },
    {
      "epoch": 1.7936507936507935,
      "grad_norm": 0.06685226410627365,
      "learning_rate": 0.0001303225806451613,
      "loss": 0.4441,
      "step": 113
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 0.06719360500574112,
      "learning_rate": 0.00012967741935483872,
      "loss": 0.4293,
      "step": 114
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 0.06173240393400192,
      "learning_rate": 0.00012903225806451613,
      "loss": 0.414,
      "step": 115
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 0.060426145792007446,
      "learning_rate": 0.00012838709677419357,
      "loss": 0.3851,
      "step": 116
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.06128440052270889,
      "learning_rate": 0.00012774193548387096,
      "loss": 0.3929,
      "step": 117
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 0.06951971352100372,
      "learning_rate": 0.0001270967741935484,
      "loss": 0.4103,
      "step": 118
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.06331700086593628,
      "learning_rate": 0.0001264516129032258,
      "loss": 0.4227,
      "step": 119
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 0.06287665665149689,
      "learning_rate": 0.00012580645161290322,
      "loss": 0.4573,
      "step": 120
    },
    {
      "epoch": 1.9206349206349205,
      "grad_norm": 0.06498024612665176,
      "learning_rate": 0.00012516129032258066,
      "loss": 0.4246,
      "step": 121
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 0.06061136722564697,
      "learning_rate": 0.00012451612903225808,
      "loss": 0.3524,
      "step": 122
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 0.06297581642866135,
      "learning_rate": 0.0001238709677419355,
      "loss": 0.3967,
      "step": 123
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 0.06562827527523041,
      "learning_rate": 0.0001232258064516129,
      "loss": 0.5015,
      "step": 124
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 0.056962013244628906,
      "learning_rate": 0.00012258064516129034,
      "loss": 0.3517,
      "step": 125
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.08107832819223404,
      "learning_rate": 0.00012193548387096774,
      "loss": 0.3684,
      "step": 126
    },
    {
      "epoch": 2.015873015873016,
      "grad_norm": 0.05904068052768707,
      "learning_rate": 0.00012129032258064516,
      "loss": 0.3546,
      "step": 127
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 0.06134658679366112,
      "learning_rate": 0.00012064516129032259,
      "loss": 0.4449,
      "step": 128
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 0.06628772616386414,
      "learning_rate": 0.00012,
      "loss": 0.4863,
      "step": 129
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 0.062417447566986084,
      "learning_rate": 0.00011935483870967743,
      "loss": 0.3869,
      "step": 130
    },
    {
      "epoch": 2.0793650793650795,
      "grad_norm": 0.0650833249092102,
      "learning_rate": 0.00011870967741935484,
      "loss": 0.4446,
      "step": 131
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.06941645592451096,
      "learning_rate": 0.00011806451612903227,
      "loss": 0.4471,
      "step": 132
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.06953158974647522,
      "learning_rate": 0.00011741935483870967,
      "loss": 0.3907,
      "step": 133
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 0.06276839226484299,
      "learning_rate": 0.0001167741935483871,
      "loss": 0.3897,
      "step": 134
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.06309181451797485,
      "learning_rate": 0.00011612903225806453,
      "loss": 0.3294,
      "step": 135
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 0.06554418057203293,
      "learning_rate": 0.00011548387096774193,
      "loss": 0.4456,
      "step": 136
    },
    {
      "epoch": 2.1746031746031744,
      "grad_norm": 0.06608234345912933,
      "learning_rate": 0.00011483870967741937,
      "loss": 0.3343,
      "step": 137
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.06598804146051407,
      "learning_rate": 0.00011419354838709677,
      "loss": 0.3295,
      "step": 138
    },
    {
      "epoch": 2.2063492063492065,
      "grad_norm": 0.06843389570713043,
      "learning_rate": 0.0001135483870967742,
      "loss": 0.4001,
      "step": 139
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.06988789141178131,
      "learning_rate": 0.00011290322580645163,
      "loss": 0.4552,
      "step": 140
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 0.0795336440205574,
      "learning_rate": 0.00011225806451612903,
      "loss": 0.4145,
      "step": 141
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 0.07008913159370422,
      "learning_rate": 0.00011161290322580646,
      "loss": 0.4154,
      "step": 142
    },
    {
      "epoch": 2.2698412698412698,
      "grad_norm": 0.06520912796258926,
      "learning_rate": 0.00011096774193548387,
      "loss": 0.3937,
      "step": 143
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.06989016383886337,
      "learning_rate": 0.0001103225806451613,
      "loss": 0.4056,
      "step": 144
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 0.07171572744846344,
      "learning_rate": 0.00010967741935483871,
      "loss": 0.3618,
      "step": 145
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 0.07670335471630096,
      "learning_rate": 0.00010903225806451614,
      "loss": 0.4855,
      "step": 146
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.07057314366102219,
      "learning_rate": 0.00010838709677419356,
      "loss": 0.3844,
      "step": 147
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 0.06174873560667038,
      "learning_rate": 0.00010774193548387097,
      "loss": 0.3457,
      "step": 148
    },
    {
      "epoch": 2.365079365079365,
      "grad_norm": 0.06634185463190079,
      "learning_rate": 0.0001070967741935484,
      "loss": 0.3632,
      "step": 149
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.08316557854413986,
      "learning_rate": 0.0001064516129032258,
      "loss": 0.4605,
      "step": 150
    },
    {
      "epoch": 2.3968253968253967,
      "grad_norm": 0.07217267155647278,
      "learning_rate": 0.00010580645161290324,
      "loss": 0.4137,
      "step": 151
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 0.06767100095748901,
      "learning_rate": 0.00010516129032258064,
      "loss": 0.3531,
      "step": 152
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.0704692080616951,
      "learning_rate": 0.00010451612903225806,
      "loss": 0.374,
      "step": 153
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.07895034551620483,
      "learning_rate": 0.0001038709677419355,
      "loss": 0.4001,
      "step": 154
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 0.06819166988134384,
      "learning_rate": 0.0001032258064516129,
      "loss": 0.2963,
      "step": 155
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.07240531593561172,
      "learning_rate": 0.00010258064516129033,
      "loss": 0.3912,
      "step": 156
    },
    {
      "epoch": 2.492063492063492,
      "grad_norm": 0.06818981468677521,
      "learning_rate": 0.00010193548387096774,
      "loss": 0.3848,
      "step": 157
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 0.07433660328388214,
      "learning_rate": 0.00010129032258064517,
      "loss": 0.3953,
      "step": 158
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 0.08345786482095718,
      "learning_rate": 0.00010064516129032258,
      "loss": 0.3655,
      "step": 159
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 0.08129537105560303,
      "learning_rate": 0.0001,
      "loss": 0.441,
      "step": 160
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.07453690469264984,
      "learning_rate": 9.935483870967742e-05,
      "loss": 0.3921,
      "step": 161
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.07393092662096024,
      "learning_rate": 9.870967741935484e-05,
      "loss": 0.3219,
      "step": 162
    },
    {
      "epoch": 2.5873015873015874,
      "grad_norm": 0.09069965779781342,
      "learning_rate": 9.806451612903226e-05,
      "loss": 0.4725,
      "step": 163
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 0.08251963555812836,
      "learning_rate": 9.741935483870968e-05,
      "loss": 0.4176,
      "step": 164
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 0.07162495702505112,
      "learning_rate": 9.677419354838711e-05,
      "loss": 0.3826,
      "step": 165
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 0.08166127651929855,
      "learning_rate": 9.612903225806452e-05,
      "loss": 0.4757,
      "step": 166
    },
    {
      "epoch": 2.6507936507936507,
      "grad_norm": 0.0731821209192276,
      "learning_rate": 9.548387096774195e-05,
      "loss": 0.3655,
      "step": 167
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.07038106769323349,
      "learning_rate": 9.483870967741936e-05,
      "loss": 0.3368,
      "step": 168
    },
    {
      "epoch": 2.682539682539683,
      "grad_norm": 0.07405439764261246,
      "learning_rate": 9.419354838709677e-05,
      "loss": 0.4156,
      "step": 169
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 0.0717744380235672,
      "learning_rate": 9.35483870967742e-05,
      "loss": 0.399,
      "step": 170
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 0.07462441176176071,
      "learning_rate": 9.290322580645162e-05,
      "loss": 0.3906,
      "step": 171
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 0.06561123579740524,
      "learning_rate": 9.225806451612904e-05,
      "loss": 0.3011,
      "step": 172
    },
    {
      "epoch": 2.746031746031746,
      "grad_norm": 0.08058612793684006,
      "learning_rate": 9.161290322580646e-05,
      "loss": 0.4236,
      "step": 173
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.07808157801628113,
      "learning_rate": 9.096774193548387e-05,
      "loss": 0.3473,
      "step": 174
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.07154671847820282,
      "learning_rate": 9.032258064516129e-05,
      "loss": 0.3177,
      "step": 175
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 0.07268234342336655,
      "learning_rate": 8.967741935483871e-05,
      "loss": 0.3573,
      "step": 176
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 0.07587035745382309,
      "learning_rate": 8.903225806451614e-05,
      "loss": 0.3512,
      "step": 177
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 0.0868161991238594,
      "learning_rate": 8.838709677419355e-05,
      "loss": 0.4934,
      "step": 178
    },
    {
      "epoch": 2.8412698412698414,
      "grad_norm": 0.0768859013915062,
      "learning_rate": 8.774193548387098e-05,
      "loss": 0.4015,
      "step": 179
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.07788753509521484,
      "learning_rate": 8.709677419354839e-05,
      "loss": 0.414,
      "step": 180
    },
    {
      "epoch": 2.873015873015873,
      "grad_norm": 0.07881541550159454,
      "learning_rate": 8.645161290322581e-05,
      "loss": 0.4283,
      "step": 181
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.08755245059728622,
      "learning_rate": 8.580645161290323e-05,
      "loss": 0.5273,
      "step": 182
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 0.06889556348323822,
      "learning_rate": 8.516129032258064e-05,
      "loss": 0.3279,
      "step": 183
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 0.09283960610628128,
      "learning_rate": 8.451612903225808e-05,
      "loss": 0.4852,
      "step": 184
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 0.07589731365442276,
      "learning_rate": 8.387096774193549e-05,
      "loss": 0.406,
      "step": 185
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.0658230409026146,
      "learning_rate": 8.32258064516129e-05,
      "loss": 0.3022,
      "step": 186
    },
    {
      "epoch": 2.9682539682539684,
      "grad_norm": 0.0715320035815239,
      "learning_rate": 8.258064516129033e-05,
      "loss": 0.3479,
      "step": 187
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 0.07289306819438934,
      "learning_rate": 8.193548387096774e-05,
      "loss": 0.3468,
      "step": 188
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.10511019825935364,
      "learning_rate": 8.129032258064517e-05,
      "loss": 0.4714,
      "step": 189
    },
    {
      "epoch": 3.015873015873016,
      "grad_norm": 0.06982027739286423,
      "learning_rate": 8.064516129032258e-05,
      "loss": 0.3554,
      "step": 190
    },
    {
      "epoch": 3.0317460317460316,
      "grad_norm": 0.06532452255487442,
      "learning_rate": 8e-05,
      "loss": 0.3418,
      "step": 191
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 0.06501694023609161,
      "learning_rate": 7.935483870967743e-05,
      "loss": 0.3253,
      "step": 192
    },
    {
      "epoch": 3.0634920634920633,
      "grad_norm": 0.07637125998735428,
      "learning_rate": 7.870967741935484e-05,
      "loss": 0.3856,
      "step": 193
    },
    {
      "epoch": 3.0793650793650795,
      "grad_norm": 0.0755516067147255,
      "learning_rate": 7.806451612903226e-05,
      "loss": 0.325,
      "step": 194
    },
    {
      "epoch": 3.0952380952380953,
      "grad_norm": 0.07738368958234787,
      "learning_rate": 7.741935483870968e-05,
      "loss": 0.3966,
      "step": 195
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.07165656238794327,
      "learning_rate": 7.67741935483871e-05,
      "loss": 0.3273,
      "step": 196
    },
    {
      "epoch": 3.126984126984127,
      "grad_norm": 0.07704345136880875,
      "learning_rate": 7.612903225806451e-05,
      "loss": 0.366,
      "step": 197
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.0821310356259346,
      "learning_rate": 7.548387096774195e-05,
      "loss": 0.3884,
      "step": 198
    },
    {
      "epoch": 3.1587301587301586,
      "grad_norm": 0.0752238929271698,
      "learning_rate": 7.483870967741936e-05,
      "loss": 0.3918,
      "step": 199
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 0.08072386682033539,
      "learning_rate": 7.419354838709677e-05,
      "loss": 0.3687,
      "step": 200
    },
    {
      "epoch": 3.1904761904761907,
      "grad_norm": 0.08581508696079254,
      "learning_rate": 7.35483870967742e-05,
      "loss": 0.4945,
      "step": 201
    },
    {
      "epoch": 3.2063492063492065,
      "grad_norm": 0.07127813249826431,
      "learning_rate": 7.290322580645161e-05,
      "loss": 0.3334,
      "step": 202
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.08057837188243866,
      "learning_rate": 7.225806451612904e-05,
      "loss": 0.3694,
      "step": 203
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 0.07467483729124069,
      "learning_rate": 7.161290322580646e-05,
      "loss": 0.374,
      "step": 204
    },
    {
      "epoch": 3.253968253968254,
      "grad_norm": 0.08467676490545273,
      "learning_rate": 7.096774193548388e-05,
      "loss": 0.3907,
      "step": 205
    },
    {
      "epoch": 3.2698412698412698,
      "grad_norm": 0.07899701595306396,
      "learning_rate": 7.03225806451613e-05,
      "loss": 0.4401,
      "step": 206
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.08768874406814575,
      "learning_rate": 6.967741935483871e-05,
      "loss": 0.4269,
      "step": 207
    },
    {
      "epoch": 3.3015873015873014,
      "grad_norm": 0.07823817431926727,
      "learning_rate": 6.903225806451613e-05,
      "loss": 0.3243,
      "step": 208
    },
    {
      "epoch": 3.317460317460317,
      "grad_norm": 0.07664670795202255,
      "learning_rate": 6.838709677419355e-05,
      "loss": 0.3713,
      "step": 209
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.08271868526935577,
      "learning_rate": 6.774193548387096e-05,
      "loss": 0.3643,
      "step": 210
    },
    {
      "epoch": 3.3492063492063493,
      "grad_norm": 0.07274874299764633,
      "learning_rate": 6.709677419354839e-05,
      "loss": 0.3292,
      "step": 211
    },
    {
      "epoch": 3.365079365079365,
      "grad_norm": 0.07834668457508087,
      "learning_rate": 6.645161290322582e-05,
      "loss": 0.3481,
      "step": 212
    },
    {
      "epoch": 3.380952380952381,
      "grad_norm": 0.0786781907081604,
      "learning_rate": 6.580645161290323e-05,
      "loss": 0.3978,
      "step": 213
    },
    {
      "epoch": 3.3968253968253967,
      "grad_norm": 0.07326188683509827,
      "learning_rate": 6.516129032258065e-05,
      "loss": 0.3333,
      "step": 214
    },
    {
      "epoch": 3.4126984126984126,
      "grad_norm": 0.0881933644413948,
      "learning_rate": 6.451612903225807e-05,
      "loss": 0.5098,
      "step": 215
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 0.08044983446598053,
      "learning_rate": 6.387096774193548e-05,
      "loss": 0.3848,
      "step": 216
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.08375199139118195,
      "learning_rate": 6.32258064516129e-05,
      "loss": 0.4335,
      "step": 217
    },
    {
      "epoch": 3.4603174603174605,
      "grad_norm": 0.07974772900342941,
      "learning_rate": 6.258064516129033e-05,
      "loss": 0.3713,
      "step": 218
    },
    {
      "epoch": 3.4761904761904763,
      "grad_norm": 0.0805010199546814,
      "learning_rate": 6.193548387096774e-05,
      "loss": 0.3614,
      "step": 219
    },
    {
      "epoch": 3.492063492063492,
      "grad_norm": 0.08584393560886383,
      "learning_rate": 6.129032258064517e-05,
      "loss": 0.3647,
      "step": 220
    },
    {
      "epoch": 3.507936507936508,
      "grad_norm": 0.08542750775814056,
      "learning_rate": 6.064516129032258e-05,
      "loss": 0.4204,
      "step": 221
    },
    {
      "epoch": 3.5238095238095237,
      "grad_norm": 0.08203855156898499,
      "learning_rate": 6e-05,
      "loss": 0.4421,
      "step": 222
    },
    {
      "epoch": 3.5396825396825395,
      "grad_norm": 0.07749775052070618,
      "learning_rate": 5.935483870967742e-05,
      "loss": 0.3728,
      "step": 223
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.0761774554848671,
      "learning_rate": 5.870967741935483e-05,
      "loss": 0.3396,
      "step": 224
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 0.0912390947341919,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 0.4275,
      "step": 225
    },
    {
      "epoch": 3.5873015873015874,
      "grad_norm": 0.07812965661287308,
      "learning_rate": 5.7419354838709685e-05,
      "loss": 0.3543,
      "step": 226
    },
    {
      "epoch": 3.6031746031746033,
      "grad_norm": 0.08977531641721725,
      "learning_rate": 5.67741935483871e-05,
      "loss": 0.3674,
      "step": 227
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 0.08052665740251541,
      "learning_rate": 5.612903225806452e-05,
      "loss": 0.3581,
      "step": 228
    },
    {
      "epoch": 3.634920634920635,
      "grad_norm": 0.08387645334005356,
      "learning_rate": 5.5483870967741936e-05,
      "loss": 0.3953,
      "step": 229
    },
    {
      "epoch": 3.6507936507936507,
      "grad_norm": 0.07983753085136414,
      "learning_rate": 5.4838709677419355e-05,
      "loss": 0.3607,
      "step": 230
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.08881065249443054,
      "learning_rate": 5.419354838709678e-05,
      "loss": 0.474,
      "step": 231
    },
    {
      "epoch": 3.682539682539683,
      "grad_norm": 0.07281573116779327,
      "learning_rate": 5.35483870967742e-05,
      "loss": 0.3404,
      "step": 232
    },
    {
      "epoch": 3.6984126984126986,
      "grad_norm": 0.07702560722827911,
      "learning_rate": 5.290322580645162e-05,
      "loss": 0.3959,
      "step": 233
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 0.07960637658834457,
      "learning_rate": 5.225806451612903e-05,
      "loss": 0.3917,
      "step": 234
    },
    {
      "epoch": 3.7301587301587302,
      "grad_norm": 0.08305100351572037,
      "learning_rate": 5.161290322580645e-05,
      "loss": 0.3577,
      "step": 235
    },
    {
      "epoch": 3.746031746031746,
      "grad_norm": 0.08623701333999634,
      "learning_rate": 5.096774193548387e-05,
      "loss": 0.4062,
      "step": 236
    },
    {
      "epoch": 3.761904761904762,
      "grad_norm": 0.08199374377727509,
      "learning_rate": 5.032258064516129e-05,
      "loss": 0.3907,
      "step": 237
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.08283599466085434,
      "learning_rate": 4.967741935483871e-05,
      "loss": 0.3711,
      "step": 238
    },
    {
      "epoch": 3.7936507936507935,
      "grad_norm": 0.07515156269073486,
      "learning_rate": 4.903225806451613e-05,
      "loss": 0.3331,
      "step": 239
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 0.08058030158281326,
      "learning_rate": 4.8387096774193554e-05,
      "loss": 0.3674,
      "step": 240
    },
    {
      "epoch": 3.825396825396825,
      "grad_norm": 0.09267976880073547,
      "learning_rate": 4.774193548387097e-05,
      "loss": 0.4892,
      "step": 241
    },
    {
      "epoch": 3.8412698412698414,
      "grad_norm": 0.07741719484329224,
      "learning_rate": 4.7096774193548385e-05,
      "loss": 0.349,
      "step": 242
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 0.07989339530467987,
      "learning_rate": 4.645161290322581e-05,
      "loss": 0.3596,
      "step": 243
    },
    {
      "epoch": 3.873015873015873,
      "grad_norm": 0.0878860354423523,
      "learning_rate": 4.580645161290323e-05,
      "loss": 0.3255,
      "step": 244
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.08578049391508102,
      "learning_rate": 4.516129032258064e-05,
      "loss": 0.3755,
      "step": 245
    },
    {
      "epoch": 3.9047619047619047,
      "grad_norm": 0.0809776782989502,
      "learning_rate": 4.451612903225807e-05,
      "loss": 0.3844,
      "step": 246
    },
    {
      "epoch": 3.9206349206349205,
      "grad_norm": 0.07778040319681168,
      "learning_rate": 4.387096774193549e-05,
      "loss": 0.3235,
      "step": 247
    },
    {
      "epoch": 3.9365079365079367,
      "grad_norm": 0.08840771019458771,
      "learning_rate": 4.322580645161291e-05,
      "loss": 0.3934,
      "step": 248
    },
    {
      "epoch": 3.9523809523809526,
      "grad_norm": 0.08246558904647827,
      "learning_rate": 4.258064516129032e-05,
      "loss": 0.3037,
      "step": 249
    },
    {
      "epoch": 3.9682539682539684,
      "grad_norm": 0.08614592254161835,
      "learning_rate": 4.1935483870967746e-05,
      "loss": 0.4223,
      "step": 250
    },
    {
      "epoch": 3.984126984126984,
      "grad_norm": 0.08802183717489243,
      "learning_rate": 4.1290322580645165e-05,
      "loss": 0.3943,
      "step": 251
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.13731680810451508,
      "learning_rate": 4.0645161290322584e-05,
      "loss": 0.5051,
      "step": 252
    },
    {
      "epoch": 4.015873015873016,
      "grad_norm": 0.08359129726886749,
      "learning_rate": 4e-05,
      "loss": 0.435,
      "step": 253
    },
    {
      "epoch": 4.031746031746032,
      "grad_norm": 0.08722721040248871,
      "learning_rate": 3.935483870967742e-05,
      "loss": 0.4128,
      "step": 254
    },
    {
      "epoch": 4.0476190476190474,
      "grad_norm": 0.07869764417409897,
      "learning_rate": 3.870967741935484e-05,
      "loss": 0.3206,
      "step": 255
    },
    {
      "epoch": 4.063492063492063,
      "grad_norm": 0.08615037798881531,
      "learning_rate": 3.8064516129032254e-05,
      "loss": 0.3974,
      "step": 256
    },
    {
      "epoch": 4.079365079365079,
      "grad_norm": 0.08276639133691788,
      "learning_rate": 3.741935483870968e-05,
      "loss": 0.3605,
      "step": 257
    },
    {
      "epoch": 4.095238095238095,
      "grad_norm": 0.07948938757181168,
      "learning_rate": 3.67741935483871e-05,
      "loss": 0.3641,
      "step": 258
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.07899954915046692,
      "learning_rate": 3.612903225806452e-05,
      "loss": 0.3479,
      "step": 259
    },
    {
      "epoch": 4.1269841269841265,
      "grad_norm": 0.08239299803972244,
      "learning_rate": 3.548387096774194e-05,
      "loss": 0.3814,
      "step": 260
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 0.08166327327489853,
      "learning_rate": 3.483870967741936e-05,
      "loss": 0.3601,
      "step": 261
    },
    {
      "epoch": 4.158730158730159,
      "grad_norm": 0.08913901448249817,
      "learning_rate": 3.4193548387096776e-05,
      "loss": 0.4316,
      "step": 262
    },
    {
      "epoch": 4.174603174603175,
      "grad_norm": 0.08715775609016418,
      "learning_rate": 3.3548387096774195e-05,
      "loss": 0.375,
      "step": 263
    },
    {
      "epoch": 4.190476190476191,
      "grad_norm": 0.0927799642086029,
      "learning_rate": 3.2903225806451614e-05,
      "loss": 0.3902,
      "step": 264
    },
    {
      "epoch": 4.2063492063492065,
      "grad_norm": 0.082341767847538,
      "learning_rate": 3.2258064516129034e-05,
      "loss": 0.3357,
      "step": 265
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.08487816900014877,
      "learning_rate": 3.161290322580645e-05,
      "loss": 0.3902,
      "step": 266
    },
    {
      "epoch": 4.238095238095238,
      "grad_norm": 0.08334791660308838,
      "learning_rate": 3.096774193548387e-05,
      "loss": 0.3357,
      "step": 267
    },
    {
      "epoch": 4.253968253968254,
      "grad_norm": 0.09196804463863373,
      "learning_rate": 3.032258064516129e-05,
      "loss": 0.5195,
      "step": 268
    },
    {
      "epoch": 4.26984126984127,
      "grad_norm": 0.08687686920166016,
      "learning_rate": 2.967741935483871e-05,
      "loss": 0.4114,
      "step": 269
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.07872631400823593,
      "learning_rate": 2.9032258064516133e-05,
      "loss": 0.3435,
      "step": 270
    },
    {
      "epoch": 4.301587301587301,
      "grad_norm": 0.0854455754160881,
      "learning_rate": 2.838709677419355e-05,
      "loss": 0.393,
      "step": 271
    },
    {
      "epoch": 4.317460317460317,
      "grad_norm": 0.08513186126947403,
      "learning_rate": 2.7741935483870968e-05,
      "loss": 0.3662,
      "step": 272
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.0790388286113739,
      "learning_rate": 2.709677419354839e-05,
      "loss": 0.3115,
      "step": 273
    },
    {
      "epoch": 4.349206349206349,
      "grad_norm": 0.08724933862686157,
      "learning_rate": 2.645161290322581e-05,
      "loss": 0.4278,
      "step": 274
    },
    {
      "epoch": 4.365079365079365,
      "grad_norm": 0.08447098731994629,
      "learning_rate": 2.5806451612903226e-05,
      "loss": 0.3835,
      "step": 275
    },
    {
      "epoch": 4.380952380952381,
      "grad_norm": 0.08737237006425858,
      "learning_rate": 2.5161290322580645e-05,
      "loss": 0.4027,
      "step": 276
    },
    {
      "epoch": 4.396825396825397,
      "grad_norm": 0.08827514201402664,
      "learning_rate": 2.4516129032258064e-05,
      "loss": 0.3627,
      "step": 277
    },
    {
      "epoch": 4.412698412698413,
      "grad_norm": 0.082783043384552,
      "learning_rate": 2.3870967741935486e-05,
      "loss": 0.3888,
      "step": 278
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 0.0811176523566246,
      "learning_rate": 2.3225806451612906e-05,
      "loss": 0.3322,
      "step": 279
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.07645615935325623,
      "learning_rate": 2.258064516129032e-05,
      "loss": 0.3691,
      "step": 280
    },
    {
      "epoch": 4.4603174603174605,
      "grad_norm": 0.08335814625024796,
      "learning_rate": 2.1935483870967744e-05,
      "loss": 0.3427,
      "step": 281
    },
    {
      "epoch": 4.476190476190476,
      "grad_norm": 0.08256904035806656,
      "learning_rate": 2.129032258064516e-05,
      "loss": 0.3724,
      "step": 282
    },
    {
      "epoch": 4.492063492063492,
      "grad_norm": 0.08157655596733093,
      "learning_rate": 2.0645161290322582e-05,
      "loss": 0.3444,
      "step": 283
    },
    {
      "epoch": 4.507936507936508,
      "grad_norm": 0.08123907446861267,
      "learning_rate": 2e-05,
      "loss": 0.3031,
      "step": 284
    },
    {
      "epoch": 4.523809523809524,
      "grad_norm": 0.08395569771528244,
      "learning_rate": 1.935483870967742e-05,
      "loss": 0.3276,
      "step": 285
    },
    {
      "epoch": 4.5396825396825395,
      "grad_norm": 0.09312883764505386,
      "learning_rate": 1.870967741935484e-05,
      "loss": 0.3943,
      "step": 286
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.07947432994842529,
      "learning_rate": 1.806451612903226e-05,
      "loss": 0.3374,
      "step": 287
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.08512399345636368,
      "learning_rate": 1.741935483870968e-05,
      "loss": 0.3528,
      "step": 288
    },
    {
      "epoch": 4.587301587301587,
      "grad_norm": 0.08190086483955383,
      "learning_rate": 1.6774193548387098e-05,
      "loss": 0.3672,
      "step": 289
    },
    {
      "epoch": 4.603174603174603,
      "grad_norm": 0.07899173349142075,
      "learning_rate": 1.6129032258064517e-05,
      "loss": 0.328,
      "step": 290
    },
    {
      "epoch": 4.619047619047619,
      "grad_norm": 0.07686494290828705,
      "learning_rate": 1.5483870967741936e-05,
      "loss": 0.2839,
      "step": 291
    },
    {
      "epoch": 4.634920634920634,
      "grad_norm": 0.0852079764008522,
      "learning_rate": 1.4838709677419355e-05,
      "loss": 0.3661,
      "step": 292
    },
    {
      "epoch": 4.650793650793651,
      "grad_norm": 0.08192063868045807,
      "learning_rate": 1.4193548387096774e-05,
      "loss": 0.3475,
      "step": 293
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.09131912142038345,
      "learning_rate": 1.3548387096774195e-05,
      "loss": 0.3877,
      "step": 294
    },
    {
      "epoch": 4.682539682539683,
      "grad_norm": 0.08222337067127228,
      "learning_rate": 1.2903225806451613e-05,
      "loss": 0.3314,
      "step": 295
    },
    {
      "epoch": 4.698412698412699,
      "grad_norm": 0.07738342881202698,
      "learning_rate": 1.2258064516129032e-05,
      "loss": 0.3022,
      "step": 296
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 0.08114346116781235,
      "learning_rate": 1.1612903225806453e-05,
      "loss": 0.3456,
      "step": 297
    },
    {
      "epoch": 4.73015873015873,
      "grad_norm": 0.07820428162813187,
      "learning_rate": 1.0967741935483872e-05,
      "loss": 0.2932,
      "step": 298
    },
    {
      "epoch": 4.746031746031746,
      "grad_norm": 0.08722956478595734,
      "learning_rate": 1.0322580645161291e-05,
      "loss": 0.394,
      "step": 299
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 0.08381213992834091,
      "learning_rate": 9.67741935483871e-06,
      "loss": 0.3892,
      "step": 300
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 0.08995606750249863,
      "learning_rate": 9.03225806451613e-06,
      "loss": 0.3802,
      "step": 301
    },
    {
      "epoch": 4.7936507936507935,
      "grad_norm": 0.08642221242189407,
      "learning_rate": 8.387096774193549e-06,
      "loss": 0.3311,
      "step": 302
    },
    {
      "epoch": 4.809523809523809,
      "grad_norm": 0.08770199120044708,
      "learning_rate": 7.741935483870968e-06,
      "loss": 0.4361,
      "step": 303
    },
    {
      "epoch": 4.825396825396825,
      "grad_norm": 0.08046238869428635,
      "learning_rate": 7.096774193548387e-06,
      "loss": 0.3889,
      "step": 304
    },
    {
      "epoch": 4.841269841269841,
      "grad_norm": 0.08020899444818497,
      "learning_rate": 6.451612903225806e-06,
      "loss": 0.3817,
      "step": 305
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.08722624182701111,
      "learning_rate": 5.806451612903226e-06,
      "loss": 0.4536,
      "step": 306
    },
    {
      "epoch": 4.8730158730158735,
      "grad_norm": 0.08247951418161392,
      "learning_rate": 5.161290322580646e-06,
      "loss": 0.3255,
      "step": 307
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.07934007048606873,
      "learning_rate": 4.516129032258065e-06,
      "loss": 0.375,
      "step": 308
    },
    {
      "epoch": 4.904761904761905,
      "grad_norm": 0.08798835426568985,
      "learning_rate": 3.870967741935484e-06,
      "loss": 0.4408,
      "step": 309
    },
    {
      "epoch": 4.920634920634921,
      "grad_norm": 0.08316712826490402,
      "learning_rate": 3.225806451612903e-06,
      "loss": 0.3328,
      "step": 310
    },
    {
      "epoch": 4.936507936507937,
      "grad_norm": 0.08306081593036652,
      "learning_rate": 2.580645161290323e-06,
      "loss": 0.412,
      "step": 311
    },
    {
      "epoch": 4.9523809523809526,
      "grad_norm": 0.08388376981019974,
      "learning_rate": 1.935483870967742e-06,
      "loss": 0.3586,
      "step": 312
    },
    {
      "epoch": 4.968253968253968,
      "grad_norm": 0.08259361982345581,
      "learning_rate": 1.2903225806451614e-06,
      "loss": 0.3515,
      "step": 313
    },
    {
      "epoch": 4.984126984126984,
      "grad_norm": 0.08576295524835587,
      "learning_rate": 6.451612903225807e-07,
      "loss": 0.364,
      "step": 314
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.10586319863796234,
      "learning_rate": 0.0,
      "loss": 0.3224,
      "step": 315
    }
  ],
  "logging_steps": 1,
  "max_steps": 315,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.105903549739745e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
