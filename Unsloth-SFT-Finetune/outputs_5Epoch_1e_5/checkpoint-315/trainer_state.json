{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 315,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 0.07576869428157806,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.5031,
      "step": 1
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 0.10457322746515274,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.6923,
      "step": 2
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.0913715660572052,
      "learning_rate": 6e-06,
      "loss": 0.6222,
      "step": 3
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 0.07139355689287186,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.4547,
      "step": 4
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 0.08250463008880615,
      "learning_rate": 1e-05,
      "loss": 0.514,
      "step": 5
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.08289271593093872,
      "learning_rate": 9.967741935483871e-06,
      "loss": 0.5084,
      "step": 6
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.077050119638443,
      "learning_rate": 9.935483870967742e-06,
      "loss": 0.4689,
      "step": 7
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 0.09212721884250641,
      "learning_rate": 9.903225806451614e-06,
      "loss": 0.6245,
      "step": 8
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.086361825466156,
      "learning_rate": 9.870967741935485e-06,
      "loss": 0.6088,
      "step": 9
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 0.07856345176696777,
      "learning_rate": 9.838709677419356e-06,
      "loss": 0.4405,
      "step": 10
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 0.07758528739213943,
      "learning_rate": 9.806451612903226e-06,
      "loss": 0.4722,
      "step": 11
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.07548371702432632,
      "learning_rate": 9.774193548387097e-06,
      "loss": 0.5148,
      "step": 12
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 0.08104866743087769,
      "learning_rate": 9.74193548387097e-06,
      "loss": 0.4943,
      "step": 13
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.08720286935567856,
      "learning_rate": 9.70967741935484e-06,
      "loss": 0.5342,
      "step": 14
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.0870775356888771,
      "learning_rate": 9.67741935483871e-06,
      "loss": 0.5672,
      "step": 15
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 0.0867704227566719,
      "learning_rate": 9.645161290322581e-06,
      "loss": 0.5462,
      "step": 16
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 0.08459436893463135,
      "learning_rate": 9.612903225806453e-06,
      "loss": 0.5224,
      "step": 17
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.08022835850715637,
      "learning_rate": 9.580645161290322e-06,
      "loss": 0.5462,
      "step": 18
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 0.08200905472040176,
      "learning_rate": 9.548387096774195e-06,
      "loss": 0.5731,
      "step": 19
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 0.09121935069561005,
      "learning_rate": 9.516129032258065e-06,
      "loss": 0.6323,
      "step": 20
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.08225414901971817,
      "learning_rate": 9.483870967741936e-06,
      "loss": 0.6049,
      "step": 21
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 0.07355564087629318,
      "learning_rate": 9.451612903225808e-06,
      "loss": 0.4457,
      "step": 22
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 0.08455807715654373,
      "learning_rate": 9.419354838709677e-06,
      "loss": 0.5809,
      "step": 23
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.08613986521959305,
      "learning_rate": 9.38709677419355e-06,
      "loss": 0.5198,
      "step": 24
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 0.09283928573131561,
      "learning_rate": 9.35483870967742e-06,
      "loss": 0.5779,
      "step": 25
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 0.0827762633562088,
      "learning_rate": 9.32258064516129e-06,
      "loss": 0.6115,
      "step": 26
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.07202994078397751,
      "learning_rate": 9.290322580645163e-06,
      "loss": 0.5032,
      "step": 27
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.0751582682132721,
      "learning_rate": 9.258064516129034e-06,
      "loss": 0.5334,
      "step": 28
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 0.06979353725910187,
      "learning_rate": 9.225806451612904e-06,
      "loss": 0.4915,
      "step": 29
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.070015087723732,
      "learning_rate": 9.193548387096775e-06,
      "loss": 0.5453,
      "step": 30
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 0.07193941622972488,
      "learning_rate": 9.161290322580645e-06,
      "loss": 0.5081,
      "step": 31
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 0.0702451840043068,
      "learning_rate": 9.129032258064518e-06,
      "loss": 0.4656,
      "step": 32
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 0.06974975764751434,
      "learning_rate": 9.096774193548388e-06,
      "loss": 0.5073,
      "step": 33
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 0.07286100834608078,
      "learning_rate": 9.064516129032259e-06,
      "loss": 0.5806,
      "step": 34
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.06441924721002579,
      "learning_rate": 9.03225806451613e-06,
      "loss": 0.4595,
      "step": 35
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.06842779368162155,
      "learning_rate": 9e-06,
      "loss": 0.548,
      "step": 36
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 0.05986151099205017,
      "learning_rate": 8.967741935483871e-06,
      "loss": 0.4359,
      "step": 37
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 0.06746941059827805,
      "learning_rate": 8.935483870967743e-06,
      "loss": 0.5664,
      "step": 38
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 0.06997179239988327,
      "learning_rate": 8.903225806451614e-06,
      "loss": 0.5355,
      "step": 39
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 0.064387246966362,
      "learning_rate": 8.870967741935484e-06,
      "loss": 0.457,
      "step": 40
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 0.058707479387521744,
      "learning_rate": 8.838709677419357e-06,
      "loss": 0.4904,
      "step": 41
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.06547928601503372,
      "learning_rate": 8.806451612903226e-06,
      "loss": 0.5184,
      "step": 42
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 0.058285679668188095,
      "learning_rate": 8.774193548387098e-06,
      "loss": 0.4691,
      "step": 43
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 0.0674382895231247,
      "learning_rate": 8.741935483870969e-06,
      "loss": 0.5521,
      "step": 44
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.06507391482591629,
      "learning_rate": 8.70967741935484e-06,
      "loss": 0.5561,
      "step": 45
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 0.05914637818932533,
      "learning_rate": 8.677419354838712e-06,
      "loss": 0.4429,
      "step": 46
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 0.06131543591618538,
      "learning_rate": 8.64516129032258e-06,
      "loss": 0.5204,
      "step": 47
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.06465722620487213,
      "learning_rate": 8.612903225806453e-06,
      "loss": 0.5158,
      "step": 48
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.060691215097904205,
      "learning_rate": 8.580645161290323e-06,
      "loss": 0.4751,
      "step": 49
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 0.05302172899246216,
      "learning_rate": 8.548387096774194e-06,
      "loss": 0.4363,
      "step": 50
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 0.06049170717597008,
      "learning_rate": 8.516129032258065e-06,
      "loss": 0.4441,
      "step": 51
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 0.05741098150610924,
      "learning_rate": 8.483870967741937e-06,
      "loss": 0.4771,
      "step": 52
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 0.05899903178215027,
      "learning_rate": 8.451612903225808e-06,
      "loss": 0.5274,
      "step": 53
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.05903957411646843,
      "learning_rate": 8.419354838709678e-06,
      "loss": 0.4609,
      "step": 54
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 0.06402431428432465,
      "learning_rate": 8.387096774193549e-06,
      "loss": 0.5395,
      "step": 55
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.05392368137836456,
      "learning_rate": 8.35483870967742e-06,
      "loss": 0.4644,
      "step": 56
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 0.05917578563094139,
      "learning_rate": 8.322580645161292e-06,
      "loss": 0.5139,
      "step": 57
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 0.0571153499186039,
      "learning_rate": 8.29032258064516e-06,
      "loss": 0.4941,
      "step": 58
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 0.06195475533604622,
      "learning_rate": 8.258064516129033e-06,
      "loss": 0.5475,
      "step": 59
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.05870894342660904,
      "learning_rate": 8.225806451612904e-06,
      "loss": 0.5209,
      "step": 60
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 0.05805911123752594,
      "learning_rate": 8.193548387096774e-06,
      "loss": 0.502,
      "step": 61
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 0.0575631707906723,
      "learning_rate": 8.161290322580647e-06,
      "loss": 0.4942,
      "step": 62
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.07296154648065567,
      "learning_rate": 8.129032258064517e-06,
      "loss": 0.6025,
      "step": 63
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 0.06405603885650635,
      "learning_rate": 8.096774193548388e-06,
      "loss": 0.5806,
      "step": 64
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 0.05352560803294182,
      "learning_rate": 8.064516129032258e-06,
      "loss": 0.5017,
      "step": 65
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.05680715665221214,
      "learning_rate": 8.032258064516129e-06,
      "loss": 0.4611,
      "step": 66
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 0.05451000854372978,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.4591,
      "step": 67
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 0.056967251002788544,
      "learning_rate": 7.967741935483872e-06,
      "loss": 0.5298,
      "step": 68
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.05553979054093361,
      "learning_rate": 7.935483870967743e-06,
      "loss": 0.4894,
      "step": 69
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.0565379299223423,
      "learning_rate": 7.903225806451613e-06,
      "loss": 0.5559,
      "step": 70
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 0.050194259732961655,
      "learning_rate": 7.870967741935484e-06,
      "loss": 0.4307,
      "step": 71
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.050213512033224106,
      "learning_rate": 7.838709677419354e-06,
      "loss": 0.4468,
      "step": 72
    },
    {
      "epoch": 1.1587301587301586,
      "grad_norm": 0.05443916469812393,
      "learning_rate": 7.806451612903227e-06,
      "loss": 0.4846,
      "step": 73
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 0.05081924423575401,
      "learning_rate": 7.774193548387097e-06,
      "loss": 0.4329,
      "step": 74
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.053956907242536545,
      "learning_rate": 7.741935483870968e-06,
      "loss": 0.5546,
      "step": 75
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 0.054170675575733185,
      "learning_rate": 7.70967741935484e-06,
      "loss": 0.5102,
      "step": 76
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.05648219957947731,
      "learning_rate": 7.67741935483871e-06,
      "loss": 0.4762,
      "step": 77
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 0.049886662513017654,
      "learning_rate": 7.645161290322582e-06,
      "loss": 0.4321,
      "step": 78
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 0.05012383684515953,
      "learning_rate": 7.612903225806451e-06,
      "loss": 0.4688,
      "step": 79
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 0.05770634487271309,
      "learning_rate": 7.580645161290323e-06,
      "loss": 0.5623,
      "step": 80
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.048410430550575256,
      "learning_rate": 7.548387096774194e-06,
      "loss": 0.444,
      "step": 81
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 0.0519765205681324,
      "learning_rate": 7.516129032258065e-06,
      "loss": 0.4773,
      "step": 82
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 0.060480114072561264,
      "learning_rate": 7.483870967741936e-06,
      "loss": 0.6426,
      "step": 83
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.05672495812177658,
      "learning_rate": 7.451612903225807e-06,
      "loss": 0.507,
      "step": 84
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 0.05194208025932312,
      "learning_rate": 7.4193548387096784e-06,
      "loss": 0.4854,
      "step": 85
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 0.048124607652425766,
      "learning_rate": 7.38709677419355e-06,
      "loss": 0.437,
      "step": 86
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.047847434878349304,
      "learning_rate": 7.35483870967742e-06,
      "loss": 0.4492,
      "step": 87
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 0.04483857750892639,
      "learning_rate": 7.322580645161291e-06,
      "loss": 0.3893,
      "step": 88
    },
    {
      "epoch": 1.4126984126984126,
      "grad_norm": 0.052421312779188156,
      "learning_rate": 7.290322580645162e-06,
      "loss": 0.5229,
      "step": 89
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.04974312335252762,
      "learning_rate": 7.258064516129033e-06,
      "loss": 0.4538,
      "step": 90
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.04917997121810913,
      "learning_rate": 7.225806451612903e-06,
      "loss": 0.4612,
      "step": 91
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 0.05726848170161247,
      "learning_rate": 7.1935483870967745e-06,
      "loss": 0.6226,
      "step": 92
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 0.0502915158867836,
      "learning_rate": 7.161290322580646e-06,
      "loss": 0.4268,
      "step": 93
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 0.05914531648159027,
      "learning_rate": 7.1290322580645166e-06,
      "loss": 0.6458,
      "step": 94
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 0.05301593616604805,
      "learning_rate": 7.096774193548388e-06,
      "loss": 0.4943,
      "step": 95
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.05056071653962135,
      "learning_rate": 7.064516129032259e-06,
      "loss": 0.5432,
      "step": 96
    },
    {
      "epoch": 1.5396825396825395,
      "grad_norm": 0.05015300586819649,
      "learning_rate": 7.03225806451613e-06,
      "loss": 0.4777,
      "step": 97
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.054059211164712906,
      "learning_rate": 7e-06,
      "loss": 0.4728,
      "step": 98
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.05166726931929588,
      "learning_rate": 6.967741935483871e-06,
      "loss": 0.4915,
      "step": 99
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 0.049890875816345215,
      "learning_rate": 6.935483870967743e-06,
      "loss": 0.4503,
      "step": 100
    },
    {
      "epoch": 1.6031746031746033,
      "grad_norm": 0.04791245236992836,
      "learning_rate": 6.9032258064516135e-06,
      "loss": 0.4656,
      "step": 101
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 0.05034768208861351,
      "learning_rate": 6.870967741935485e-06,
      "loss": 0.4925,
      "step": 102
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 0.05235103890299797,
      "learning_rate": 6.838709677419355e-06,
      "loss": 0.4893,
      "step": 103
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 0.04960734024643898,
      "learning_rate": 6.806451612903226e-06,
      "loss": 0.4414,
      "step": 104
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.050720274448394775,
      "learning_rate": 6.774193548387097e-06,
      "loss": 0.5202,
      "step": 105
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 0.052476827055215836,
      "learning_rate": 6.741935483870968e-06,
      "loss": 0.5502,
      "step": 106
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 0.047029346227645874,
      "learning_rate": 6.70967741935484e-06,
      "loss": 0.4676,
      "step": 107
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.04850948229432106,
      "learning_rate": 6.67741935483871e-06,
      "loss": 0.4618,
      "step": 108
    },
    {
      "epoch": 1.7301587301587302,
      "grad_norm": 0.05215098336338997,
      "learning_rate": 6.645161290322582e-06,
      "loss": 0.4878,
      "step": 109
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 0.049728427082300186,
      "learning_rate": 6.612903225806452e-06,
      "loss": 0.5348,
      "step": 110
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 0.04581483453512192,
      "learning_rate": 6.580645161290323e-06,
      "loss": 0.4605,
      "step": 111
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.04962595924735069,
      "learning_rate": 6.548387096774194e-06,
      "loss": 0.5008,
      "step": 112
    },
    {
      "epoch": 1.7936507936507935,
      "grad_norm": 0.04943176358938217,
      "learning_rate": 6.516129032258065e-06,
      "loss": 0.5129,
      "step": 113
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 0.05069313198328018,
      "learning_rate": 6.483870967741937e-06,
      "loss": 0.5066,
      "step": 114
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 0.04725533723831177,
      "learning_rate": 6.451612903225806e-06,
      "loss": 0.4851,
      "step": 115
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 0.045258741825819016,
      "learning_rate": 6.419354838709678e-06,
      "loss": 0.4579,
      "step": 116
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.04687250033020973,
      "learning_rate": 6.3870967741935485e-06,
      "loss": 0.4633,
      "step": 117
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 0.052101053297519684,
      "learning_rate": 6.35483870967742e-06,
      "loss": 0.4819,
      "step": 118
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.046067312359809875,
      "learning_rate": 6.3225806451612906e-06,
      "loss": 0.4914,
      "step": 119
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 0.04890090599656105,
      "learning_rate": 6.290322580645162e-06,
      "loss": 0.5274,
      "step": 120
    },
    {
      "epoch": 1.9206349206349205,
      "grad_norm": 0.04585222527384758,
      "learning_rate": 6.2580645161290335e-06,
      "loss": 0.4946,
      "step": 121
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 0.045101895928382874,
      "learning_rate": 6.225806451612903e-06,
      "loss": 0.4185,
      "step": 122
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 0.046895626932382584,
      "learning_rate": 6.193548387096775e-06,
      "loss": 0.4656,
      "step": 123
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 0.05063142254948616,
      "learning_rate": 6.161290322580645e-06,
      "loss": 0.5826,
      "step": 124
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 0.044013746082782745,
      "learning_rate": 6.129032258064517e-06,
      "loss": 0.4192,
      "step": 125
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.05556971952319145,
      "learning_rate": 6.0967741935483874e-06,
      "loss": 0.4364,
      "step": 126
    },
    {
      "epoch": 2.015873015873016,
      "grad_norm": 0.04469727352261543,
      "learning_rate": 6.064516129032259e-06,
      "loss": 0.4373,
      "step": 127
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 0.048260655254125595,
      "learning_rate": 6.0322580645161295e-06,
      "loss": 0.5307,
      "step": 128
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 0.055173102766275406,
      "learning_rate": 6e-06,
      "loss": 0.5769,
      "step": 129
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 0.0484379306435585,
      "learning_rate": 5.967741935483872e-06,
      "loss": 0.4723,
      "step": 130
    },
    {
      "epoch": 2.0793650793650795,
      "grad_norm": 0.05105580389499664,
      "learning_rate": 5.935483870967742e-06,
      "loss": 0.5368,
      "step": 131
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.04959096759557724,
      "learning_rate": 5.903225806451614e-06,
      "loss": 0.5393,
      "step": 132
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.04480551555752754,
      "learning_rate": 5.8709677419354835e-06,
      "loss": 0.4672,
      "step": 133
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 0.046406034380197525,
      "learning_rate": 5.838709677419355e-06,
      "loss": 0.471,
      "step": 134
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.0455876961350441,
      "learning_rate": 5.806451612903226e-06,
      "loss": 0.4126,
      "step": 135
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 0.0486496239900589,
      "learning_rate": 5.774193548387097e-06,
      "loss": 0.5339,
      "step": 136
    },
    {
      "epoch": 2.1746031746031744,
      "grad_norm": 0.049675747752189636,
      "learning_rate": 5.7419354838709685e-06,
      "loss": 0.4135,
      "step": 137
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.04530677944421768,
      "learning_rate": 5.709677419354839e-06,
      "loss": 0.4125,
      "step": 138
    },
    {
      "epoch": 2.2063492063492065,
      "grad_norm": 0.04697703570127487,
      "learning_rate": 5.677419354838711e-06,
      "loss": 0.4862,
      "step": 139
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.051029022783041,
      "learning_rate": 5.645161290322582e-06,
      "loss": 0.5429,
      "step": 140
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 0.050406619906425476,
      "learning_rate": 5.612903225806452e-06,
      "loss": 0.5022,
      "step": 141
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 0.04615898057818413,
      "learning_rate": 5.580645161290323e-06,
      "loss": 0.4963,
      "step": 142
    },
    {
      "epoch": 2.2698412698412698,
      "grad_norm": 0.04693670570850372,
      "learning_rate": 5.548387096774194e-06,
      "loss": 0.474,
      "step": 143
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.046206530183553696,
      "learning_rate": 5.516129032258065e-06,
      "loss": 0.4876,
      "step": 144
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 0.04886864870786667,
      "learning_rate": 5.483870967741935e-06,
      "loss": 0.4458,
      "step": 145
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 0.053312718868255615,
      "learning_rate": 5.451612903225807e-06,
      "loss": 0.5791,
      "step": 146
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.04770783707499504,
      "learning_rate": 5.419354838709678e-06,
      "loss": 0.4653,
      "step": 147
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 0.043349798768758774,
      "learning_rate": 5.387096774193549e-06,
      "loss": 0.4241,
      "step": 148
    },
    {
      "epoch": 2.365079365079365,
      "grad_norm": 0.043999068439006805,
      "learning_rate": 5.35483870967742e-06,
      "loss": 0.4421,
      "step": 149
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.05031939968466759,
      "learning_rate": 5.322580645161291e-06,
      "loss": 0.5499,
      "step": 150
    },
    {
      "epoch": 2.3968253968253967,
      "grad_norm": 0.04858776926994324,
      "learning_rate": 5.290322580645162e-06,
      "loss": 0.4979,
      "step": 151
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 0.04718497395515442,
      "learning_rate": 5.258064516129032e-06,
      "loss": 0.4343,
      "step": 152
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.04577534645795822,
      "learning_rate": 5.2258064516129035e-06,
      "loss": 0.4551,
      "step": 153
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.048694416880607605,
      "learning_rate": 5.193548387096775e-06,
      "loss": 0.4873,
      "step": 154
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 0.04390623793005943,
      "learning_rate": 5.161290322580646e-06,
      "loss": 0.3721,
      "step": 155
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.04524239897727966,
      "learning_rate": 5.129032258064517e-06,
      "loss": 0.4688,
      "step": 156
    },
    {
      "epoch": 2.492063492063492,
      "grad_norm": 0.04522491991519928,
      "learning_rate": 5.096774193548387e-06,
      "loss": 0.4642,
      "step": 157
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 0.04628477990627289,
      "learning_rate": 5.064516129032258e-06,
      "loss": 0.4775,
      "step": 158
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 0.044331301003694534,
      "learning_rate": 5.032258064516129e-06,
      "loss": 0.4411,
      "step": 159
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 0.04935946688055992,
      "learning_rate": 5e-06,
      "loss": 0.5255,
      "step": 160
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.047365400940179825,
      "learning_rate": 4.967741935483871e-06,
      "loss": 0.471,
      "step": 161
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.04266296327114105,
      "learning_rate": 4.9354838709677425e-06,
      "loss": 0.3957,
      "step": 162
    },
    {
      "epoch": 2.5873015873015874,
      "grad_norm": 0.05663471296429634,
      "learning_rate": 4.903225806451613e-06,
      "loss": 0.5719,
      "step": 163
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 0.0484282560646534,
      "learning_rate": 4.870967741935485e-06,
      "loss": 0.498,
      "step": 164
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 0.04877278953790665,
      "learning_rate": 4.838709677419355e-06,
      "loss": 0.4696,
      "step": 165
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 0.05399129167199135,
      "learning_rate": 4.806451612903227e-06,
      "loss": 0.5618,
      "step": 166
    },
    {
      "epoch": 2.6507936507936507,
      "grad_norm": 0.048075899481773376,
      "learning_rate": 4.774193548387097e-06,
      "loss": 0.4454,
      "step": 167
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.046584129333496094,
      "learning_rate": 4.741935483870968e-06,
      "loss": 0.4121,
      "step": 168
    },
    {
      "epoch": 2.682539682539683,
      "grad_norm": 0.049947261810302734,
      "learning_rate": 4.7096774193548385e-06,
      "loss": 0.4945,
      "step": 169
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 0.049021027982234955,
      "learning_rate": 4.67741935483871e-06,
      "loss": 0.4883,
      "step": 170
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 0.04775698110461235,
      "learning_rate": 4.6451612903225815e-06,
      "loss": 0.4747,
      "step": 171
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 0.043206240981817245,
      "learning_rate": 4.612903225806452e-06,
      "loss": 0.3749,
      "step": 172
    },
    {
      "epoch": 2.746031746031746,
      "grad_norm": 0.046057943254709244,
      "learning_rate": 4.580645161290323e-06,
      "loss": 0.5018,
      "step": 173
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.04351181536912918,
      "learning_rate": 4.548387096774194e-06,
      "loss": 0.418,
      "step": 174
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.0467776358127594,
      "learning_rate": 4.516129032258065e-06,
      "loss": 0.3922,
      "step": 175
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 0.046912726014852524,
      "learning_rate": 4.4838709677419354e-06,
      "loss": 0.4399,
      "step": 176
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 0.045104969292879105,
      "learning_rate": 4.451612903225807e-06,
      "loss": 0.4269,
      "step": 177
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 0.05649905279278755,
      "learning_rate": 4.419354838709678e-06,
      "loss": 0.5848,
      "step": 178
    },
    {
      "epoch": 2.8412698412698414,
      "grad_norm": 0.04971959441900253,
      "learning_rate": 4.387096774193549e-06,
      "loss": 0.477,
      "step": 179
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.04849028214812279,
      "learning_rate": 4.35483870967742e-06,
      "loss": 0.4997,
      "step": 180
    },
    {
      "epoch": 2.873015873015873,
      "grad_norm": 0.04986976459622383,
      "learning_rate": 4.32258064516129e-06,
      "loss": 0.5127,
      "step": 181
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.05920875445008278,
      "learning_rate": 4.290322580645162e-06,
      "loss": 0.6301,
      "step": 182
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 0.04662853851914406,
      "learning_rate": 4.258064516129032e-06,
      "loss": 0.3976,
      "step": 183
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 0.053110964596271515,
      "learning_rate": 4.225806451612904e-06,
      "loss": 0.5727,
      "step": 184
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 0.050119832158088684,
      "learning_rate": 4.193548387096774e-06,
      "loss": 0.484,
      "step": 185
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.04189394414424896,
      "learning_rate": 4.161290322580646e-06,
      "loss": 0.3702,
      "step": 186
    },
    {
      "epoch": 2.9682539682539684,
      "grad_norm": 0.045564163476228714,
      "learning_rate": 4.1290322580645165e-06,
      "loss": 0.4198,
      "step": 187
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 0.04369593784213066,
      "learning_rate": 4.096774193548387e-06,
      "loss": 0.4219,
      "step": 188
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.0647159069776535,
      "learning_rate": 4.064516129032259e-06,
      "loss": 0.5475,
      "step": 189
    },
    {
      "epoch": 3.015873015873016,
      "grad_norm": 0.043336495757102966,
      "learning_rate": 4.032258064516129e-06,
      "loss": 0.4387,
      "step": 190
    },
    {
      "epoch": 3.0317460317460316,
      "grad_norm": 0.04717482998967171,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.431,
      "step": 191
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 0.04321788623929024,
      "learning_rate": 3.967741935483871e-06,
      "loss": 0.4047,
      "step": 192
    },
    {
      "epoch": 3.0634920634920633,
      "grad_norm": 0.047411054372787476,
      "learning_rate": 3.935483870967742e-06,
      "loss": 0.4786,
      "step": 193
    },
    {
      "epoch": 3.0793650793650795,
      "grad_norm": 0.04339059069752693,
      "learning_rate": 3.903225806451613e-06,
      "loss": 0.4106,
      "step": 194
    },
    {
      "epoch": 3.0952380952380953,
      "grad_norm": 0.04655003547668457,
      "learning_rate": 3.870967741935484e-06,
      "loss": 0.4913,
      "step": 195
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.04519449919462204,
      "learning_rate": 3.838709677419355e-06,
      "loss": 0.4115,
      "step": 196
    },
    {
      "epoch": 3.126984126984127,
      "grad_norm": 0.04665243253111839,
      "learning_rate": 3.8064516129032257e-06,
      "loss": 0.4562,
      "step": 197
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.048808395862579346,
      "learning_rate": 3.774193548387097e-06,
      "loss": 0.4803,
      "step": 198
    },
    {
      "epoch": 3.1587301587301586,
      "grad_norm": 0.049398619681596756,
      "learning_rate": 3.741935483870968e-06,
      "loss": 0.4835,
      "step": 199
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 0.04643741622567177,
      "learning_rate": 3.7096774193548392e-06,
      "loss": 0.4596,
      "step": 200
    },
    {
      "epoch": 3.1904761904761907,
      "grad_norm": 0.05806175619363785,
      "learning_rate": 3.67741935483871e-06,
      "loss": 0.6057,
      "step": 201
    },
    {
      "epoch": 3.2063492063492065,
      "grad_norm": 0.04366632550954819,
      "learning_rate": 3.645161290322581e-06,
      "loss": 0.4175,
      "step": 202
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.04738911986351013,
      "learning_rate": 3.6129032258064515e-06,
      "loss": 0.4636,
      "step": 203
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 0.04738834127783775,
      "learning_rate": 3.580645161290323e-06,
      "loss": 0.4625,
      "step": 204
    },
    {
      "epoch": 3.253968253968254,
      "grad_norm": 0.049009498208761215,
      "learning_rate": 3.548387096774194e-06,
      "loss": 0.4804,
      "step": 205
    },
    {
      "epoch": 3.2698412698412698,
      "grad_norm": 0.05112422630190849,
      "learning_rate": 3.516129032258065e-06,
      "loss": 0.5438,
      "step": 206
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.05334772542119026,
      "learning_rate": 3.4838709677419357e-06,
      "loss": 0.5207,
      "step": 207
    },
    {
      "epoch": 3.3015873015873014,
      "grad_norm": 0.04413218796253204,
      "learning_rate": 3.4516129032258067e-06,
      "loss": 0.4067,
      "step": 208
    },
    {
      "epoch": 3.317460317460317,
      "grad_norm": 0.04730646312236786,
      "learning_rate": 3.4193548387096773e-06,
      "loss": 0.4588,
      "step": 209
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.047354258596897125,
      "learning_rate": 3.3870967741935484e-06,
      "loss": 0.4523,
      "step": 210
    },
    {
      "epoch": 3.3492063492063493,
      "grad_norm": 0.04586699977517128,
      "learning_rate": 3.35483870967742e-06,
      "loss": 0.4126,
      "step": 211
    },
    {
      "epoch": 3.365079365079365,
      "grad_norm": 0.04555964842438698,
      "learning_rate": 3.322580645161291e-06,
      "loss": 0.4335,
      "step": 212
    },
    {
      "epoch": 3.380952380952381,
      "grad_norm": 0.0472799576818943,
      "learning_rate": 3.2903225806451615e-06,
      "loss": 0.4862,
      "step": 213
    },
    {
      "epoch": 3.3968253968253967,
      "grad_norm": 0.04385358840227127,
      "learning_rate": 3.2580645161290326e-06,
      "loss": 0.4077,
      "step": 214
    },
    {
      "epoch": 3.4126984126984126,
      "grad_norm": 0.05299340933561325,
      "learning_rate": 3.225806451612903e-06,
      "loss": 0.611,
      "step": 215
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 0.04968428984284401,
      "learning_rate": 3.1935483870967742e-06,
      "loss": 0.478,
      "step": 216
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.0475553534924984,
      "learning_rate": 3.1612903225806453e-06,
      "loss": 0.5314,
      "step": 217
    },
    {
      "epoch": 3.4603174603174605,
      "grad_norm": 0.047602392733097076,
      "learning_rate": 3.1290322580645167e-06,
      "loss": 0.458,
      "step": 218
    },
    {
      "epoch": 3.4761904761904763,
      "grad_norm": 0.04873068258166313,
      "learning_rate": 3.0967741935483874e-06,
      "loss": 0.4493,
      "step": 219
    },
    {
      "epoch": 3.492063492063492,
      "grad_norm": 0.05056352913379669,
      "learning_rate": 3.0645161290322584e-06,
      "loss": 0.4592,
      "step": 220
    },
    {
      "epoch": 3.507936507936508,
      "grad_norm": 0.05468128249049187,
      "learning_rate": 3.0322580645161295e-06,
      "loss": 0.521,
      "step": 221
    },
    {
      "epoch": 3.5238095238095237,
      "grad_norm": 0.049773987382650375,
      "learning_rate": 3e-06,
      "loss": 0.5267,
      "step": 222
    },
    {
      "epoch": 3.5396825396825395,
      "grad_norm": 0.04587310925126076,
      "learning_rate": 2.967741935483871e-06,
      "loss": 0.4557,
      "step": 223
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.045874256640672684,
      "learning_rate": 2.9354838709677417e-06,
      "loss": 0.426,
      "step": 224
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 0.05471725016832352,
      "learning_rate": 2.903225806451613e-06,
      "loss": 0.522,
      "step": 225
    },
    {
      "epoch": 3.5873015873015874,
      "grad_norm": 0.04681580886244774,
      "learning_rate": 2.8709677419354843e-06,
      "loss": 0.4388,
      "step": 226
    },
    {
      "epoch": 3.6031746031746033,
      "grad_norm": 0.05392635241150856,
      "learning_rate": 2.8387096774193553e-06,
      "loss": 0.4624,
      "step": 227
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 0.047292932868003845,
      "learning_rate": 2.806451612903226e-06,
      "loss": 0.4449,
      "step": 228
    },
    {
      "epoch": 3.634920634920635,
      "grad_norm": 0.05075131729245186,
      "learning_rate": 2.774193548387097e-06,
      "loss": 0.489,
      "step": 229
    },
    {
      "epoch": 3.6507936507936507,
      "grad_norm": 0.047429826110601425,
      "learning_rate": 2.7419354838709676e-06,
      "loss": 0.4506,
      "step": 230
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.05555824562907219,
      "learning_rate": 2.709677419354839e-06,
      "loss": 0.5786,
      "step": 231
    },
    {
      "epoch": 3.682539682539683,
      "grad_norm": 0.04741578921675682,
      "learning_rate": 2.67741935483871e-06,
      "loss": 0.4193,
      "step": 232
    },
    {
      "epoch": 3.6984126984126986,
      "grad_norm": 0.04699312523007393,
      "learning_rate": 2.645161290322581e-06,
      "loss": 0.4819,
      "step": 233
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 0.047096285969018936,
      "learning_rate": 2.6129032258064518e-06,
      "loss": 0.48,
      "step": 234
    },
    {
      "epoch": 3.7301587301587302,
      "grad_norm": 0.04839617386460304,
      "learning_rate": 2.580645161290323e-06,
      "loss": 0.4406,
      "step": 235
    },
    {
      "epoch": 3.746031746031746,
      "grad_norm": 0.05340711399912834,
      "learning_rate": 2.5483870967741934e-06,
      "loss": 0.4936,
      "step": 236
    },
    {
      "epoch": 3.761904761904762,
      "grad_norm": 0.04921147599816322,
      "learning_rate": 2.5161290322580645e-06,
      "loss": 0.4805,
      "step": 237
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.04838187247514725,
      "learning_rate": 2.4838709677419355e-06,
      "loss": 0.4555,
      "step": 238
    },
    {
      "epoch": 3.7936507936507935,
      "grad_norm": 0.046620991080999374,
      "learning_rate": 2.4516129032258066e-06,
      "loss": 0.4136,
      "step": 239
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 0.04862736538052559,
      "learning_rate": 2.4193548387096776e-06,
      "loss": 0.4546,
      "step": 240
    },
    {
      "epoch": 3.825396825396825,
      "grad_norm": 0.05590185895562172,
      "learning_rate": 2.3870967741935486e-06,
      "loss": 0.5943,
      "step": 241
    },
    {
      "epoch": 3.8412698412698414,
      "grad_norm": 0.04667958244681358,
      "learning_rate": 2.3548387096774193e-06,
      "loss": 0.4305,
      "step": 242
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 0.04732165113091469,
      "learning_rate": 2.3225806451612907e-06,
      "loss": 0.4425,
      "step": 243
    },
    {
      "epoch": 3.873015873015873,
      "grad_norm": 0.05186545476317406,
      "learning_rate": 2.2903225806451614e-06,
      "loss": 0.4062,
      "step": 244
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.048461537808179855,
      "learning_rate": 2.2580645161290324e-06,
      "loss": 0.4608,
      "step": 245
    },
    {
      "epoch": 3.9047619047619047,
      "grad_norm": 0.04938273876905441,
      "learning_rate": 2.2258064516129034e-06,
      "loss": 0.4731,
      "step": 246
    },
    {
      "epoch": 3.9206349206349205,
      "grad_norm": 0.04641064628958702,
      "learning_rate": 2.1935483870967745e-06,
      "loss": 0.4007,
      "step": 247
    },
    {
      "epoch": 3.9365079365079367,
      "grad_norm": 0.04744308814406395,
      "learning_rate": 2.161290322580645e-06,
      "loss": 0.4769,
      "step": 248
    },
    {
      "epoch": 3.9523809523809526,
      "grad_norm": 0.043558329343795776,
      "learning_rate": 2.129032258064516e-06,
      "loss": 0.3793,
      "step": 249
    },
    {
      "epoch": 3.9682539682539684,
      "grad_norm": 0.049317728728055954,
      "learning_rate": 2.096774193548387e-06,
      "loss": 0.5153,
      "step": 250
    },
    {
      "epoch": 3.984126984126984,
      "grad_norm": 0.05109180882573128,
      "learning_rate": 2.0645161290322582e-06,
      "loss": 0.4891,
      "step": 251
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.07760121673345566,
      "learning_rate": 2.0322580645161293e-06,
      "loss": 0.6138,
      "step": 252
    },
    {
      "epoch": 4.015873015873016,
      "grad_norm": 0.050729088485240936,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.5363,
      "step": 253
    },
    {
      "epoch": 4.031746031746032,
      "grad_norm": 0.05100219324231148,
      "learning_rate": 1.967741935483871e-06,
      "loss": 0.5113,
      "step": 254
    },
    {
      "epoch": 4.0476190476190474,
      "grad_norm": 0.04638238623738289,
      "learning_rate": 1.935483870967742e-06,
      "loss": 0.4077,
      "step": 255
    },
    {
      "epoch": 4.063492063492063,
      "grad_norm": 0.05074995756149292,
      "learning_rate": 1.9032258064516128e-06,
      "loss": 0.4994,
      "step": 256
    },
    {
      "epoch": 4.079365079365079,
      "grad_norm": 0.04833143204450607,
      "learning_rate": 1.870967741935484e-06,
      "loss": 0.4545,
      "step": 257
    },
    {
      "epoch": 4.095238095238095,
      "grad_norm": 0.04723207652568817,
      "learning_rate": 1.838709677419355e-06,
      "loss": 0.4582,
      "step": 258
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.04670998826622963,
      "learning_rate": 1.8064516129032258e-06,
      "loss": 0.4401,
      "step": 259
    },
    {
      "epoch": 4.1269841269841265,
      "grad_norm": 0.048264097422361374,
      "learning_rate": 1.774193548387097e-06,
      "loss": 0.4789,
      "step": 260
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 0.04589568451046944,
      "learning_rate": 1.7419354838709678e-06,
      "loss": 0.4555,
      "step": 261
    },
    {
      "epoch": 4.158730158730159,
      "grad_norm": 0.050922736525535583,
      "learning_rate": 1.7096774193548387e-06,
      "loss": 0.5417,
      "step": 262
    },
    {
      "epoch": 4.174603174603175,
      "grad_norm": 0.050444599241018295,
      "learning_rate": 1.67741935483871e-06,
      "loss": 0.4743,
      "step": 263
    },
    {
      "epoch": 4.190476190476191,
      "grad_norm": 0.05247979983687401,
      "learning_rate": 1.6451612903225808e-06,
      "loss": 0.4949,
      "step": 264
    },
    {
      "epoch": 4.2063492063492065,
      "grad_norm": 0.05227323994040489,
      "learning_rate": 1.6129032258064516e-06,
      "loss": 0.4199,
      "step": 265
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.048823338001966476,
      "learning_rate": 1.5806451612903226e-06,
      "loss": 0.4877,
      "step": 266
    },
    {
      "epoch": 4.238095238095238,
      "grad_norm": 0.047295037657022476,
      "learning_rate": 1.5483870967741937e-06,
      "loss": 0.4263,
      "step": 267
    },
    {
      "epoch": 4.253968253968254,
      "grad_norm": 0.057347290217876434,
      "learning_rate": 1.5161290322580647e-06,
      "loss": 0.6307,
      "step": 268
    },
    {
      "epoch": 4.26984126984127,
      "grad_norm": 0.050277773290872574,
      "learning_rate": 1.4838709677419356e-06,
      "loss": 0.5163,
      "step": 269
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.04587031155824661,
      "learning_rate": 1.4516129032258066e-06,
      "loss": 0.4316,
      "step": 270
    },
    {
      "epoch": 4.301587301587301,
      "grad_norm": 0.051958560943603516,
      "learning_rate": 1.4193548387096776e-06,
      "loss": 0.4855,
      "step": 271
    },
    {
      "epoch": 4.317460317460317,
      "grad_norm": 0.04999806731939316,
      "learning_rate": 1.3870967741935485e-06,
      "loss": 0.4573,
      "step": 272
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.045091643929481506,
      "learning_rate": 1.3548387096774195e-06,
      "loss": 0.3999,
      "step": 273
    },
    {
      "epoch": 4.349206349206349,
      "grad_norm": 0.052811212837696075,
      "learning_rate": 1.3225806451612906e-06,
      "loss": 0.5303,
      "step": 274
    },
    {
      "epoch": 4.365079365079365,
      "grad_norm": 0.04931625351309776,
      "learning_rate": 1.2903225806451614e-06,
      "loss": 0.4821,
      "step": 275
    },
    {
      "epoch": 4.380952380952381,
      "grad_norm": 0.053534381091594696,
      "learning_rate": 1.2580645161290322e-06,
      "loss": 0.5077,
      "step": 276
    },
    {
      "epoch": 4.396825396825397,
      "grad_norm": 0.05146634578704834,
      "learning_rate": 1.2258064516129033e-06,
      "loss": 0.4579,
      "step": 277
    },
    {
      "epoch": 4.412698412698413,
      "grad_norm": 0.048575807362794876,
      "learning_rate": 1.1935483870967743e-06,
      "loss": 0.4861,
      "step": 278
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 0.050120893865823746,
      "learning_rate": 1.1612903225806454e-06,
      "loss": 0.4212,
      "step": 279
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.04594824090600014,
      "learning_rate": 1.1290322580645162e-06,
      "loss": 0.4606,
      "step": 280
    },
    {
      "epoch": 4.4603174603174605,
      "grad_norm": 0.04879163205623627,
      "learning_rate": 1.0967741935483872e-06,
      "loss": 0.4342,
      "step": 281
    },
    {
      "epoch": 4.476190476190476,
      "grad_norm": 0.05147502198815346,
      "learning_rate": 1.064516129032258e-06,
      "loss": 0.4703,
      "step": 282
    },
    {
      "epoch": 4.492063492063492,
      "grad_norm": 0.04994874447584152,
      "learning_rate": 1.0322580645161291e-06,
      "loss": 0.4371,
      "step": 283
    },
    {
      "epoch": 4.507936507936508,
      "grad_norm": 0.04756498336791992,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.3882,
      "step": 284
    },
    {
      "epoch": 4.523809523809524,
      "grad_norm": 0.05008558928966522,
      "learning_rate": 9.67741935483871e-07,
      "loss": 0.4197,
      "step": 285
    },
    {
      "epoch": 4.5396825396825395,
      "grad_norm": 0.05437804013490677,
      "learning_rate": 9.35483870967742e-07,
      "loss": 0.5027,
      "step": 286
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.047875531017780304,
      "learning_rate": 9.032258064516129e-07,
      "loss": 0.4229,
      "step": 287
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.04907495155930519,
      "learning_rate": 8.709677419354839e-07,
      "loss": 0.4469,
      "step": 288
    },
    {
      "epoch": 4.587301587301587,
      "grad_norm": 0.049040064215660095,
      "learning_rate": 8.38709677419355e-07,
      "loss": 0.4579,
      "step": 289
    },
    {
      "epoch": 4.603174603174603,
      "grad_norm": 0.04670430347323418,
      "learning_rate": 8.064516129032258e-07,
      "loss": 0.4168,
      "step": 290
    },
    {
      "epoch": 4.619047619047619,
      "grad_norm": 0.04537650942802429,
      "learning_rate": 7.741935483870968e-07,
      "loss": 0.3668,
      "step": 291
    },
    {
      "epoch": 4.634920634920634,
      "grad_norm": 0.049650900065898895,
      "learning_rate": 7.419354838709678e-07,
      "loss": 0.4614,
      "step": 292
    },
    {
      "epoch": 4.650793650793651,
      "grad_norm": 0.047457169741392136,
      "learning_rate": 7.096774193548388e-07,
      "loss": 0.4366,
      "step": 293
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.05334414169192314,
      "learning_rate": 6.774193548387098e-07,
      "loss": 0.485,
      "step": 294
    },
    {
      "epoch": 4.682539682539683,
      "grad_norm": 0.04959506914019585,
      "learning_rate": 6.451612903225807e-07,
      "loss": 0.4223,
      "step": 295
    },
    {
      "epoch": 4.698412698412699,
      "grad_norm": 0.04610529541969299,
      "learning_rate": 6.129032258064516e-07,
      "loss": 0.3838,
      "step": 296
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 0.04653547704219818,
      "learning_rate": 5.806451612903227e-07,
      "loss": 0.4392,
      "step": 297
    },
    {
      "epoch": 4.73015873015873,
      "grad_norm": 0.04549776762723923,
      "learning_rate": 5.483870967741936e-07,
      "loss": 0.3739,
      "step": 298
    },
    {
      "epoch": 4.746031746031746,
      "grad_norm": 0.05330967158079147,
      "learning_rate": 5.161290322580646e-07,
      "loss": 0.5003,
      "step": 299
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 0.05087769404053688,
      "learning_rate": 4.838709677419355e-07,
      "loss": 0.4853,
      "step": 300
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 0.05235074460506439,
      "learning_rate": 4.5161290322580644e-07,
      "loss": 0.4738,
      "step": 301
    },
    {
      "epoch": 4.7936507936507935,
      "grad_norm": 0.04957415536046028,
      "learning_rate": 4.193548387096775e-07,
      "loss": 0.4179,
      "step": 302
    },
    {
      "epoch": 4.809523809523809,
      "grad_norm": 0.05233403667807579,
      "learning_rate": 3.870967741935484e-07,
      "loss": 0.5372,
      "step": 303
    },
    {
      "epoch": 4.825396825396825,
      "grad_norm": 0.04940532147884369,
      "learning_rate": 3.548387096774194e-07,
      "loss": 0.4827,
      "step": 304
    },
    {
      "epoch": 4.841269841269841,
      "grad_norm": 0.04726874828338623,
      "learning_rate": 3.2258064516129035e-07,
      "loss": 0.4727,
      "step": 305
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.053674452006816864,
      "learning_rate": 2.9032258064516134e-07,
      "loss": 0.5601,
      "step": 306
    },
    {
      "epoch": 4.8730158730158735,
      "grad_norm": 0.049096278846263885,
      "learning_rate": 2.580645161290323e-07,
      "loss": 0.4132,
      "step": 307
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.0528995543718338,
      "learning_rate": 2.2580645161290322e-07,
      "loss": 0.4708,
      "step": 308
    },
    {
      "epoch": 4.904761904761905,
      "grad_norm": 0.053789280354976654,
      "learning_rate": 1.935483870967742e-07,
      "loss": 0.5461,
      "step": 309
    },
    {
      "epoch": 4.920634920634921,
      "grad_norm": 0.04681137204170227,
      "learning_rate": 1.6129032258064518e-07,
      "loss": 0.4163,
      "step": 310
    },
    {
      "epoch": 4.936507936507937,
      "grad_norm": 0.050652045756578445,
      "learning_rate": 1.2903225806451614e-07,
      "loss": 0.5028,
      "step": 311
    },
    {
      "epoch": 4.9523809523809526,
      "grad_norm": 0.049334816634655,
      "learning_rate": 9.67741935483871e-08,
      "loss": 0.4535,
      "step": 312
    },
    {
      "epoch": 4.968253968253968,
      "grad_norm": 0.05091541260480881,
      "learning_rate": 6.451612903225807e-08,
      "loss": 0.4449,
      "step": 313
    },
    {
      "epoch": 4.984126984126984,
      "grad_norm": 0.05487387627363205,
      "learning_rate": 3.2258064516129035e-08,
      "loss": 0.462,
      "step": 314
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.060718752443790436,
      "learning_rate": 0.0,
      "loss": 0.3962,
      "step": 315
    }
  ],
  "logging_steps": 1,
  "max_steps": 315,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.105903549739745e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
